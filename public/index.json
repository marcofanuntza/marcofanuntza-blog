[{"content":"Nel mio Homelab, come ho descritto in precedenza in altri post, utilizzo un cluster Proxmox.\nSebbene il cluster disponga già di una dashboard integrata, questa risulta piuttosto spartana. Così, in questo tranquillo pomeriggio delle ferie natalizie, ho colto l\u0026rsquo;occasione per sperimentare una soluzione più avanzata: InfluxDB + Grafana.\nInfluxDB è un database open-source progettato specificamente per la gestione di serie temporali, ovvero dati che variano nel tempo come metriche, eventi e log. Grazie alla sua architettura ottimizzata, è in grado di gestire grandi volumi di dati in tempo reale, rendendolo una scelta ideale per applicazioni di monitoraggio e analisi delle prestazioni.\nGrazie alla sua natura scalabile e alla capacità di integrarsi con strumenti come Grafana, Prometheus e Telegraf, InfluxDB è ampiamente utilizzato nel campo DevOps. Con un\u0026rsquo;interfaccia semplice e un set di funzionalità avanzate, rappresenta una soluzione potente per chiunque abbia bisogno di analizzare e visualizzare dati basati sul tempo.\nGrafana è una piattaforma open-source progettata per la visualizzazione e l\u0026rsquo;analisi di dati in tempo reale, utilizzata principalmente per il monitoraggio delle metriche e dei log. Grazie alla sua capacità di supportare numerose fonti di dati, come Prometheus, InfluxDB, Elasticsearch e database relazionali, consente agli utenti di creare dashboard altamente personalizzabili e interattive per analizzare informazioni complesse.\nOltre alla visualizzazione, Grafana offre funzionalità avanzate di alerting, permettendo di configurare avvisi personalizzati in base a specifiche condizioni e inviarli tramite email, Slack o altri canali.\nIl paradigma è semplice \u0026raquo; Proxmox genera le metriche, InfluxDB si occupa di salvare le metriche in un database (bucket) e Grafana infine restituirà le stesse sottoforma di dashboard con grafici dettagliati.\nPer installare InfluxDB e Grafana ho deciso di procedere utilizzando dei container Docker e per comodità ho scelto un\u0026rsquo;approccio molto easy \u0026amp; lazy\u0026hellip; tradotto significa.. utilizzo Portainer e suoi templates. Basta semplicemente andare su Templates e nella ricerca scrivere influxdb e poi grafana, troverete rispettivamente: InfluxDB for Edge e Grafana Dashboard\nI template su Portainer sono già strutturati per l\u0026rsquo;inserimento delle variabili, nello specifico per InfluxDB ci sarà la creazione di un primo bucket (Influx Bucket Name) e una organization (Influx Org Name), questi dati saranno utili nel passaggio successivo.\nDopo aver installato entrambi vai istruito il cluster per indirizzare le metriche su InfluxDB, su Proxmox dalla GUI sulla sezione root del Datacenter si va sul menù Metric Server ADD \u0026gt; InfluxDB\nCome potete notare viene chiesto anche un Token, questo va creato su InfluxDB, io per comodità ho clonato il token admin già presente su sezione Load Data \u0026raquo; API Tokens\nSalvate su Proxmox e se i dati sono stati inseriti correttamente dovreste già vedere su InfluxDB nella sezione Data Explorer il bucket popolarsi con le metriche, nel mio caso il bucket si chiama proxmox-mnt.\nLa parte di configurazione su Proxmox e InfluxDB possiamo considerarla conclusa, ora si potrà continuare su Grafana.\nSu Grafana iniziamo con \u0026ldquo;agganciare\u0026rdquo; la base dati salvata da InfluxDB, per fare questo rechiamoci sulla sezione Connections e poi Data Sources, qui nella ricerca inseriamo InfluxDB come sorgente e compiliamo i dati necessari\nPrestate attenzione nel linguaggio SQL da scegliere, e importante selezionare Flux\nCompletiamo inserendo i dati per far raggiungere InfluxDB e relativo bucket, il token che andremo a utilizzare si è lo stesso di prima. Salvate con test positivo.\nAdesso arriva la parte finale e più soddisfacente, su Grafana dobbiamo creare la nostra Dashboard. Cioè creare\u0026hellip; c\u0026rsquo;è già qualcuno più bravo di noi e disponibile che ha già fatto una dashboard!\nGrazie alla community Grafana permette di condividere dashboard già pronte e semplici da importare direttamente sulla vostra istanza, queste si trovano facilmente sul sito grafanalabs.\nProvate a cercare proxmox\n","permalink":"https://marcofanuntza.it/posts/monitorare-cluster-proxmox-con-influxdb-e-grafana/","summary":"\u003cp\u003eNel mio Homelab, come ho descritto in precedenza in altri post, utilizzo un cluster Proxmox.\u003c/p\u003e\n\u003cp\u003eSebbene il cluster disponga già di una dashboard integrata, questa risulta piuttosto spartana.\nCosì, in questo tranquillo pomeriggio delle ferie natalizie, ho colto l\u0026rsquo;occasione per sperimentare una soluzione più avanzata: InfluxDB + Grafana.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInfluxDB\u003c/strong\u003e è un database open-source progettato specificamente per la gestione di serie temporali, ovvero dati che variano nel tempo come metriche, eventi e log. Grazie alla sua architettura ottimizzata, è in grado di gestire grandi volumi di dati in tempo reale, rendendolo una scelta ideale per applicazioni di monitoraggio e analisi delle prestazioni.\u003c/p\u003e","title":"Come monitoro il mio cluster Proxmox con InfluxDB e Grafana"},{"content":"Nel precedente articolo avevo mostrato come procedere all\u0026rsquo;installazione di GitLab su un nostro server locale, clicca qui per leggerlo.\nGitLab è uno strumento leader nel mondo DevOps, oggi è tra i più diffusi per il versionamento del proprio codice software, ma oltre a questo è molto di più!\nGitLab tra le tante funzionalità, mette a disposizione anche un Container Registry.\nIl GitLab Container Registry è un registro integrato all\u0026rsquo;interno di GitLab stesso, che consente di archiviare, condividere e distribuire facilmente le immagini dei container all\u0026rsquo;interno dei progetti GitLab.\nQuesta integrazione, insieme a GitLab CI, rende GitLab una piattaforma completa per automatizzare e accelerare i processi DevOps, semplificando ulteriormente l\u0026rsquo;intero ciclo di vita dello sviluppo software.\nSe utilizzate GitLab come servizio direttamente su gitlab.com, troverete il registry già abilitato, se invece lo avete installato su un vostro server locale sarà necessario abilitarlo.\nLa documentazione ufficiale può sembrare molto esaustiva ma spesso ci si perde tra i dettagli.\nIn questa guida vi mostrerò brevemente solo quattro semplici stringhe da modificare per abilitare il registry.\nPartiamo dalla premessa che voi siate l\u0026rsquo;amministratore del sistema e che ne abbiate accesso e pieno controllo, il file da editare sarà il classico \u0026quot;/etc/gitlab/gitlab.rb\u0026quot;.\nLimitiamoci a editare queste sole stringhe, ovviamente voi adattatele alla vostra url\nregistry_external_url 'https://gitlab.marcofanuntza.it:5050' gitlab_rails['registry_enabled'] = true gitlab_rails['registry_host'] = \u0026quot;gitlab.marcofanuntza.it\u0026quot; gitlab_rails['registry_path'] = \u0026quot;/var/opt/gitlab/gitlab-rails/shared/registry\u0026quot; Dovete sapere che potete impostare una url specifica esclusiva per il registry, come per esempio registry.gitlab.esempio.it ma nel mio caso ho preferito avere un\u0026rsquo;unico indirizzo in comune con il servizio principale. Salvate il file e subito dopo eseguite il comando per applicare la configurazione alla vostra istanza in esecuzione con:\nsudo gitlab-ctl reconfigure sudo gitlab-ctl restart Prima di procedere con ulteriori test assicuratevi che la porta 5050 sia ora in ascolto, è la porta default che viene utilizzata per contattare il registry, se la porta è in ascolto possiamo andare avanti.\nOra già per un primo controllo \u0026ldquo;visivo\u0026rdquo; accedete su un vostro project all\u0026rsquo;interno di gitlab, tra le voci presenti sulla sinistra andate su \u0026ldquo;Deploy\u0026rdquo; dovrebbe esservi apparsa nel sottomenù la voce \u0026ldquo;Container Ragistry\u0026rdquo;\nEntrando sulla pagina vi mostrerà i comandi da eseguire sul vostro client docker per procedere con creazione e upload del container sul vostro nuovo Gitlab Container Registry\nProcediamo con quest\u0026rsquo;ultima verifica direttamente dal client Docker\u0026hellip;\nDobbiamo istruire il nostro client per considerare il registry come \u0026ldquo;insicuro\u0026rdquo; editando/creando il file /etc/docker/daemon.json\n{ \u0026quot;insecure-registries\u0026quot; : [ \u0026quot;https://gitlab.marcofanuntza.it:5050\u0026quot; ] } Non allarmatevi questa istruzione è necessaria quando si utilizza un certificato self-signed e la CA non può essere verificata da Docker\nPer la successiva build prepariamo un semplice Dockerfile come segue:\nFROM ubuntu RUN apt-get update CMD [\u0026quot;echo\u0026quot;, \u0026quot;Ciao, Gitlab Registry!\u0026quot;] Adesso siamo pronti! eseguiamo il login con il primo comando, le credenziali saranno le stesse del vostro user gitlab che vi permette di accedere al progetto\ndocker login gitlab.marcofanuntza.it:5050 Con il comando che segue eseguiamo la build del nostro container\ndocker build -t gitlab.marcofanuntza.it:5050/utentezero/testregistry . Ora il momento cruciale, con il comando successivo carichiamo l\u0026rsquo;immagine docker finalmente sul nostro Gitlab Container Registry\ndocker push gitlab.marcofanuntza.it:5050/utentezero/testregistry Nel prossimo articolo cercherò di mostrarvi lo stesso concetto però eseguito in automatico da una pipeline, sempre all\u0026rsquo;interno di Gitlab!\n","permalink":"https://marcofanuntza.it/posts/abilitiamo-il-gitlab-container-registry/","summary":"\u003cp\u003eNel precedente articolo avevo mostrato come procedere all\u0026rsquo;installazione di GitLab su un nostro server locale, \u003ca href=\"https://marcofanuntza.it/posts/installiamo-gitlab/\"\u003eclicca qui per leggerlo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eGitLab è uno strumento leader nel mondo DevOps, oggi è tra i più diffusi per il versionamento del proprio codice software, ma oltre a questo è molto di più!\u003cbr\u003e\nGitLab tra le tante funzionalità, mette a disposizione anche un Container Registry.\u003c/p\u003e\n\u003cp\u003eIl \u003cstrong\u003eGitLab Container Registry\u003c/strong\u003e è un registro integrato all\u0026rsquo;interno di GitLab stesso, che consente di archiviare, condividere e distribuire facilmente le immagini dei container all\u0026rsquo;interno dei progetti GitLab.\u003c/p\u003e","title":"Abilitiamo il Gitlab Container Registry"},{"content":"Come eseguo il backup dei volumi Docker? Niente di più semplice!\nTralasciando la pappardella su quanto sia importante avere dei backup vi mostro come eseguo il backup dei volumi Docker presenti sul mio Raspberry Pi 5\nEseguo applicazioni in self hosting e alcune sono dei container Docker, ho citato il Raspberry ma la stessa procedura può essere utilizzata su qualsiasi altra distribuzione Linux\nPartiamo dal presupposto che utilizziate Docker che esegue dei container e che questi abbiano dei volumi persistenti come nel mio caso.\ndocker volume ls DRIVER VOLUME NAME local portainer_data local vaultw-data Se non conoscete il percorso dove Docker posiziona i volumi potete ottenerlo semplicemente con un inspect del volume\ndocker inspect vaultw-data [ { \u0026quot;CreatedAt\u0026quot;: \u0026quot;2024-12-01T08:45:34Z\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Labels\u0026quot;: null, \u0026quot;Mountpoint\u0026quot;: \u0026quot;/var/lib/docker/volumes/vaultw-data/_data\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;vaultw-data\u0026quot;, \u0026quot;Options\u0026quot;: null, \u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot; } ] Ecco lo script\nQuesto è lo script che utilizzo, ci sono i commenti per ogni fase, spero siano comprensibili per voi.\n#! /bin/sh PATH=/usr/local/bin:/usr/bin: #Script per eseguire backup dei volumi docker #Dichiaro Variabili DATE_BIN=$(command -v date) DATE=`${DATE_BIN} +%y-%m-%d--%H:%M:%S` DESTINATION=/home/marco/backup-docker-volume/backups/backup-docker/$DATE.tar.gz SOURCEFOLDER=/var/lib/docker/volumes/ #Stop dei container in esecuzione, è importante per avere una situazione senza scritture sui volumi docker stop $(docker ps -q) #Creo un file zip tar.gz della directory dove Docker posiziona i volumi e salvo il file sulla directory di destinazione tar -cpzf $DESTINATION $SOURCEFOLDER #Restart dei container Docker precedentemente fermati docker restart $(docker ps -a -q) #Cancello i file backup più vecchi di 7 giorni find $DESTINATION/ -mtime +7 -type f -delete Salvato il file lo si rende eseguibile, il nome che ho dato allo script è: script-bck.sh voi potete scegliere il nome che preferite basta che venga mantenuta estensione del file .sh\nchmod a+x script-bck.sh Sarebbe buona norma eseguirlo almeno una volta al giorno e considerando il fatto che i container devono essere momentaneamente fermati vi consiglio di eseguirlo la notte, ci pensa CRON (Obviously numero 1)\n0 0 * * * /usr/bin/sh /home/marco/backup-docker-volume/script-bck.sh Il backup è importante ma la sua funzione potrebbe essere vana se lasciassimo i files sullo stesso host, muore questo perdiamo anche i backup.. (Obviously numero 2)\nPer questo i files vengono direttamente scritti su uno share NFS presente sul mio server NAS (TruenasCore)\nQuello che infatti non ho specificato sullo script è che la variabile $DESTINATION è uno share NFS montato da fstab\nSawadee kap!\n","permalink":"https://marcofanuntza.it/posts/come-eseguo-backup-volumi-docker/","summary":"\u003cp\u003eCome eseguo il backup dei volumi Docker? Niente di più semplice!\u003c/p\u003e\n\u003cp\u003eTralasciando la pappardella su quanto sia importante avere dei backup vi mostro come eseguo il backup dei volumi Docker presenti sul mio Raspberry Pi 5\u003c/p\u003e\n\u003cp\u003eEseguo applicazioni in self hosting e alcune sono dei container Docker, ho citato il Raspberry ma la stessa procedura può essere utilizzata su qualsiasi altra distribuzione Linux\u003c/p\u003e\n\u003cp\u003ePartiamo dal presupposto che utilizziate Docker che esegue dei container e che questi abbiano dei volumi persistenti come nel mio caso.\u003c/p\u003e","title":"Come eseguo backup dei volumi Docker"},{"content":"Questo articolo continua la serie denominata \u0026ldquo;Il potere CI/CD\u0026rdquo;, in precedenza abbiamo mostrato come installare ArgoCD, poi siamo passati al registry con Harbor, adesso è arrivato il momento di Gitlab.\nChe cos\u0026rsquo;è Gitlab?\nGitLab è una piattaforma per la gestione del software basata su Git, fornisce un vasto set di strumenti per favorire la collaborazione, automatizzare processi e monitorare lo sviluppo del software durante il suo ciclo di vita.\nEcco alcuni aspetti chiave di GitLab:\nRepository Git: Fornisce repository Git per il controllo delle versioni del codice sorgente del software.\nGestione del Progetto: Include strumenti per la gestione delle attività, la pianificazione, la tracciabilità dei problemi e altro ancora.\nIntegrazione Continua e Rilascio Continuo (CI/CD): Supporta la creazione, il test e il rilascio automatici del software attraverso pipeline CI/CD.\nControllo degli Accessi e Autorizzazioni: Consente la definizione precisa dei permessi di accesso per garantire una collaborazione sicura.\nIssue Tracking e Kanban Boards: Offre strumenti avanzati per la gestione delle problematiche e la visualizzazione del lavoro in corso attraverso Kanban boards.\nCollaborazione e Commenti: Favorisce la collaborazione tra membri del team con funzionalità di commento e comunicazione integrate.\nWiki e Documentazione: Include un sistema di wiki e strumenti per la documentazione, consentendo la creazione e la condivisione di informazioni.\nRegistri e Monitoraggio: Fornisce funzionalità per la registrazione delle attività, il monitoraggio delle performance e la gestione dei log.\nIntegrazioni: Si integra con una varietà di strumenti di sviluppo di terze parti per una maggiore flessibilità.\nGitLab è disponibile in diverse edizioni, tra cui una versione gratuita e open source, GitLab Community Edition (CE), e una versione a pagamento, GitLab Enterprise Edition (EE), che offre funzionalità avanzate e supporto aziendale.\nPrerequisiti\n-VM o server con distribuzione Ubuntu 22.04 -Accesso alla rete -Sul file hosts inserite il nome che esporrà il servizio, \u0026ldquo;EXTERNAL_URL\u0026rdquo; nel mio caso sarà: gitlab.marcofanuntza.it\nProcedura\nPer installazione dei pacchetti utilizzeremo il sistema APT, partiamo con installare alcune dipendenze con questo comando che segue\nsudo apt-get install -y curl openssh-server ca-certificates tzdata perl Per l\u0026rsquo;installazione di Gitlab sarà necessario utilizzare il loro repository specifico, recuperiamolo tramite curl\ncurl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.deb.sh | sudo bash Adesso che abbiamo a disposizione il repository ufficiale possiamo procedere con installazione tramite APT, nel comando possiamo già da ora settare la variabile EXTERNAL_URL\nsudo EXTERNAL_URL=\u0026quot;https://gitlab.marcofanuntza.it\u0026quot; apt-get install gitlab-ee Nota.1 come avrete potuto notare stiamo installando il pacchetto *-ee che sta per enterprise edition, non preoccupatevi, recentemente gitlab distribuisce un solo pacchetto senza più distinguerlo come faceva in precedenza con la versione CE community edition e enterprise edition EE appunto, la gestione della licenza è facoltativa e successiva per chi ne avrà l\u0026rsquo;esigenza, il pacchetto sarà comunque lo stesso.\nNota.2 durante installazione in maniera automatica cercherà di crearvi un certificato lets\u0026rsquo;encrypt, se la vostra \u0026ldquo;EXTERNAL_URL\u0026rdquo; non è già esposta su dns pubblici e non sarà raggiungibile questa procedurà fallirà, ma non preoccupatevi l\u0026rsquo;installazione verrà comunque completata.\nAttendiamo che l\u0026rsquo;installazione tramite APT verrà completata, oltre a gitlab verranno installati vari servizi, ad esempio anche nginx e postgres. Terminata installazione, errore citato su nota 2 a parte, siamo pronti per chiamare il servizio tramite l\u0026rsquo;interfaccia web.\nLe credenziali per il login saranno root e la password temporanea potete recuperarla in chiaro sul file /etc/gitlab/initial_root_password\nDopo il primo login manco a dirlo procediamo subito con la sostituzione della nostra password di root, dopo averla sostituita ci ritroveremo nella pagina iniziale pronti per eseguire un nuovo login con le nuove credenziali.\nIl file principale che contiene e permette la maggior parte dei settings Gitlab è \u0026quot;/etc/gitlab/gitlab.rb\u0026quot;\nPer completare la nostra installazione a questo punto possiamo configurare alcuni aspetti, prima di tutto mettere in sicurezza il nostro servizio gitlab configurando il certificato SSL (per chi ha avuto l\u0026rsquo;errore in fase di installazione), poi possiamo passare a configurare le notifiche mail.\nPer le notiche mail possiamo utilizzare un servizio SMTP esterno, la configurazione dei parametri si può editare sempre dal file /etc/gitlab/gitlab.rb, qui un\u0026rsquo;esempio dei settings, potete comunque prendere visione della specifica pagina sulla documentazione ufficiale QUI\ngitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \u0026quot;smtp.server\u0026quot; gitlab_rails['smtp_port'] = 465 gitlab_rails['smtp_user_name'] = \u0026quot;smtp user\u0026quot; gitlab_rails['smtp_password'] = \u0026quot;smtp password\u0026quot; gitlab_rails['smtp_domain'] = \u0026quot;example.com\u0026quot; gitlab_rails['smtp_authentication'] = \u0026quot;login\u0026quot; gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['smtp_openssl_verify_mode'] = 'peer' # If your SMTP server does not like the default 'From: gitlab@localhost' you # can change the 'From' with this setting. gitlab_rails['gitlab_email_from'] = 'gitlab@example.com' gitlab_rails['gitlab_email_reply_to'] = 'noreply@example.com' # If your SMTP server is using a self signed certificate or a certificate which # is signed by a CA which is not trusted by default, you can specify a custom ca file. # Please note that the certificates from /etc/gitlab/trusted-certs/ are # not used for the verification of the SMTP server certificate. gitlab_rails['smtp_ca_file'] = '/path/to/your/cacert.pem' Ogni volta che editiamo il file e modifichiamo le impostazioni contenute all\u0026rsquo;interno è necessario eseguire il seguente comando per applicare le modifiche:\nsudo gitlab-ctl reconfigure La configurazione delle notifiche mail è di vitale importanza per la gestione degli user per la conferma delle credenziali, cambio password etc..\nL\u0026rsquo;articolo che completerà la serie mostrerà una pipeline completa che eseguirà un classico rilascio software sfruttando il sistema CI/CD e per farlo utilizzerà gli elementi che sono stato oggetto degli articoli precdenti, Gitlab, ArgoCD, Harbor e Kubernetes\n","permalink":"https://marcofanuntza.it/posts/installiamo-gitlab/","summary":"\u003cp\u003eQuesto articolo continua la serie denominata \u0026ldquo;Il potere CI/CD\u0026rdquo;, in precedenza abbiamo mostrato come installare ArgoCD, poi siamo passati al registry con Harbor, adesso è arrivato il momento di Gitlab.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChe cos\u0026rsquo;è Gitlab?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGitLab è una piattaforma per la gestione del software basata su Git, fornisce un vasto set di strumenti per favorire la collaborazione, automatizzare processi e monitorare lo sviluppo del software durante il suo ciclo di vita.\u003c/p\u003e\n\u003cp\u003eEcco alcuni aspetti chiave di GitLab:\u003c/p\u003e","title":"Installiamo Gitlab"},{"content":"Harbor è un registry open-source per la gestione delle immagini dei container. Progettato per funzionare con orchestration tools come Kubernetes e Docker Swarm.\nEcco alcune caratteristiche principali di Harbor:\nHarbor offre un registry sicuro e privato per le immagini dei container, permettendo un controllo totale sulla loro archiviazione e distribuzione.\nPolitiche di Sicurezza: Supporta politiche per garantire che solo immagini sicure e approvate vengano utilizzate nell\u0026rsquo;ambiente.\nControllo degli Accessi: Dispone di un sistema robusto di controllo degli accessi, consentendo la definizione precisa di chi può accedere e distribuire immagini specifiche.\nIntegrazione Universale: Progettato per integrarsi facilmente con strumenti comuni come Kubernetes, Docker e altri.\nReplicazione di Immagini: Consente la replicazione tra diversi registri Harbor, garantendo la disponibilità e la distribuzione su scala geografica.\nScansione delle Vulnerabilità: Fornisce strumenti di scansione per identificare e affrontare potenziali problemi di sicurezza nelle immagini dei container.\nGestione del Ciclo di Vita: Supporta l\u0026rsquo;intero ciclo di vita delle immagini, dalla creazione alla distribuzione e alla dismissione.\nHarbor è particolarmente utile in ambienti aziendali in cui la sicurezza e il controllo dell\u0026rsquo;accesso alle immagini container sono fondamentali. La sua architettura aperta e scalabile lo rende adatto sia a scenari di piccola scala che a implementazioni di grandi dimensioni.\nProviamolo installandolo su un server o su una VM\nPrerequisiti\n-VM o server con distribuzione Linux (in questo articolo ho utilizzato ubuntu-server-22-04-LTS)\n-Docker\n-Docker compose\n-Openssl\nPer installare Docker e Docker Compose posso segnalarvi questo howto che ho scritto in precedenza QUI\nProcedura\nDopo aver installato i prerequisti, dobbiamo assicurarci che il server o la VM soddisfino i requisiti consigliati da Harbor, deve avere a disposizione almeno 4GB di Ram e 2CPU, per lo spazio disco possono bastare 20GB, che comunque possono essere estesi al bisogno se avete utilizzato LVM.\nL\u0026rsquo;installazione di Harbor viene eseguita tramite un\u0026rsquo;installer che possiamo scaricare dal sito ufficiale, noi utilizzeremo la versione 2.10.0\nwget https://github.com/goharbor/harbor/releases/download/v2.10.0/harbor-online-installer-v2.10.0.tgz tar -xzvf harbor-online-installer-v2.10.0.tgz cd harbor cp harbor.yml.tmpl harbor.yml a questo punto dobbiamo creare il nostro certificato SSL, per questa guida inizialmente utilizzeremo un certificato self-signed creato con openssl\nmkdir -p certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout certs/harbor.key -addext \u0026quot;subjectAltName = DNS:harbor.marcofanuntza.it\u0026quot; -x509 -days 365 -out certs/harbor.crt dopo aver terminato copiamo il risultato sul path dei certificati\nsudo cp certs/harbor.* /etc/ssl/certs/ editiamo il file harbor.yml che abbiamo copiato in precedenza, dobbiamo inserire hostname e il path del certificato SSL\nhostname: harbor.marcofanuntza.it # http related config http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 80 https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /etc/ssl/certs/harbor.crt private_key: /etc/ssl/certs/harbor.key adesso sempre all\u0026rsquo;interno della directory harbor possiamo notare due script bash, il primo da eseguire sarà prepare e in seguito eseguiremo install.sh\n./prepare lo script prepare scaricherà una prima immagine docker da docker-hub, questa preparerà una base per lo script successivo che di fatto andrà a tirare sù tutto lo stack docker tramite docker compose\n./install.sh mettetevi comodi perche ci vorrà un pò, verranno scaricate le immagini docker per redis, postresql, registry, proxy, core e portal, successivamente verranno tutte avviate in automatico, dovete attendere un output simile a quanto segue\n✔ Container harbor-log Started ✔ Container harbor-portal Started ✔ Container registry Started ✔ Container redis Started ✔ Container registryctl Started ✔ Container harbor-db Started ✔ Container harbor-core Started ✔ Container harbor-jobservice Started ✔ Container nginx Started sembra che tutto sia andato per il verso giusto, per scrupolo comunque potete verificare che tutti i container siano in stato di esecuzione tramite docker ps\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a069f83f6725 goharbor/harbor-jobservice:v2.10.0 \u0026quot;/harbor/entrypoint.…\u0026quot; 3 minutes ago Up About a minute (healthy) harbor-jobservice e96dfe0dc5a0 goharbor/nginx-photon:v2.10.0 \u0026quot;nginx -g 'daemon of…\u0026quot; 3 minutes ago Up 2 minutes (healthy) 0.0.0.0:80-\u0026gt;8080/tcp, :::80-\u0026gt;8080/tcp, 0.0.0.0:443-\u0026gt;8443/tcp, :::443-\u0026gt;8443/tcp nginx 89a0f9fc181b goharbor/harbor-core:v2.10.0 \u0026quot;/harbor/entrypoint.…\u0026quot; 4 minutes ago Up 2 minutes (healthy) harbor-core b0ed82f07593 goharbor/registry-photon:v2.10.0 \u0026quot;/home/harbor/entryp…\u0026quot; 4 minutes ago Up 3 minutes (healthy) registry edabed017c3d goharbor/harbor-db:v2.10.0 \u0026quot;/docker-entrypoint.…\u0026quot; 4 minutes ago Up 3 minutes (healthy) harbor-db d83c114bbebd goharbor/harbor-registryctl:v2.10.0 \u0026quot;/home/harbor/start.…\u0026quot; 4 minutes ago Up 3 minutes (healthy) registryctl 14fc36784402 goharbor/harbor-portal:v2.10.0 \u0026quot;nginx -g 'daemon of…\u0026quot; 4 minutes ago Up 3 minutes (healthy) harbor-portal 7f759eda6cb2 goharbor/redis-photon:v2.10.0 \u0026quot;redis-server /etc/r…\u0026quot; 4 minutes ago Up 3 minutes (healthy) redis 1d24113d3097 goharbor/harbor-log:v2.10.0 \u0026quot;/bin/sh -c /usr/loc…\u0026quot; 4 minutes ago Up 3 minutes (healthy) 127.0.0.1:1514-\u0026gt;10514/tcp harbor-log Si la nostra installazione di Harbor sembra completata senza intoppi, siamo pronti per accedere alla sua interfaccia web, apriamo il broswer e inseriamo la credenziali admin, la password la trovate in chiaro sul file harbor.yml, cambiatela dopo il primo login\necco la pagina principale\nAdesso non ci resta che creare gli user e relativi permessi per poter così iniziare a caricare immagini sui rispettivi project!\n","permalink":"https://marcofanuntza.it/posts/harbor-come-registry/","summary":"\u003cp\u003e\u003cstrong\u003eHarbor\u003c/strong\u003e è un registry open-source per la gestione delle immagini dei container. Progettato per funzionare con orchestration tools come Kubernetes e Docker Swarm.\u003c/p\u003e\n\u003cp\u003eEcco alcune caratteristiche principali di Harbor:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHarbor\u003c/strong\u003e offre un registry sicuro e privato per le immagini dei container, permettendo un controllo totale sulla loro archiviazione e distribuzione.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePolitiche di Sicurezza:\u003c/strong\u003e Supporta politiche per garantire che solo immagini sicure e approvate vengano utilizzate nell\u0026rsquo;ambiente.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eControllo degli Accessi:\u003c/strong\u003e Dispone di un sistema robusto di controllo degli accessi, consentendo la definizione precisa di chi può accedere e distribuire immagini specifiche.\u003c/p\u003e","title":"Harbor come Registry"},{"content":"Se siete arrivati a leggere questo articolo do per scontato conosciate già Kubectl, il non plus ultra della riga di comando per Kubernetes!\nSapevate che Kubectl può acquisire ancora più potenza grazie a Krew?\nKrew è uno strumento che semplifica la gestione, l\u0026rsquo;installazione e l\u0026rsquo;aggiornamento di tutta una serie di plugin specifici per Kubectl, estendendone di fatto la già ampia funzionalità.\nIl suo funzionamento può sembrare molto simile ai tradizionali gestori di pacchetti yum, apt, apk, brew e altri, utilizza un modello basato su repository per distribuire e gestire i plugin. L\u0026rsquo;installazione di nuovi plugin avviene in modo uniforme, fornendo una procedura standardizzata che semplifica il processo.\nKrew semplifica anche il processo di aggiornamento dei plugin, consentendo agli utenti di mantenere facilmente le loro estensioni aggiornate con le ultime funzionalità e correzioni di bug.\nTutto questo avviene direttamente dalla stessa interfaccia di comando Kubectl, gli utenti possono accedere a tutte le funzionalità aggiuntive avendo a disposizione un\u0026rsquo;ambiente totalmente integrato!\nSe è la prima volta che sentite parlare di krew, posso consigliarvi di dare un\u0026rsquo;occhiata alla documentazione ufficiale QUI\nIn questo articolo vi mostrerò come installarlo e proveremo alcuni plugin che ho avuto modo di testare.\nPrerequisiti\ndistribuzione linux git installato Procedimento Basandoci sulla documentazione ufficiale utilizzeremo uno script che si occuperà di installare Krew in autonomia, lo script è il seguente:\n( set -x; cd \u0026quot;$(mktemp -d)\u0026quot; \u0026amp;\u0026amp; OS=\u0026quot;$(uname | tr '[:upper:]' '[:lower:]')\u0026quot; \u0026amp;\u0026amp; ARCH=\u0026quot;$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\u0026quot; \u0026amp;\u0026amp; KREW=\u0026quot;krew-${OS}_${ARCH}\u0026quot; \u0026amp;\u0026amp; curl -fsSLO \u0026quot;https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\u0026quot; \u0026amp;\u0026amp; tar zxvf \u0026quot;${KREW}.tar.gz\u0026quot; \u0026amp;\u0026amp; ./\u0026quot;${KREW}\u0026quot; install krew ) dopo che lo script avrà terminato editiamo il file .bashrc inserendo nell\u0026rsquo;ultima riga l\u0026rsquo;export path che segue\nexport PATH=\u0026quot;${KREW_ROOT:-$HOME/.krew}/bin:$PATH\u0026quot; salvate il file e ricaricate la vostra sessione shell, il comando che segue se tutto è andato bene vi restituirà lo stesso output\nkubectl krew version OPTION VALUE GitTag v0.4.4 GitCommit 343e657 IndexURI https://github.com/kubernetes-sigs/krew-index.git BasePath /root/.krew IndexPath /root/.krew/index/default InstallPath /root/.krew/store BinPath /root/.krew/bin DetectedPlatform linux/amd64 Ok a questo punto abbiamo Krew installato, possiamo provare qualche plugin. Testiamo ad\u0026rsquo;esempio il plugin Tree, come potete vedere per installare il plugin basterà un semplice comando\nkubectl krew install tree La funzione principale del plugin Tree è fornire una rappresentazione visuale ad albero delle risorse all\u0026rsquo;interno di un cluster Kubernetes. Invece di visualizzare le risorse in un formato tabellare o di elenco, il plugin kubectl tree organizza gerarchicamente le risorse in una struttura ad albero, rendendo più facile comprendere la relazione tra di esse.\nEcco un\u0026rsquo;esempio con il deployment di Velero\nkubectl tree deployment velero -n velero NAMESPACE NAME READY REASON AGE velero Deployment/velero - 2y221d velero ├─ReplicaSet/velero-668d7f99cc - 630d velero │ └─Pod/velero-668d7f99cc-cqbpf True 287d velero ├─ReplicaSet/velero-86c77779f6 - 630d velero └─ReplicaSet/velero-8cb65ddbc - 2y221d Proviamo poi il plugin Clog che banalmente colora l\u0026rsquo;output dei logs quando visualizzati da kubectl\nkubectl krew install clog Proviamo il plugin Outdated che come potete immaginare farà una scansione delle immagini utilizzate segnalando tag corrente in utilizzo e versione latest, aiutandoci a individuare quali sarebbe meglio aggiornare\u0026hellip;\nkubectl krew install outdated kubectl outdated Come già scritto, sono presenti una mirade di plugin, al momento ne contiamo 231, vi rimando alla pagina ufficiale e scegliete il plugin che desiderate!\n","permalink":"https://marcofanuntza.it/posts/installiamo-krew/","summary":"\u003cp\u003eSe siete arrivati a leggere questo articolo do per scontato conosciate già Kubectl, il non plus ultra della riga di comando per Kubernetes!\u003c/p\u003e\n\u003cp\u003eSapevate che Kubectl può acquisire ancora più potenza grazie a Krew?\u003c/p\u003e\n\u003cp\u003eKrew è uno strumento che semplifica la gestione, l\u0026rsquo;installazione e l\u0026rsquo;aggiornamento di tutta una serie di plugin specifici per Kubectl, estendendone di fatto la già ampia funzionalità.\u003c/p\u003e\n\u003cp\u003eIl suo funzionamento può sembrare molto simile ai tradizionali gestori di pacchetti yum, apt, apk, brew e altri, utilizza un modello basato su repository per distribuire e gestire i plugin. L\u0026rsquo;installazione di nuovi plugin avviene in modo uniforme, fornendo una procedura standardizzata che semplifica il processo.\u003c/p\u003e","title":"Installiamo Krew"},{"content":"Velero è uno strumento che aiuta a gestire il backup e il ripristino delle risorse e dei volumi persistenti del tuo cluster Kubernetes.\nVelero è uno strumento open source molto utilizzato che consente il backup e il ripristino delle risorse Kubernetes e dei volumi persistenti tra cluster in cloud o on-premises. Supporta la maggior parte dei provider di archiviazione, come AWS, Azure, GCP, DigitalOcean e altri. Possiamo utilizzare Velero per creare snapshot del cluster Kubernetes in un determinato momento e ripristinare gli oggetti su un cluster differente o in uno stato diverso. Possiamo utilizzarlo anche per migrare i carichi di lavoro tra cluster in cloud e non, oppure per eseguire il ripristino in caso di guasti o perdita di dati.\nCaratteristiche e funzionalità di Velero:\nBackup e ripristino di risorse Kubernetes e volumi persistenti, manualmente o su base programmata.\nSupporto per vari provider di archiviazione come AWS S3, Azure Blob Storage, Google Cloud Storage, DigitalOcean Spaces, e altri.\nCapacità di filtrare e selezionare risorse su cui eseguire il backup e il ripristino, basandosi su namespace, label o annotazioni.\nCapacità di eseguire comandi o script personalizzati prima e dopo le operazioni di backup e ripristino, utilizzando gli hooks.\nCapacità di estendere la funzionalità di Velero utilizzando plugin personalizzati, come driver di backup e ripristino, plugin di object store o plugin di snapshot di volumi.\nCapacità di monitorare e risolvere problemi durante le operazioni di backup e ripristino, utilizzando log, eventi e metriche.\nSfruttando le funzionalità elencate in precedenza possiamo riassumere i casi d\u0026rsquo;uso più comuni di Velero:\nBackup e ripristino: Utilizzare Velero per eseguire il backup e il ripristino delle risorse e dei volumi persistenti del tuo cluster Kubernetes, manualmente o su base programmata.\nDisaster recovery: Utilizzare Velero per eseguire il ripristino di emergenza del tuo cluster Kubernetes, in caso di guasto del cluster o perdita di dati.\nMigrazione: Utilizzare Velero per migrare i tuoi carichi di lavoro tra cluster o cloud, senza perdere lo stato o la configurazione delle tue applicazioni e dati. Ad esempio, puoi utilizzare Velero per eseguire il backup del tuo cluster da un fornitore di servizi cloud e ripristinarlo su un altro fornitore di servizi cloud, nel caso desideri cambiare o ottimizzare i tuoi servizi cloud. Puoi anche utilizzare Velero per eseguire il backup del tuo cluster da una versione di Kubernetes e ripristinarlo su un\u0026rsquo;altra versione di Kubernetes, nel caso desideri eseguire l\u0026rsquo;aggiornamento o il downgrade del tuo cluster.\nVantaggi:\nÈ open source e gratuito, con una comunità attiva e documentazione.\nÈ facile da installare e utilizzare, con un\u0026rsquo;interfaccia a riga di comando semplice e una Helm chart.\nÈ compatibile e interoperabile con altri strumenti e componenti Kubernetes, come kubectl, Helm e kustomize.\nÈ flessibile e personalizzabile, con varie opzioni e parametri per configurare le operazioni di backup e ripristino.\nSvantaggi:\nNon supporta il backup e il ripristino delle risorse a livello di cluster, come CRD, RBAC o admission controllers. È necessario utilizzare uno strumento separato, come kubeadm, per eseguire il backup e il ripristino di queste risorse.\nNon supporta il backup e il ripristino di origini dati esterne, come database o code di messaggi, non gestite da Kubernetes. È necessario utilizzare uno strumento separato, come pg_dump, per eseguire il backup e il ripristino di queste fonti di dati.\nNon fornisce un\u0026rsquo;interfaccia utente grafica o una console di gestione web. È necessario utilizzare l\u0026rsquo;interfaccia a riga di comando o uno strumento di terze parti, come Arkade, per gestire e monitorare le operazioni di backup e ripristino.\nNel prossimo articolo installeremo e proveremo Velero sul nostro cluster Kubernetes di test utilizzato negli articoli precedenti!\n","permalink":"https://marcofanuntza.it/posts/backup-con-velero/","summary":"\u003cp\u003e\u003cstrong\u003eVelero\u003c/strong\u003e è uno strumento che aiuta a gestire il backup e il ripristino delle risorse e dei volumi persistenti del tuo cluster Kubernetes.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/velero2.webp#center\" alt=\"Example image\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVelero\u003c/strong\u003e è uno strumento open source molto utilizzato che consente il backup e il ripristino delle risorse Kubernetes e dei volumi persistenti tra cluster in cloud o on-premises. Supporta la maggior parte dei provider di archiviazione, come AWS, Azure, GCP, DigitalOcean e altri.\nPossiamo utilizzare Velero per creare snapshot del cluster Kubernetes in un determinato momento e ripristinare gli oggetti su un cluster differente o in uno stato diverso. Possiamo utilizzarlo anche per migrare i carichi di lavoro tra cluster in cloud e non, oppure per eseguire il ripristino in caso di guasti o perdita di dati.\u003c/p\u003e","title":"Backup con Velero"},{"content":"ARGO CD\nArgo CD è uno strumento open-source progettato per implementare e gestire il CD (continous deployment) su infrastrutture Kubernetes. Si basa sui principi GitOps, utilizza repository Git come unica fonte di verità per la configurazione dell\u0026rsquo;infrastruttura e delle applicazioni.\nI principali punti chiave di ARGO CD\nContinuous Deployment: ARGO CD automatizza il processo di implementazione delle applicazioni su cluster Kubernetes, garantendo che lo stato attuale corrisponda a quello dichiarato nel repository Git.\nGitOps: Basato sul concetto di GitOps, ARGO CD utilizza un repository Git come fonte unica di verità per la configurazione di infrastruttura e applicazioni.\nDashboard Intuitiva: Fornisce un\u0026rsquo;interfaccia web che consente agli utenti di visualizzare e gestire lo stato delle applicazioni nel cluster Kubernetes.\nRilasci: Semplifica i rilasci delle applicazioni tra i diversi ambienti, facilitando il passaggio da sviluppo a produzione attraverso il processo GitOps.\nRollback: Permette di tornare a versioni precedenti delle applicazioni in modo controllato in caso di problemi dopo un rilascio.\nIntegrazione con Strumenti di CI/CD: Può essere integrato con strumenti di CI/CD per creare un flusso di lavoro completo dall\u0026rsquo;implementazione iniziale alla gestione continua delle applicazioni.\nMulti-Cluster e Multi-Tenancy: Supporta l\u0026rsquo;implementazione su più cluster Kubernetes e offre funzionalità di multi-tenancy per isolare risorse e permessi.\nEstendibile e Personalizzabile: Essendo open-source ARGO CD è estendibile, consentendo agli utenti di creare estensioni personalizzate per adattarsi alle esigenze specifiche.\nAttraverso queste caratteristiche ARGO CD semplifica e automatizza il ciclo di vita delle applicazioni deployate su Kubernetes.\nPrerequisiti\nAvere a disposizione un cluster Kubernetes (anche locale) Avere una workstation/notebook, il vostro server di sviluppo ad\u0026rsquo;esempio, dove poter installare quanto segue Kubectl Helm Se non avete a disposizione un cluster Kubernetes, per installarlo possiamo seguire molteplici strade, in questo blog ho già scritto alcune guide che potrebbero aiutarvi nel raggiungere lo scopo. Tenete conto che non abbiamo bisogno di un cluster production-grade, basta un semplice cluster locale installato sulla vostra workstation/notebook.\nPer questo articolo io ho utilizzato un cluster Kubernetes installato con Kind, potete seguire questa guida QUI\nProcedimento\nOra.. se avete seguito la guida che vi ho indicato più sù, diamo per scontato che abbiamo già il cluster Kubernetes pronto con Kubectl, non ci rimane quindi che installare Helm.\nsudo curl -s https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash helm version version.BuildInfo{Version:\u0026quot;v3.14.0\u0026quot;, GitCommit:\u0026quot;3fc9f4b2638e76f26739cd77c7017139be81d0ea\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, GoVersion:\u0026quot;go1.21.5\u0026quot;} Adesso che abbiamo tutti gli elementi per procedere, iniziamo con creare il namespace da dedicare a argocd sul nostro cluster kubernetes\nsudo kubectl create namespace argocd L\u0026rsquo;installazione di Argo-CD verrà eseguita tramite Helm, nello specifico utilizzeremo il repository sviluppato dal team Bitnami, aggiungiamo il repo\nsudo helm repo add bitnami https://charts.bitnami.com/bitnami sudo helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;bitnami\u0026quot; chart repository Update Complete. ⎈Happy Helming!⎈ Adattate il comando che segue in base alle vostre esigenze, prestate attenzione alla password e all\u0026rsquo;hostname\nsudo helm install argocd --set config.secret.argocdServerAdminPassword=AB12345 --set server.ingress.enabled=true --set server.ingress.ingressClassName=nginx --set server.service.type=ClusterIP --set server.ingress.pathType=Prefix --set server.ingress.hostname=argocd.marcofanuntza.it bitnami/argo-cd --namespace argocd Riceverete questo output:\nNAME: argocd LAST DEPLOYED: Fri Jan 26 08:33:07 2024 NAMESPACE: argocd STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: argo-cd CHART VERSION: 5.5.0 APP VERSION: 2.9.5 ** Please be patient while the chart is being deployed ** 1. Access your Argo CD installation: Connect to one of the following hosts: http://argocd.marcofanuntza.it 2. Execute the following commands to obtain the Argo CD credentials: echo \u0026quot;Username: \\\u0026quot;admin\\\u0026quot;\u0026quot; echo \u0026quot;Password: $(kubectl -n argocd get secret argocd-secret -o jsonpath=\u0026quot;{.data.clearPassword}\u0026quot; | base64 -d)\u0026quot; Verifichiamo per sicurezza se la password rispecchia quanto abbiamo impostato in fase di installazione\nsudo echo \u0026quot;Password: $(sudo kubectl -n argocd get secret argocd-secret -o jsonpath=\u0026quot;{.data.clearPassword}\u0026quot; | base64 -d)\u0026quot; Password: AB12345 Ok perfetto, possiamo andare avanti.\nAdesso dovete prestare attenzione a questi passaggi perche saranno essenziali per permetterci di raggiungere la Web-Gui di Argo-CD.\nIniziamo con provare a chiamare sul browser la url impostata in fase di installazione, riceverete il warning sul certificato che è normale essendo self-signed, accettate e andate avanti, dovreste ricevere lo stesso errore che segue\nPerchè succede questo? Il problema è che di default Argo-CD gestisce la terminazione TLS in autonomia e reindirizza sempre le richieste HTTP in HTTPS.\nNoi abbiamo l\u0026rsquo;ingress controller che gestisce la terminazione TLS e comunica sempre con il servizio backend tramite HTTP, il risultato è che il server di Argo-CD risponderà sempre con un reindirizzamento in HTTPS. Da quì il nostro errore!\nUna delle soluzioni consiste nel disabilitare HTTPS su Argo-CD, che possiamo impostare utilizzando il flag \u0026ndash;insecure sul deployment argocd-server.\nQuesto problema è effettivamente documentato qui: link documentazione\nModifichiamo quindi il nostro deployment\nsudo kubectl edit deployment argocd-argo-cd-server -n argocd Editiamo solo questa sezione del file inserendo l\u0026rsquo;istruzione \u0026ndash;insecure, salvate e chiudete il file, Kubernetes in automatico eseguirà un restart del deployment\ncontainers: - args: - argocd-server - --insecure - --staticassets - /opt/bitnami/argo-cd/app Adesso non dovreste più avere l\u0026rsquo;errore precedente e si presenterà la pagina per il login sulla web-gui di Argo-CD\nProviamo un login utilizzando lo user admin e la password impostata in fase d\u0026rsquo;installazione\nL\u0026rsquo;installazione di ARGO-CD a questo punto possiamo considerarla conclusa, ora non ci (vi) resta che provare lo strumento, per fare questo abbiamo bisogno però di una applicazione e di un server git/gitlab da interrogare.. sarà l\u0026rsquo;oggetto del mio prossimo post su questo blog?\nContenuto Extra\nDimenticavo che sarebbe altrettanto utile avere la CLI di Argo-CD, quindi installiamola! Possiamo installare la CLI sul notebook o sulla stessa macchina dove abbiamo già installato Helm, oppure dove preferite a patto che Argo-CD sia raggiungibile.\nsudo curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 sudo chmod +x /usr/local/bin/argocd argocd version argocd: v2.8.9+3ef5ba7 BuildDate: 2024-01-19T18:33:48Z GitCommit: 3ef5ba76a95d71de88bfa9b6e887c86025a88fdd GitTreeState: clean GoVersion: go1.20.12 Compiler: gc Platform: linux/amd64 FATA[0000] Argo CD server address unspecified Adesso facciamo in modo di agganciare la CLI sul nostro ARGO-CD installato precedentemente sul cluster Kubernetes, niente di più semplice puntiamo la stessa url utilizzata per la web-gui, ci verranno chieste le credenziali admin e finito\nsudo argocd login argocd.marcofanuntza.it WARNING: server certificate had error: tls: failed to verify certificate: x509: certificate is valid for ingress.local, not argocd.marcofanuntza.it. Proceed insecurely (y/n)? y WARN[0008] Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web. Username: admin Password: 'admin:login' logged in successfully Context 'argocd.marcofanuntza.it' updated Con la CLI possiamo eseguire in maniera più diretta le stesse operazioni possibili dalla web-gui\nsudo argocd context CURRENT NAME SERVER * argocd.marcofanuntza.it argocd.marcofanuntza.it sudo argocd cluster list --grpc-web SERVER NAME VERSION STATUS MESSAGE PROJECT https://kubernetes.default.svc in-cluster Unknown Cluster has no applications and is not being monitored. Anche la CLI è stata installata, per avere una visione completa dei comandi possibili vi lascio il link della documentazione ufficiale QUI\n","permalink":"https://marcofanuntza.it/posts/installiamo-argocd/","summary":"\u003cp\u003e\u003cstrong\u003eARGO CD\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eArgo CD è uno strumento open-source progettato per implementare e gestire il CD (continous deployment) su infrastrutture Kubernetes.\nSi basa sui principi GitOps, utilizza repository Git come unica fonte di verità per la configurazione dell\u0026rsquo;infrastruttura e delle applicazioni.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eI principali punti chiave di ARGO CD\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eContinuous Deployment:\u003c/strong\u003e ARGO CD automatizza il processo di implementazione delle applicazioni su cluster Kubernetes, garantendo che lo stato attuale corrisponda a quello dichiarato nel repository Git.\u003c/p\u003e","title":"Installiamo Argocd"},{"content":"Proviamo Kubernetes con Kind\nQuesta guida è indicata per tutti coloro che hanno esigenza di interagire con un cluster Kubernetes per meri scopi di test, conoscenza e sviluppo utilizzando una workstation o notebook con risorse limitate.\nKind (Kubernetes IN Docker) è uno strumento open-source progettato per semplificare la creazione e la gestione di cluster Kubernetes locali utilizzando container Docker come nodi del cluster.\nEcco alcune caratteristiche chiave di kind:\nInstallazione Semplificata: kind semplifica notevolmente il processo di installazione di Kubernetes su una macchina locale, consentendo agli sviluppatori di creare rapidamente e facilmente cluster Kubernetes per scopi di sviluppo o test.\nUtilizzo di Docker come Nodi: kind utilizza container Docker per creare i nodi del cluster Kubernetes. Ogni nodo del cluster viene eseguito come un container Docker specifico, consentendo un\u0026rsquo;implementazione leggera e isolata del cluster locale.\nAmbienti Isolati: kind consente agli sviluppatori di creare cluster Kubernetes completamente isolati, garantendo che le risorse e le configurazioni di un cluster non interferiscano con altri cluster o ambienti.\nConfigurazione Dichiarativa: La configurazione di kind è dichiarativa e può essere definita attraverso file di configurazione in stile YAML. Questo approccio semplifica la creazione e la gestione di cluster con configurazioni complesse.\nIntegrazione con Strumenti di CI/CD: kind è spesso utilizzato negli ambienti di sviluppo e nei flussi di lavoro (CI/CD) per testare e validare applicazioni Kubernetes.\nAgilità nello Sviluppo e nel Test: kind consente agli sviluppatori di eseguire e testare le proprie applicazioni Kubernetes in un ambiente locale, facilitando lo sviluppo, il debug e il test delle applicazioni Kubernetes senza la necessità di un cluster remoto.\nEstendibile e Configurabile: In quanto strumento open-source, kind è estendibile e può essere configurato per soddisfare esigenze specifiche. Gli sviluppatori possono personalizzare i cluster creati con kind per rispecchiare le configurazioni desiderate.\nIn sintesi, kind è uno strumento che semplifica l\u0026rsquo;installazione di cluster Kubernetes locali, facilitando il processo di sviluppo, test e debug delle applicazioni Kubernetes in un ambiente controllato e isolato.\nPrerequisiti\nVirtual Machine, Workstation o Notebook con distribuzione Linux (si può utilizzare anche WSL2 su Windows con Docker Desktop installati) Docker Kubectl per interagire con il cluster Procedimento\nPer questo esempio io utilizzo una VM installata sul mio cluster Proxmox, la distribuzione utilizzata è Ubuntu 22-04-LTS, iniziamo con installare Docker\n# Aggiungiamo la chiave **GPG key** ufficiale del repository Docker sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \u0026quot;deb [arch=\u0026quot;$(dpkg --print-architecture)\u0026quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \u0026quot;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026quot;$VERSION_CODENAME\u0026quot;)\u0026quot; stable\u0026quot; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Installiamo Kind, sostanzialmente si tratta di scaricare il file binario eseguibile, impostare i giusti permessi e spostarlo sul path riservato ai file eseguibili\n# For AMD64 / x86_64 [ $(uname -m) = x86_64 ] \u0026amp;\u0026amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64 sudo chmod +x ./kind sudo mv ./kind /usr/local/bin/kind Installiamo Kubectl, il concetto è lo stesso precedente\nsudo curl -LO \u0026quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x kubectl sudo mv kubectl /usr/local/bin Verifichiamo le versioni appena installate di docker, Kind e Kubectl\ndocker --version Docker version 25.0.1, build 29cf629 kind --version kind version 0.20.0 kubectl version Client Version: v1.29.1 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Adesso siamo pronti per creare il nostro primo cluster Kubernetes utilizzando Kind, decidiamo prima come deve essere composto il nostro cluster, per esempio per le nostre esigenze desideriamo un cluster con 1 nodo control-plane e 3 nodi worker. Per istruire Kind a fare questo dobbiamo scrivere un file di configurazione.\nFile esempio in formato yaml, cluster-config.yaml\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 # patch the generated kubeadm config with some extra settings kubeadmConfigPatches: - | apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration evictionHard: nodefs.available: \u0026quot;0%\u0026quot; # patch it further using a JSON 6902 patch kubeadmConfigPatchesJSON6902: - group: kubeadm.k8s.io version: v1beta2 kind: ClusterConfiguration patch: | - op: add path: /apiServer/certSANs/- value: my-hostname # 1 control plane node and 3 workers nodes: # the control plane node config - role: control-plane image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72 kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \u0026quot;ingress-ready=true\u0026quot; extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP # the three workers - role: worker image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72 - role: worker image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72 - role: worker image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72 Il file oltre a dichiarare i tre nodi, 1 control-plane + 3 worker pre-abilita anche l\u0026rsquo;ingress controller con le porte 80 e 443. Mi raccomando sul file prestate attenzione alla corretta identazione yaml, altrimenti riceverete errore.\nEseguiamo il seguente comando che ci permetterà di creare il cluster con la configurazione desiderata.\nsudo kind create cluster --name kube-kind-test --config cluster-config.yaml Il comando completerà i task in circa due minuti, questo potrebbe variare in base alla vostra connessione internet, tenete conto che Docker dovrà scaricare le apposite immagini.\nDovreste ritrovarvi con un\u0026rsquo;output simile\nCreating cluster \u0026quot;kube-kind-test\u0026quot; ... ✓ Ensuring node image (kindest/node:v1.27.3) 🖼 ✓ Preparing nodes 📦 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜 Set kubectl context to \u0026quot;kind-kube-kind-test\u0026quot; You can now use your cluster with: kubectl cluster-info --context kind-kube-kind-test Thanks for using kind! 😊 A questo punto possiamo utilizzare kind per verificare il cluster appena creato e kubectl per interagire con esso\nsudo kind get clusters kube-kind-test sudo kubectl get nodes NAME STATUS ROLES AGE VERSION kube-kind-test-control-plane Ready control-plane 5m47s v1.27.3 kube-kind-test-worker Ready \u0026lt;none\u0026gt; 5m21s v1.27.3 kube-kind-test-worker2 Ready \u0026lt;none\u0026gt; 5m27s v1.27.3 kube-kind-test-worker3 Ready \u0026lt;none\u0026gt; 5m26s v1.27.3 sudo kubectl get ns NAME STATUS AGE default Active 2m kube-node-lease Active 2m kube-public Active 2m kube-system Active 2m local-path-storage Active 2m sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-5d78c9869d-6dgr5 1/1 Running 0 7m48s kube-system coredns-5d78c9869d-fchdj 1/1 Running 0 7m48s kube-system etcd-kube-kind-test-control-plane 1/1 Running 0 8m2s kube-system kindnet-6p787 1/1 Running 0 7m44s kube-system kindnet-7cwt7 1/1 Running 0 7m45s kube-system kindnet-fx5lg 1/1 Running 0 7m39s kube-system kindnet-tcwbt 1/1 Running 0 7m48s kube-system kube-apiserver-kube-kind-test-control-plane 1/1 Running 0 8m2s kube-system kube-controller-manager-kube-kind-test-control-plane 1/1 Running 0 8m2s kube-system kube-proxy-cjs8z 1/1 Running 0 7m45s kube-system kube-proxy-n97zk 1/1 Running 0 7m44s kube-system kube-proxy-qhmvm 1/1 Running 0 7m48s kube-system kube-proxy-vzxpr 1/1 Running 0 7m39s kube-system kube-scheduler-kube-kind-test-control-plane 1/1 Running 0 8m2s local-path-storage local-path-provisioner-6bc4bddd6b-zzw4d 1/1 Running 0 7m48s Infine per completare il discorso ingress-controller procederemo con installazione dell\u0026rsquo;ingress controller NGINX, il deployment verrà eseguito con kubectl scaricando il file direttamente dai repository ufficiali NGINX\nsudo kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml Verifichiamo sempre tramite kubectl\nsudo kubectl get deployment -n ingress-nginx NAME READY UP-TO-DATE AVAILABLE AGE ingress-nginx-controller 1/1 1 1 114s sudo kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-jk94q 0/1 Completed 0 2m44s ingress-nginx-admission-patch-qwc96 0/1 Completed 0 2m44s ingress-nginx-controller-864894d997-48cn4 1/1 Running 0 2m44s Assicuriamoci che l\u0026rsquo;ingress controller stia funzionando creando questo pod con service e ingress di esempio, al termine provate i due comandi curl\nsudo kubectl apply -f https://kind.sigs.k8s.io/examples/ingress/usage.yaml # should output \u0026quot;foo-app\u0026quot; curl localhost/foo/hostname # should output \u0026quot;bar-app\u0026quot; curl localhost/bar/hostname Bonus extra Installiamo Metal-lb - metal-lb è un servizio che simula la presenza di un load balancer sul nostro cluster Kubernetes, quando attivo si occuperà di assegnare un indirizzo IP al service di tipo LoadBalancer\nsudo kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml eseguiamo wget di questo file che poi andremo a editare\nwget https://kind.sigs.k8s.io/examples/loadbalancer/metallb-config.yaml editate il file inserendo un range di indirizzi IP consentiti al loadbalancer metallb, questo range deve appartenere alla vostra rete docker, nel mio caso ho editato solo questa parte del file lasciando il resto invariato\nspec: addresses: - 172.18.255.200-172.18.255.250 tradotto, metal-lb ogni volta che voi creerete un service di tipo LoadBalncer dentro Kubernetes, assegnerà un IP estrapolato da quel range, come potete vedere io ho assegnato 51 indirizzi disponibili. Applichiamo la configurazione eseguendo il seguente comando\nsudo kubectl apply -f metallb-config.yaml Adesso avete a disposizione tutti gli elementi per provare, testare e utilizzare un cluster Kubernetes. La guida termina quì, spero sia stata semplice e chiara da seguire!\nPs. immagine in copertina creata da Dall-E tramite Microsoft Co-pilot (peccato per il testo :P ) sta AI è talmente intelligente che ha pure la dislessia! LOL\n","permalink":"https://marcofanuntza.it/posts/proviamo-kubernetes-con-kind/","summary":"\u003cp\u003e\u003cstrong\u003eProviamo Kubernetes con Kind\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eQuesta guida è indicata per tutti coloro che hanno esigenza di interagire con un cluster Kubernetes per meri scopi di test, conoscenza e sviluppo utilizzando una workstation o notebook con risorse limitate.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKind (Kubernetes IN Docker)\u003c/strong\u003e è uno strumento open-source progettato per semplificare la creazione e la gestione di cluster Kubernetes locali utilizzando container Docker come nodi del cluster.\u003c/p\u003e\n\u003cp\u003eEcco alcune caratteristiche chiave di kind:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInstallazione Semplificata:\u003c/strong\u003e kind semplifica notevolmente il processo di installazione di Kubernetes su una macchina locale, consentendo agli sviluppatori di creare rapidamente e facilmente cluster Kubernetes per scopi di sviluppo o test.\u003c/p\u003e","title":"Proviamo Kubernetes con Kind"},{"content":"Automatic for the people è un album dei R.E.M. mi è venuto in mente quando ho pensato che Ansible è un prodotto di \u0026ldquo;automation\u0026rdquo; IT.\nAnsible è una potente e flessibile piattaforma di automazione IT progettata per semplificare e automatizzare una vasta gamma di compiti, processi e operazioni legate all\u0026rsquo;infrastruttura e allo sviluppo del software.\nDi seguito alcuni aspetti salienti:\nAnsible si distingue per la sua facilità d\u0026rsquo;uso, grazie a una sintassi dichiarativa basata su YAML è accessibile anche a chi ha una conoscenza limitata della programmazione.\nAnsible opera tramite connessioni SSH, eliminando la necessità di installare agenti o client aggiuntivi sulle macchine di destinazione.\nAnsible consente di definire la configurazione di un\u0026rsquo;intera infrastruttura (Infrastracture as a Code) attraverso un semplice file di testo, migliorandone riproducibilità e versioning.\nOltre alla gestione delle configurazioni può automatizzare una vasta gamma di operazioni inclusi il rilascio di applicazioni, il monitoraggio e la scalabilità.\nAnsible è progettato per funzionare su diverse piattaforme e sul cloud, offrendo flessibilità e portabilità.\nAnsible inoltre è in grado di gestire ambienti di varie dimensioni, rendendolo adatto per piccoli progetti fino a progetti più articolati e complessi.\nNel mio ambito lavorativo Ansible era da un pò che ne sentivo parlare ma personalmente non avevo mai avuto modo di utilizzarlo, con questo post sul mio blog unisco l\u0026rsquo;utile al dilettevole cogliendo l\u0026rsquo;occasione per conoscerlo meglio!\nPartiamo dalla basi prima di tutto installandolo, eseguirò i test su una VM con distribuzione Ubuntu 22-04, ansible e i relativi moduli python verranno installati tramite APT\n#Installo ansible sudo add-apt-repository --yes --update ppa:ansible/ansible sudo apt install ansible-core #Verifico la versione installata sudo ansible --version ansible [core 2.15.8] config file = /etc/ansible/ansible.cfg configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python3/dist-packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/bin/ansible python version = 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (/usr/bin/python3) jinja version = 3.0.3 libyaml = True Adesso che Ansible è installato dobbiamo avere a disposizione dei server o delle VM con cui utilizzarlo, per fare questo io personalmente ho creato 3 istanze LXC sul mio cluster Proxmox, anche in questo caso la distribuzione utilizzata è ubuntu, giusto a titolo d\u0026rsquo;esempio qualsiasi altra distribuzione linux andrebbe bene.\nQuesto è il recap delle macchine che ho a disposizione:\nans-serv-01 192.168.1.149\nans-serv-02 192.168.1.150\nans-serv-03 192.168.1.154\nOra i punti cardine principali di Ansible sono i file di configurazione \u0026ldquo;inventario\u0026rdquo; e \u0026ldquo;playbook\u0026rdquo;\nIl file inventario di Ansible contiene informazioni su tutti gli host che Ansible andrà a gestire, in questo file si ha la possibilità di organizzare gli host in diversi gruppi in base ai loro ruoli o funzioni, come ad esempio web-server, database, server di frontend, oppure possiamo categorizzarli in base al sistema operativo. Il file in questione lo troviamo nel path /etc/ansible ed\u0026rsquo;è chiamato hosts, andando a editarlo scoprirete che in parte è già strutturato per aiutarne la comprensione, basterà quindi eliminare i commenti e adattarlo a piacimento.\nNoi in questo esempio andiamo a editarlo così per questa sola parte\n[webservers] ## alpha.example.org ## beta.example.org 192.168.1.149 192.168.1.150 192.168.1.154 come potete intuire sono gli IP delle macchine elencate in precedenza, le ho inserite nel gruppo \u0026ldquo;webserver\u0026rdquo;. Verifichiamo con il comando seguente\nsudo ansible-inventory --list -y all: children: webservers: hosts: 192.168.1.149: {} 192.168.1.150: {} 192.168.1.154: {} Come annunciato in precedenza Ansible non utilizza client o agent all\u0026rsquo;interno dei server da gestire ma si avvale della sola connessione SSH, per utilizzarla però deve essere in grado di connettersi sui server di destinazione, per fare questo è necessario copiare la chiave SSH dal server Ansible verso i server da gestire. Questo inoltre eviterà di dover utilizzare la passwd per connettersi che andrebbe a inficiare sull\u0026rsquo;automatismo non rendendolo possibile\nIniziamo con creare la chiave sul server Ansible\n#Date sempre invio dopo il comando sudo ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:ZRAsk+qjBjQjpsLdEF39qdUmOa5ARx7QPFr6EEQhbDY root@ubuntu-22-04-lts The key's randomart image is: +---[RSA 3072]----+ | o.+OO. | | . E=..X | | +..oB * + | |.= .. = = B o | |* +.o . S + + | |+. .o. . o . | |.. . . . . | | o . | | . | +----[SHA256]-----+ la chiave pubblica adesso va copiata sui server da gestire, il comando che segue va eseguito su tutti e tre i server, la password andrà inserita la prima volta.\nsudo ssh-copy-id root@192.168.1.149 sudo ssh-copy-id root@192.168.1.150 sudo ssh-copy-id root@192.168.1.154 Verifichiamo il buon esito eseguendo questo comando\nsudo ansible all -m ping -u root 192.168.1.149 | SUCCESS =\u0026gt; { \u0026quot;ansible_facts\u0026quot;: { \u0026quot;discovered_interpreter_python\u0026quot;: \u0026quot;/usr/bin/python3\u0026quot; }, \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } 192.168.1.150 | SUCCESS =\u0026gt; { \u0026quot;ansible_facts\u0026quot;: { \u0026quot;discovered_interpreter_python\u0026quot;: \u0026quot;/usr/bin/python3\u0026quot; }, \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } 192.168.1.154 | SUCCESS =\u0026gt; { \u0026quot;ansible_facts\u0026quot;: { \u0026quot;discovered_interpreter_python\u0026quot;: \u0026quot;/usr/bin/python3\u0026quot; }, \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } Adesso Ansible è pronto!\nDopo il file inventario adesso è il momento di mettere mano al file \u0026ldquo;playbook\u0026rdquo;. Rispetto al file inventario che definisce le macchine nel concetto, passatemi il termine hardware, nel file playbook invece vengono definiti gli aspetti software, quindi si andranno a dichiarare tutti gli elementi software e correlati.\nNel nostro esempio abbiamo deciso di installare Nginx, il nostro primo file playbook quindi sarà: /etc/ansible/nginx-playbook.yml\nEcco il contenuto, è un\u0026rsquo;esempio trovato in rete\n--- - hosts: webservers tasks: - ping: ~ - name: Update APT package manager repositories cache become: true apt: update_cache: yes - name: Upgrade installed packages become: true apt: upgrade: safe - name: Install Nginx web server become: true apt: name: nginx state: latest riuscite a intuire che operazioni eseguirà? dai è semplice e mi raccomando prestate attenzione all\u0026rsquo;identazione del file, è YAML lo sai\u0026hellip;.\nSiamo pronti per provare il nostro primo playbook, per eseguirlo utilizzeremo questo comando semplice e basilare\nsudo ansible-playbook nginx-playbook.yml Ecco l\u0026rsquo;output che riceveremo\nPLAY [webservers] ****************************************************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************************************************* ok: [192.168.1.154] ok: [192.168.1.150] ok: [192.168.1.149] TASK [ping] ************************************************************************************************************************************************************ ok: [192.168.1.154] ok: [192.168.1.150] ok: [192.168.1.149] TASK [Update APT package manager repositories cache] ******************************************************************************************************************* changed: [192.168.1.150] changed: [192.168.1.154] changed: [192.168.1.149] TASK [Upgrade installed packages] ************************************************************************************************************************************** ok: [192.168.1.150] ok: [192.168.1.154] ok: [192.168.1.149] TASK [Install Nginx web server] **************************************************************************************************************************************** changed: [192.168.1.149] changed: [192.168.1.154] changed: [192.168.1.150] PLAY RECAP ************************************************************************************************************************************************************* 192.168.1.149 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.150 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.154 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Grande Giove!\nEntriamo in uno dei server e verifichiamo la presenza di Nginx\nssh root@192.168.1.154 Welcome to Ubuntu 23.10 (GNU/Linux 6.5.11-7-pve x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/pro Last login: Fri Jan 19 15:37:04 2024 from 192.168.1.147 root@ans-serv-03:~# ps -ef | grep nginx root 1181 1 0 15:37 ? 00:00:00 nginx: master process /usr/sbin/nginx -g daemon on; master_process on; www-data 1183 1181 0 15:37 ? 00:00:00 nginx: worker process root 1241 1231 0 15:44 pts/3 00:00:00 grep --color=auto nginx E\u0026rsquo; tutto vero! Eh si è!\nBuon Ansible a tutti! Questo articolo termina quì, è solo un semplice esempio di quanto sia in grado di fare questo grande strumento di automazione, l\u0026rsquo;unico peccato è non averlo utilizzato prima!\n","permalink":"https://marcofanuntza.it/posts/automazione-con-ansible/","summary":"\u003cp\u003eAutomatic for the people è un album dei R.E.M. mi è venuto in mente quando ho pensato che Ansible è un prodotto di \u0026ldquo;automation\u0026rdquo; IT.\u003c/p\u003e\n\u003cp\u003eAnsible è una potente e flessibile piattaforma di automazione IT progettata per semplificare e automatizzare una vasta gamma di compiti, processi e operazioni legate all\u0026rsquo;infrastruttura e allo sviluppo del software.\u003c/p\u003e\n\u003cp\u003eDi seguito alcuni aspetti salienti:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAnsible si distingue per la sua facilità d\u0026rsquo;uso, grazie a una sintassi dichiarativa basata su YAML è accessibile anche a chi ha una conoscenza limitata della programmazione.\u003c/p\u003e","title":"Automazione con Ansible"},{"content":"Rancher come eseguire un reset della password dell\u0026rsquo;utente Admin\nSi può capitare a tutti di dimenticare una password, ad alcuni spesso, ma niente paura possiamo eseguire un reset eseguendo questi semplici comandi che seguono..\nL\u0026rsquo;esempio che segue mostra come eseguire il reset della password dell\u0026rsquo;utente admin di Rancher installato all\u0026rsquo;interno di un cluster Kubernetes, le operazioni in parte sono valide anche nel caso il vostro Rancher fosse stato installato su un semplice container docker.\nPrerequisiti\navere accesso al cluster Kubernetes tramite kubectl avere accesso al server che esegue docker (nel secondo caso) Procedura\nTramite kubectl individuiamo il pod che sta erogando il servizio Rancher\nkubectl get pods -n cattle-system | grep Running rancher-webhook-7476c74c6c-scbvs 1/1 Running 0 42m rancher-7484b7b4c5-jb9dt 1/1 Running 0 42m a questo punto dobbiamo entrare all\u0026rsquo;interno della console del pod rancher-7484b7b4c5-jb9dt, per farlo eseguiremo\nkubectl exec --stdin --tty rancher-7484b7b4c5-jb9dt -n cattle-system -- /bin/bash noterete che il prompt dei comandi sarà cambiato, siamo all\u0026rsquo;interno del pod e possiamo eseguire dei comandi diretti, quello specifico che farà al caso nostro è un semplice \u0026ldquo;reset password\u0026rdquo;\nreset-password New password for default admin user (user-m258d): tQXngoMxYBtugFp4Xcjg dopo aver copiato la password potete uscire dalla console con \u0026ldquo;exit\u0026rdquo;.\nCon questi semplici passaggi la vostra password admin è stata rigenereata e sarete nuovamente in grado di accedere al vostro Rancher con utenza admin!\n","permalink":"https://marcofanuntza.it/posts/reset-password-admin-rancher/","summary":"\u003cp\u003e\u003cstrong\u003eRancher come eseguire un reset della password dell\u0026rsquo;utente Admin\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSi può capitare a tutti di dimenticare una password, ad alcuni spesso, ma niente paura possiamo eseguire un reset eseguendo questi semplici comandi che seguono..\u003c/p\u003e\n\u003cp\u003eL\u0026rsquo;esempio che segue mostra come eseguire il reset della password dell\u0026rsquo;utente admin di Rancher installato all\u0026rsquo;interno di un cluster Kubernetes, le operazioni in parte sono valide anche nel caso il vostro Rancher fosse stato installato su un semplice container docker.\u003c/p\u003e","title":"Reset password utente admin su Rancher"},{"content":"Kubernetes è un sistema di gestione (orchestratore) di container che è diventato di fatto lo standard per distribuire applicazioni containerizzate.\nQuesto perché Kubernetes è potente, affidabile, flessibile e per lo più facile da usare (come no).\nSi facile da utilizzare dopo che si supera il primo scoglio iniziale..\nIo personalmente ho avuto difficoltà nel visualizzare mentalmente il cluster e tutti i componenti che ne facevano parte utilizzando solo gli strumenti della riga di comando finché non ho familiarizzato con la sua struttura.\nPer fare ciò è stato di grandissimo aiuto (per me) l\u0026rsquo;utilizzo di Rancher!\nCon questo articolo spero di aiutare tutti gli interessati che vogliono conoscere e sperimentare Kubernetes, il risultato finale di questa guida vi permetterà di avere una sorta di laboratorio sulla vostra workstation/server e toccare con mano un cluster Kubernetes.\nPer raggiungere le scopo verranno installati i seguenti elementi:\nDocker: Il sistema più diffuso per gestire container\nKubectl: Strumento a riga di comando utilizzato per controllare il cluster Kubernetes\nHelm: Gestore di pacchetti per Kubernetes. Consente di installare, aggiornare e gestire applicazioni su un cluster Kubernetes.\nK3d: k3d è un progetto guidato dalla community, supportato da Rancher (SUSE). È un wrapper per eseguire k3s in Docker.\nK3s: È una distribuzione Kubernetes pronta per la produzione, molto leggera, sviluppata da Rancher.\nRancher: Banalmente potrebbe essere considerata una GUI per Kubernetes ma fa molto di più! Permette di gestire e configurare più cluster Kubernetes da un unico punto di controllo.\nPremessa\navere a disposizione una workstation o server con distribuzione Linux (si potrebbe utilizzare anche WSL2 su Windows ma non lo tratteremo in questo articolo) accesso alla rete per scaricare pacchetti Procedura parte 1\nQui procediamo con installazione degli elementi Docker, K3D, Helm e Kubectl\nDocker\n# Aggiungiamo la chiave **GPG key** ufficiale del repository Docker sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \u0026quot;deb [arch=\u0026quot;$(dpkg --print-architecture)\u0026quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \u0026quot;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026quot;$VERSION_CODENAME\u0026quot;)\u0026quot; stable\u0026quot; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io K3D\nsudo wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash Helm\nsudo curl -s https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash Kubectl\nsudo curl -LO \u0026quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x kubectl sudo mv kubectl /usr/local/bin Procedura parte 2\nAdesso procederemo con la creazione del cluster Kubernetes tramite K3D utilizzando K3S e infine completeremo installando Rancher tramite Helm.\nCreazione cluster Kubernetes\nsudo k3d cluster create k8s-test-rancher -p \u0026quot;8900:30080@agent:0\u0026quot; -p \u0026quot;8901:30081@agent:0\u0026quot; -p \u0026quot;8902:30082@agent:0\u0026quot; --agents 2 Il comando andrà a creare un cluster chiamato \u0026ldquo;k8s-test-rancher\u0026rdquo; con 3 porte esposte: 30080, 30081 e 30082. Verranno mappate rispettivamente alle porte 8900, 8901 e 8902 della workstation o server che state utilizzando. Il cluster saà composto da 1 nodo master e 2 nodi worker.\nOra facciamo in modo che Kubectl conosca il file di configurazione del cluster eseguendo il seguente comando\nsudo k3d kubeconfig merge k8s-test-rancher --kubeconfig-switch-context Eseguiamo ora una prima verifica della presenza del cluster e dei nodi elencati\nsudo kubectl get nodes Se tutto procede come dovrebbe dovreste avere lo stesso risultato di sotto\nNAME STATUS ROLES AGE VERSION k3d-k8s-test-rancher-server-0 Ready control-plane,master 36s v1.27.4+k3s1 k3d-k8s-test-rancher-agent-0 Ready \u0026lt;none\u0026gt; 32s v1.27.4+k3s1 k3d-k8s-test-rancher-agent-1 Ready \u0026lt;none\u0026gt; 32s v1.27.4+k3s1 Adesso siamo pronti per l\u0026rsquo;ultimo passaggio fondamentale, installare Rancher tramite Helm\nsudo helm repo add rancher-latest https://releases.rancher.com/server-charts/latest sudo helm install rancher rancher-latest/rancher --namespace cattle-system --create-namespace --set ingress.enabled=false --set tls=external --set replicas=1 Nel frattempo che Helm completerà l\u0026rsquo;installazione, se siete cusiosi potete interagire con il cluster e verificare cosa stia installando\nsudo kubectl get deployment -n cattle-system sudo kubectl get pods -n cattle-system A questo punto dobbiamo metterci nelle condizioni di poter raggiungere Rancher sulla propria web-gui, per farlo è necessario creare un NodePort. Create questo file yaml\nsudo vi rancher.yaml al suo interno incollate questa dichiarazione, che sostanzialmente eseguirà un match con le porte settate in precedenza\napiVersion: v1 kind: Service metadata: labels: app: rancher name: ranchernodeport namespace: cattle-system spec: ports: - name: http nodePort: 30080 port: 80 protocol: TCP targetPort: 80 - name: https nodePort: 30081 port: 443 protocol: TCP targetPort: 443 selector: app: rancher type: NodePort Attenzione è importante sia identato correttamente, dovete rispettare gli spazi, yaml non perdona, lo scoprirete presto :D\nAttiviamo il NodePort eseguendo il comando\nsudo kubectl apply -f rancher.yaml Adesso potete utilizzare il vostro browser e chiamare la seguente url\nhttps://vostroip:8901/dashboard/auth/login Non date importanza al warning sul certificato è perche utilizza un self signed, andate avanti e vi ritroverete la pagina web di Rancher, vi verrà suggerito come recuperare la passwd seguite le indicazioni con il comando kubectl\nBenvenuti su Rancher il vostro cluster Kubernetes è pronto! la guida termina quì, spero sia stata semplice e chiara da seguire!\nps. immagine in copertina creata da Dall-E tramite co-pilot\n","permalink":"https://marcofanuntza.it/posts/proviamo-kubernetes-e-rancher-con-k3d/","summary":"\u003cp\u003eKubernetes è un sistema di gestione (orchestratore) di container che è diventato di fatto lo standard per distribuire applicazioni containerizzate.\u003c/p\u003e\n\u003cp\u003eQuesto perché Kubernetes è potente, affidabile, flessibile e per lo più facile da usare (come no).\u003c/p\u003e\n\u003cp\u003eSi facile da utilizzare dopo che si supera il primo scoglio iniziale..\u003c/p\u003e\n\u003cp\u003eIo personalmente ho avuto difficoltà nel visualizzare mentalmente il cluster e tutti i componenti che ne facevano parte utilizzando solo gli strumenti della riga di comando finché non ho familiarizzato con la sua struttura.\u003c/p\u003e","title":"Proviamo Kubernetes con Rancher"},{"content":"Iniziamo con capire che cos\u0026rsquo;è K3D e non confondiamolo con K3S\nK3D\nK3D è un \u0026ldquo;wrapper\u0026rdquo; che come scrive Wikipedia \u0026ldquo;è un\u0026rsquo;avvolgitore, un modulo software che ne \u0026ldquo;riveste\u0026rdquo; un altro\u0026rdquo; Si la traduzione dall\u0026rsquo;inglese non è felicissima, in questo caso specifico consente di eseguire K3S, che è la distribuzione minimale di Kubernetes sviluppata da Rancher Labs, all\u0026rsquo;interno di Docker.\nIn altre parole, K3D semplifica la creazione e la gestione di cluster Kubernetes leggeri e portatili che utilizzano K3S, rendendo il processo più agevole, specialmente per coloro che sviluppano in locale utilizzando tecnologie Kubernetes.\nÈ importante notare che K3D è un progetto guidato dalla community, non è un prodotto ufficiale di Rancher (SUSE) che ha creato e mantiene K3S. Tuttavia, K3D offre un\u0026rsquo;opzione comoda e flessibile per coloro che desiderano utilizzare K3S all\u0026rsquo;interno di container Docker.\nPrerequisiti\navere a disposizione un server o una VM con distribuzione linux (si può installare anche su Windows o Mac ma non lo tratteremo quì) accesso alla rete per scaricare files curl e/o wget installati avere docker installato, per installarlo vi ricordo un precedente articolo QUI Procedura\nPer installare K3D sostanzialmente si tratterà semplicemente di scaricare uno script bash che si occuperà in totale autonomia dell\u0026rsquo;installazione, abbiamo due alternative, utilizzare curl oppure wget, c\u0026rsquo;è da dire che lo script utilizzerà curl per scaricare i files quindi curl dovete installarlo comunque.\nsudo apt install curl -y sudo wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash oppure tramite curl\nsudo curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash Attendete che lo script completi e poi verificatene il risultato con\nk3d version k3d version v5.6.0 k3s version v1.27.4-k3s1 (default) Ora che K3D è installato possiamo provare a creare il nostro primo cluster Kubernetes (K3S) eseguendo questo comando\nsudo k3d cluster create mycluster Attendete il completamento, nel mio caso ha completato in poco più di 30 secondi\nINFO[0000] Prep: Network INFO[0000] Created network 'k3d-mycluster' INFO[0000] Created image volume k3d-mycluster-images INFO[0000] Starting new tools node... INFO[0001] Creating node 'k3d-mycluster-server-0' INFO[0001] Pulling image 'ghcr.io/k3d-io/k3d-tools:5.6.0' INFO[0003] Pulling image 'docker.io/rancher/k3s:v1.27.4-k3s1' INFO[0003] Starting Node 'k3d-mycluster-tools' INFO[0011] Creating LoadBalancer 'k3d-mycluster-serverlb' INFO[0012] Pulling image 'ghcr.io/k3d-io/k3d-proxy:5.6.0' INFO[0017] Using the k3d-tools node to gather environment information INFO[0017] HostIP: using network gateway 172.18.0.1 address INFO[0017] Starting cluster 'mycluster' INFO[0017] Starting servers... INFO[0017] Starting Node 'k3d-mycluster-server-0' INFO[0022] All agents already running. INFO[0022] Starting helpers... INFO[0022] Starting Node 'k3d-mycluster-serverlb' INFO[0028] Injecting records for hostAliases (incl. host.k3d.internal) and for 2 network members into CoreDNS configmap... INFO[0030] Cluster 'mycluster' created successfully! INFO[0030] You can now use it like this: kubectl cluster-info Ora come suggerisce l\u0026rsquo;ultima riga dell\u0026rsquo;output, per interagire con il cluster dobbiamo utilizzare kubectl, installiamolo quindi\nsudo curl -LO \u0026quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x kubectl sudo mv kubectl /usr/local/bin Dobbiamo fare in modo che kubectl conosca il file di configurazione del cluster\nsudo k3d kubeconfig merge mycluster --kubeconfig-switch-context Adesso siamo pronti per interagire con il cluster\nsudo kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k3d-mycluster-server-0 Ready control-plane,master 12m v1.27.4+k3s1 172.18.0.2 \u0026lt;none\u0026gt; K3s dev 5.15.0-91-generic containerd://1.7.1-k3s1 Come potrete notare il cluster è composto da un unico nodo K3S, ma questo sarà comunque sufficiente per eseguire tutti i test e le esigenze di sviluppo.\nSia comunque ben chiaro che questo è solo un esempio, K3D tramite ulteriori opzioni e file di configurazione vi permette di creare cluster con più nodi. QUI la documentazione ufficiale.\nContinuando a curiosare potete vedere cosa ha installato K3D\nsudo kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 20m kube-system kube-dns ClusterIP 10.43.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 20m kube-system metrics-server ClusterIP 10.43.151.46 \u0026lt;none\u0026gt; 443/TCP 20m kube-system traefik LoadBalancer 10.43.61.96 172.18.0.2 80:32526/TCP,443:32411/TCP 19m sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-957fdf8bc-g6vsb 1/1 Running 0 20m kube-system coredns-77ccd57875-h6krb 1/1 Running 0 20m kube-system metrics-server-648b5df564-nn5kk 1/1 Running 0 20m kube-system helm-install-traefik-crd-52bfs 0/1 Completed 0 20m kube-system helm-install-traefik-nzmhv 0/1 Completed 1 20m kube-system svclb-traefik-caab8633-t7m98 2/2 Running 0 19m kube-system traefik-64f55bb67d-gnvf8 1/1 Running 0 19m Personalmente conosco anche un altra alternativa per creare velocemente un cluster kubernetes, forse scriverò un prossimo articolo, l\u0026rsquo;alternativa cmq è kind\nAggiornamento Ho scritto un\u0026rsquo;articolo specifico su Kind QUI\n","permalink":"https://marcofanuntza.it/posts/come-installare-k3d/","summary":"\u003cp\u003eIniziamo con capire che cos\u0026rsquo;è \u003cstrong\u003eK3D\u003c/strong\u003e e non confondiamolo con \u003cstrong\u003eK3S\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eK3D\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eK3D è un \u0026ldquo;wrapper\u0026rdquo; che come scrive \u003ca href=\"https://it.wikipedia.org/wiki/Wrapper\"\u003eWikipedia\u003c/a\u003e \u0026ldquo;è un\u0026rsquo;avvolgitore, un modulo software che ne \u0026ldquo;riveste\u0026rdquo; un altro\u0026rdquo; Si la traduzione dall\u0026rsquo;inglese non è felicissima, in questo caso specifico consente di eseguire K3S, che è la distribuzione minimale di Kubernetes sviluppata da Rancher Labs, all\u0026rsquo;interno di Docker.\u003c/p\u003e\n\u003cp\u003eIn altre parole, K3D semplifica la creazione e la gestione di cluster Kubernetes leggeri e portatili che utilizzano K3S, rendendo il processo più agevole, specialmente per coloro che sviluppano in locale utilizzando tecnologie Kubernetes.\u003c/p\u003e","title":"Come installare K3D"},{"content":"Premessa\nQuesta guida mostra i comandi da eseguire per creare un template di una VM da utilizzare su Proxmox, la distro utilizzata è Ubuntu e l\u0026rsquo;immagine sarà una versione specifica per il cloud.\nLe Immagini Cloud sono piccole immagini certificate e pronte per il cloud, hanno Cloud Init preinstallato e pronto per accettare una Cloud Config.\nI comandi verranno tutti eseguiti da shell all\u0026rsquo;interno di un nodo Proxmox\nProcedura\nIniziamo scaricando l\u0026rsquo;immagine Ubuntu dalla pagina specifica Ubuntu Cloud Images per questa guida utilizzeremo Ubuntu Server 24.04 LTS (Noble Numbat)\nwget https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img Creiamo la VM che diventerà la base per il successivo template\nqm create 2000 --memory 2048 --core 2 --name ubuntu-cloud-noble --net0 virtio,bridge=vmbr0 Importiamo l\u0026rsquo;immagine precedentemente scaricata sul volume locale mettendola a disposizione della VM\nqm importdisk 2000 noble-server-cloudimg-amd64.img local-lvm Attendiamo il trasferimento e poi procederemo con collegare il nuovo disco alla VM come un\u0026rsquo;unità SCSI sul controller SCSI\nqm set 2000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-2000-disk-0 Adesso aggiungiamo il cloud-init drive, questo è una parte indispensabile per la creazione del template, permette di cambiare la configurazione dell\u0026rsquo;immagine ogni volta che verrà utilizzato il template\nqm set 2000 --ide2 local-lvm:cloudinit Rendiamo l\u0026rsquo;unità cloud-init avviabile e limitiamo i permessi al solo BIOS\nqm set 2000 --boot c --bootdisk scsi0 Aggiungiamo infine serial console\nqm set 2000 --serial0 socket --vga serial0 ATTENZIONE a questo punto non avviate la VM appena creata, siamo pronti per creare il template eseguendo quest\u0026rsquo;ultimo semplice comando\nqm template 2000 Renamed \u0026quot;vm-2000-disk-0\u0026quot; to \u0026quot;base-2000-disk-0\u0026quot; in volume group \u0026quot;pve\u0026quot; Logical volume pve/base-2000-disk-0 changed. WARNING: Combining activation change with other commands is not advised. Adesso avete a disposizione un template che vi permetterà di generare N immagini Ubuntu senza il bisogno di dover ogni volta rieseguire una nuova installazione, insomma è un template!\nChiudiamo la guida con un semplice esempio creando una nuova VM partendo dal template appena creato\nqm clone 2000 110 --name ubuntu-noble-01 --full Verificando possiamo notare la VM appena creata con ID 110 e nome ubuntu-noble-01\nqm list VMID NAME STATUS MEM(MB) BOOTDISK(GB) PID 103 haos11.2 stopped 4096 32.00 0 110 ubuntu-noble-01 stopped 2048 3.50 0 2000 ubuntu-cloud-noble stopped 2048 3.50 0 Noterete anche che l\u0026rsquo;immagine ha un disco di soli 3.5GB, nessun problema in base alle vostre esigenze potete ridimensionare il disco eseguendo questo successivo comando\nqm resize 110 scsi0 15G Size of logical volume pve/vm-110-disk-0 changed from 3.50 GiB (896 extents) to 15.00 GiB (3840 extents). Logical volume pve/vm-110-disk-0 successfully resized. Adesso siamo pronti per far partire la nostra nuova VM creata dal template.. ma manca qualcosa, spero i più attenti ci abbiano fatto caso, che user e passwd dobbiamo utilizzare per il login?\nE\u0026rsquo; qui che entra in gioco cloud-init, aprite la web-gui di Proxmox e posizionatevi sulle voci di menù relative alla VM, cliccate sulla voce Cloud-init e noterete le voci che si possono editare, modificatele e provate a far partire la VM.\nPer l\u0026rsquo;accesso tramite SSH da remoto dovete utilizzare direttamente la vostra chiave SSH, si potete inserirla direttamente come sopra.\nLa guida termina qui, se velete utilizzare una distribuzione diversa da Ubuntu niente vi vieta di farlo, la guida va semplicemente adattata utilizzando un\u0026rsquo;immagine diversa.\n","permalink":"https://marcofanuntza.it/posts/come-creare-template-ubuntu-su-proxmox/","summary":"\u003cp\u003e\u003cstrong\u003ePremessa\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eQuesta guida mostra i comandi da eseguire per creare un template di una VM da utilizzare su Proxmox, la distro utilizzata è Ubuntu e l\u0026rsquo;immagine sarà una versione specifica per il cloud.\u003c/p\u003e\n\u003cp\u003eLe Immagini Cloud sono piccole immagini certificate e pronte per il cloud, hanno Cloud Init preinstallato e pronto per accettare una Cloud Config.\u003c/p\u003e\n\u003cp\u003eI comandi verranno tutti eseguiti da shell all\u0026rsquo;interno di un nodo Proxmox\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProcedura\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIniziamo scaricando l\u0026rsquo;immagine Ubuntu dalla pagina specifica \u003ca href=\"https://cloud-images.ubuntu.com/\"\u003eUbuntu Cloud Images\u003c/a\u003e per questa guida utilizzeremo Ubuntu Server 24.04 LTS (Noble Numbat)\u003c/p\u003e","title":"Come creare template Ubuntu su Proxmox"},{"content":"Come installare Docker e Docker compose su Ubuntu\nQuesta guida elenca passo per passo la procedura da seguire per installare docker, docker compose e containerd su distribuzione Ubuntu.\nPrerequisiti:\nserver o workstation con distribuzione ubuntu accesso alla rete per scaricare i pacchetti Procedura\nTutti i comandi verranno eseguiti da terminale, se in precedenza avevate già provato un\u0026rsquo;installazione di Docker sarebbe opportuno rimuoverla eseguendo il comando che segue:\nsudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras adesso si può procedere con l\u0026rsquo;installazione, si parte prima di tutto aggiungendo il repository ufficiale Docker\n# Aggiungiamo la chiave **GPG key** ufficiale del repository Docker: sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg aggiungiamo il repository ufficiale Docker al sistema APT:\necho \\ \u0026quot;deb [arch=\u0026quot;$(dpkg --print-architecture)\u0026quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \u0026quot;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026quot;$VERSION_CODENAME\u0026quot;)\u0026quot; stable\u0026quot; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update qui il comando per installazione dei pacchetti necessari:\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin verifichiamo le versioni installate di Docker e Docker compose:\ndocker -v docker compose a questo punto possiamo considerare completata l\u0026rsquo;installazione, in via opzionale ci resta solamente abilitare il nostro user per utilizzo del comando docker senza il bisogno di utilizzare ogni volta sudo, per farlo aggiungiamo semplicemente lo user al gruppo docker\nsudo usermod -aG docker $USER that\u0026rsquo;all folks!\n","permalink":"https://marcofanuntza.it/posts/come-installare-docker-e-docker-compose-su-ubuntu/","summary":"\u003cp\u003eCome installare Docker e Docker compose su Ubuntu\u003c/p\u003e\n\u003cp\u003eQuesta guida elenca passo per passo la procedura da seguire per installare docker, docker compose e containerd su distribuzione Ubuntu.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePrerequisiti:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eserver o workstation con distribuzione ubuntu\u003c/li\u003e\n\u003cli\u003eaccesso alla rete per scaricare i pacchetti\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eProcedura\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTutti i comandi verranno eseguiti da terminale, se in precedenza avevate già provato un\u0026rsquo;installazione di Docker sarebbe opportuno rimuoverla eseguendo il comando che segue:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eadesso si può procedere con l\u0026rsquo;installazione, si parte prima di tutto aggiungendo il repository ufficiale Docker\u003c/p\u003e","title":"Come installare Docker e Docker Compose su Ubuntu"},{"content":"Questo articolo continua il log partito da QUI dove spiegavo la scelta e il perchè .\nI componenti che attendevo sono arrivati, oltre la lista iniziale ho apportato alcune integrazioni aggiungendo una scheda PCI che di fatto mette a disposizione due porte SATA III aggiuntive. A questa scheda sono direttamente connessi i due dischi da 2,5 pollici, 2TB cadauno. Altra integrazione è un ulteriore banco di ram da 8GB.\nLa prima installazione è abbastanza semplice, se avete già avuto modo di installare una distro linux da pendrive USB sarà una passeggiata. Per creare la pendrive USB bootabile ho utilizzato il software Balena Etcher e la ISO di Truenas Core potete ovviamente trovarla sui repository ufficiali. La procedura guidata vi chiederà su quale disco installare l\u0026rsquo;OS, imposterete una password e successivamente sarà il turno della rete, finito! nel mio caso ha completato l\u0026rsquo;installazione in pochi minuti. A questo link comunque potete seguire la pagina ufficiale con immagini passo per passo.\nOra potete anche scollegare monitor e tastiera dal server, tutto il resto potrà essere eseguito dalla web-gui di Truenas!\nIn questa galleria di immagini seguente potete vedere la dashboard iniziale che presenta con Truens CORE\n.\nPrime operazioni basilari che ho eseguito su TrueNAS CORE\nImpostato default gateway: questo è indispensabile per cercare/installare aggiornamenti e installare plugin jail Impostato DNS: ovviamente, no dns no party Creato primo pool: primo passaggio per inizializzare lo storage a disposizione Le mie esigenze\nIn aggiornamento…..\n","permalink":"https://marcofanuntza.it/2024/01/13/il-mio-nuovo-nas-con-truenas-part2/","summary":"\u003cp\u003eQuesto articolo continua il log partito da \u003ca href=\"https://marcofanuntza.it/2024/01/01/il-mio-nuovo-nas-con-truenas/\"\u003eQUI\u003c/a\u003e dove spiegavo la scelta e il perchè .\u003c/p\u003e\n\u003cp\u003eI componenti che attendevo sono arrivati, oltre la lista iniziale ho apportato alcune integrazioni aggiungendo una scheda PCI che di fatto mette a disposizione due porte SATA III aggiuntive. A questa scheda sono direttamente connessi i due dischi da 2,5 pollici, 2TB cadauno. Altra integrazione è un ulteriore banco di ram da 8GB.\u003c/p\u003e\n\u003cp\u003eLa prima installazione è abbastanza semplice, se avete già avuto modo di installare una distro linux da pendrive USB sarà una passeggiata. Per creare la pendrive USB bootabile ho utilizzato il software \u003cstrong\u003eBalena Etcher\u003c/strong\u003e e la ISO di Truenas Core potete ovviamente trovarla sui repository ufficiali. La procedura guidata vi chiederà su quale disco installare l\u0026rsquo;OS, imposterete una password e successivamente sarà il turno della rete, finito! nel mio caso ha completato l\u0026rsquo;installazione in pochi minuti. A questo \u003ca href=\"https://www.truenas.com/blog/how-to-install-truenas-core/\"\u003elink\u003c/a\u003e  comunque potete seguire la pagina ufficiale con immagini passo per passo.\u003c/p\u003e","title":"Il mio nuovo NAS con Truenas part II"},{"content":"Introduzione:\nQuando ho deciso di riscrivere sul blog il cms scelto inizialmente era stato Wordpress, per ambito lavorativo avevo già gestito server wordpress decine di volte, avevo avuto anche un\u0026rsquo;esperienza come writer assiduo sul defunto blog actioncamitalia, la scelta quindi si era basata esclusivamente sull\u0026rsquo;esperienza passata. Dopo un pò mi sono accorto però che per le mie esigenze, per le esigenze di questo blog specifico, l\u0026rsquo;utilizzo delle risorse necessarie per worpress erano sprecate, insomma non ne avevo bisogno.\nDa qui è partita la ricerca su un nuovo CMS per il mio blog.\nCi sono molte piattaforme di blogging. Essendo un sistemista, le mie esigenze per una piattaforma di blogging potrebbero differire da quelle della maggior parte dei blogger. Io vorrei che il mio blog sia:\nFacile da mantenere per quanto riguarda gli aggiornamenti del software Semplice nelle sue funzionalità Facile per me da configurare Trasparente su ciò che sta accadendo sotto il cofano Che cos\u0026rsquo;è Hugo?\nHugo è un framework open source per la generazione di siti web statici. Creato utilizzando il linguaggio di programmazione Go (o Golang), Hugo si distingue per la sua velocità straordinaria nella generazione di contenuti statici. Alcune caratteristiche chiave di Hugo includono:\nVelocità: Grazie alla sua implementazione in Go, Hugo è notevolmente veloce nella generazione di siti web statici, rendendo il processo efficiente e immediato.\nSemplicità: Hugo è progettato per essere facile da usare e comprendere. La sua struttura chiara e la documentazione completa facilitano la creazione e la gestione di siti web.\nFlessibilità: Supporta temi e layout personalizzabili, consentendo agli sviluppatori di adattare l\u0026rsquo;aspetto e la struttura del sito secondo le proprie esigenze.\nGenerazione di Contenuti Statici: Hugo crea siti web completamente statici, eliminando la necessità di un server di backend. Ciò li rende sicuri, facili da distribuire e veloci da caricare.\nInstallare Hugo\nSu qualsiasi distribuzione linux è abbastanza semplice installare Hugo, i vari snap, apt e yum hanno nei loro repository i pacchetti necessari, ma c\u0026rsquo;è da dire che spesso non sono aggiornati. Il mio consiglio è scaricarvi il pacchetto più recente e installarlo a manina.\nNel mio caso specifico ho deciso di installare Hugo su un container LXC erogato dal mio cluster Proxmox, distribuzione ho scelto una ubuntu 23-10, dal sito di Hugo ho scaricato l\u0026rsquo;ultima release disponibile del pacchetto deb\nnota. scegliete la versione \u0026ldquo;extended\u0026rdquo;\nwget https://github.com/gohugoio/hugo/releases/download/v0.121.2/hugo_extended_0.121.2_linux-amd64.deb eseguito il comando di installazione:\ndpkg -i hugo_extended_0.121.2_linux-amd64.deb qui il risultato:\nhugo version hugo v0.121.2-6d5b44305eaa9d0a157946492a6f319da38de154+extended linux/amd64 BuildDate=2024-01-05T12:21:15Z VendorInfo=gohugoio Primi Passi\nPrima di tutto vi consiglio di leggere la documentazione di Hugo e consiglio di partire dalla quick-start\nIo non l\u0026rsquo;ho seguita alla lettera ma ho apportato alcune modifiche, procediamo con la creazione del nostro primo sitoweb con Hugo, posizionatevi in un path qualunque sul vostro server e eseguite il seguente comando:\nhugo new site mio-nuovo-blog --format yaml descrivendo nello specifico il comando, la prima parte istruisce hugo per creare un sito, mio-nuovo-blog sarà il nome del sito e l\u0026rsquo;opzione -format yaml farà in modo che i file di configurazione vengano formattati in formato yaml invece che toml, a mio avviso più semplice e intuitivo a prima vista.\nScoprirete che hugo ha creato una nuova directory con il nome del sito e posizionato al suo interno tutti i files necessari, dovreste ritrovarvi in questa simile situazione\nIl passo successivo sarà scegliere e abiltare un tema da utilizzare, date un\u0026rsquo;occhiata sul sito ufficiale e scegliete quello che preferite. Sono quasi tutti ben documentati e la procedura per attivarli è quasi sempre la stessa, sostanzialmente va utilizzato git e scaricato il repository all\u0026rsquo;interno del path creato in precedenza da Hugo.\nIo ho scelto il tema PaperMod e l\u0026rsquo;ho abilitato come submodulo di git con i seguenti comandi:\ngit init questo vi servirà anche nel caso voi decidiate di versionare il vostro sito su GitHub, procediamo poi con:\ngit submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive # needed when you reclone your repo (submodules may not get cloned automatically) Ora troverete il tema installato e a disposizione per il vostro nuovo sito, i files del tema sono posizionati all\u0026rsquo;interno del path themes/PaperMod.\nIl file che editerete principalmente sarà config.yaml, questo è di fatto il punto principale di controllo per il vostro sito Hugo.\nMa finiamo con un\u0026rsquo;ultimo comando poi lascerò a voi il bello di configurarvi il vostro sito Hugo!\nCome creo il mio primo post?\nBene niente di più semplice, per farlo utilizziamo nuovamente un comando hugo\nhugo new content posts/il-mio-primo-post.md questo comando creerà un nuovo post vuoto e come potrete intuire sarà sul path content/posts/\nA questo punto credo sia arrivato il momento di lasciarvi scoprire come andare avanti con Hugo in base alle vostre esigenze, se volete comunque dare un\u0026rsquo;occhiata al codice del mio blog, questo stesso che sta leggendo, vi lascio il link al mio repository su GitHub\nPeace \u0026amp; Love!\n","permalink":"https://marcofanuntza.it/2024/01/12/ho-scelto-hugo/","summary":"\u003cp\u003e\u003cstrong\u003eIntroduzione:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eQuando ho deciso di riscrivere sul blog il cms scelto inizialmente era stato Wordpress, per ambito lavorativo avevo già gestito server wordpress decine di volte, avevo avuto anche un\u0026rsquo;esperienza come writer assiduo sul defunto blog actioncamitalia, la scelta quindi si era basata esclusivamente sull\u0026rsquo;esperienza passata.\nDopo un pò mi sono accorto però che per le mie esigenze, per le esigenze di questo blog specifico, l\u0026rsquo;utilizzo delle risorse necessarie per worpress erano sprecate, insomma non ne avevo bisogno.\u003c/p\u003e","title":"Ho scelto Hugo!"},{"content":"Che ne sarà di noi mi verrebbe da dire.. oggi stavo continuando con il tuning del nuovo tema Hugo che sto utilizzando e ho notato che è possibile inserire un logo in homepage, quindi ho avuto la brillante idea di chiedere a bing-chat e alla nuova funzione di Microsoft chiamata Co-Pilot di generare per me un\u0026rsquo;immagine logo con l\u0026rsquo;ausilio della intelligenza artificiale. Come avrete potuto già intuire la situazione mi è leggermente sfuggita di mano.\nCo-pilot è un assistente virtuale basato sull’intelligenza artificiale che ti aiuta a svolgere diverse attività con una semplice chat. Puoi chiedere a Co-pilot di suggerirti codici, migliorare la tua scrittura, fornirti informazioni utili e molto altro. Co-pilot è integrato in vari prodotti Microsoft, come Edge, Windows 11, Word, Excel e Teams.\nMicrosoft come suo solito fare ha acquisito DALL-E che è un algoritmo di intelligenza artificiale capace di generare immagini a partire da descrizioni testuali attraverso la sintografia.\nSe anche voi volete provarlo vi basterà utilizzare il browser nativo di Microsoft Edge, cercate Bing Chat, tra i risultati vi verrà proposto di attivare e provare Co-Pilot. Sulla parte destra della pagina apparirà una finestra dove sarà possibile interagire con Co-Pilot, basterà scrivere:\n\u0026quot;Genera immagine che mostra una gatto con lo sfondo di un bosco incantato, il gatto deve essere terrificante\u0026quot; Che esempio del piffero che ho fatto, comunque tutto questo per farvi capire che il vostro unico limite sarà la fantasia!\nA me stranamente invece è venuta idea di creare immagini con ragazze asiatiche con sullo sfondo città Asiatiche. Sawadee Kaaap!\n","permalink":"https://marcofanuntza.it/2024/01/01/pensare-che-cercavo-un-logo/","summary":"\u003cp\u003eChe ne sarà di noi mi verrebbe da dire.. oggi stavo continuando con il tuning del nuovo tema Hugo che sto utilizzando e ho notato che è possibile inserire un logo in homepage, quindi ho avuto la brillante idea di chiedere a bing-chat e alla nuova funzione di Microsoft chiamata Co-Pilot di generare per me un\u0026rsquo;immagine logo con l\u0026rsquo;ausilio della intelligenza artificiale.\nCome avrete potuto già intuire la situazione mi è leggermente sfuggita di mano.\u003c/p\u003e","title":"E pensare che cercavo un logo.."},{"content":"Quanto vi infastidiscono gli annunci pubblicitari navigando sul web? Ormai ci sono pagine web piene di annunci, pop-up fastidiosissimi che non fanno altro che farci perdere tempo e voglia di visitarne il sito, i quotidiani con le news e le notizie sportive su tutti sono i più scassa bit.\nPer fortuna ci viene in aiuto Pi-Hole!\nPi-hole è un software open-source progettato per il controllo e la gestione della rete orientato nello specifico proprio per combattere la pubblicità e gli annunci correlati, sostanzialmente agisce come un filtro DNS, offrendo funzionalità avanzate per bloccare tutti gli annunci pubblicitari, tracker e tutti i contenuti indesiderati ancor prima che raggiungano i nostri dispositivi connessi alla rete.\nCaratteristiche Principali:\nBlocco degli Annunci: Pi-hole utilizza liste di blocco per intercettare le richieste DNS associate agli annunci pubblicitari, consentendo di eliminare la visualizzazione di annunci indesiderati su tutti i dispositivi connessi alla rete.\nFiltro DNS: Attraverso il blocco delle richieste DNS indesiderate, Pi-hole inoltre impedisce l’accesso a siti malevoli contenenti malware o altri contenuti indesiderati, contribuendo a migliorare la sicurezza della navigazione.\nMonitoraggio delle Prestazioni: Pi-hole fornisce statistiche dettagliate sulle richieste DNS e sui domini bloccati, permettendo agli utenti di monitorare l’attività di rete e l’efficacia dei blocchi.\nInterfaccia Web: Il software è dotato di un’interfaccia utente web che semplifica la configurazione e la gestione. Attraverso questa interfaccia, gli utenti possono monitorare le statistiche, aggiornare liste di blocco e personalizzare le impostazioni.\nListe di Blocco Personalizzabili: Pi-hole consente agli utenti di aggiungere o rimuovere domini dalle liste di blocco in base alle proprie preferenze e esigenze.\nSupporto per IPv6: Il software supporta IPv6, garantendo una copertura completa delle richieste DNS e dei blocchi su entrambi gli standard di indirizzamento IP.\nIntegrazione con DHCP: Pi-hole può fungere anche da server DHCP, semplificando ulteriormente la gestione degli indirizzi IP e la distribuzione delle configurazioni di rete.\nViste le caratteristiche non poteva mancare nella suite di servizi installati sul mio homelab, l’installazione di per sè é molto semplice infatti basterà eseguire un semplice script che vi auto guiderà nelle varie impostazioni\ncurl -sSL https://install.pi-hole.net | bash Io personalmente per ora l’ho installato su un raspberry Pi 3 dove sono già presenti le istanze di Nginx Proxy Manager e il container per la gestione del tunnel cloudflare, scriverò un’articolo specifico su questa macchina in futuro.\nVoi potete installarlo dove meglio credete, un server linux, un container docker o su una VM, up to you! La documentazione è molto chiara e completa per ogni dettaglio specifico non posso che invitarvi a leggerla cliccando QUI\nAttenzione è importante che il server abbia l’indirizzo IP statico, questo non dovrà cambiare mai perché verrà chiamato da tutti gli altri device, se avete il DHCP attivo basterà impostare una reservation.\nEcco l\u0026rsquo;aspetto che ha la comodissima interfaccia WEB Oltre a liberami dalla pubblicità Pi-Hole mi è stato utilissimo anche come DNS locale per il mio homelab\nPer completare il tutto non dovete fare altro che modificare i parametri network sui vostri device facendo in modo che il DNS utilizzato sia l’IP della vostra installazione Pi-Hole, in questo modo ogni richiesta verrà gestita da Pi-Hole e addio alla pubblicità!\nPer chi ne avesse la possibilità la modifica DNS si potrebbe eseguire già a livello del vostro router, basterà editare una sola volta sull’apparato per avere così di conseguenza tutti i device già configurati.\nEvviva l’opensource!\n","permalink":"https://marcofanuntza.it/2024/01/06/grazie-pihole-basta-pubblicita/","summary":"\u003cp\u003eQuanto vi infastidiscono gli annunci pubblicitari navigando sul web? Ormai ci sono pagine web piene di annunci, pop-up fastidiosissimi che non fanno altro che farci perdere tempo e voglia di visitarne il sito, i quotidiani con le news e le notizie sportive su tutti sono i più scassa bit.\u003c/p\u003e\n\u003cp\u003ePer fortuna ci viene in aiuto Pi-Hole!\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/pi-hole-pagina0.webp\" alt=\"Example image\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003ePi-hole è un software open-source progettato per il controllo e la gestione della rete orientato nello specifico proprio per combattere la pubblicità e gli annunci correlati, sostanzialmente agisce come un filtro DNS, offrendo funzionalità avanzate per bloccare tutti gli annunci pubblicitari, tracker e tutti i contenuti indesiderati ancor prima che raggiungano i nostri dispositivi connessi alla rete.\u003c/p\u003e","title":"Grazie a Pi-Hole basta con la pubblicità!"},{"content":"Acquistai il mio primo NAS nel lontano 2011, uno Zyxel NSA320 che nonostante tutto funziona ancora ma il peso degli anni si sente tutto, è ormai fuori supporto da tempo, nessuna opzione per upgrade o mod varie, versione tls obsoleta e opzioni su share NFS inesistenti. Quest’ultimo aspetto mi sta dando problemi con i backup dei container che ho su Proxmox e per questo ho preso la decisione… nuovo NAS sia!\nIl primo aspetto preso in considerazione è: pappa pronta acquistando un prodotto commerciale dove basta inserire i dischi accendere e via, oppure costruirmelo da me?\nDevo ammettere che i nuovi NAS in commercio hanno tutta una serie di features veramente interessanti, sono praticamente dei mini computer e oltre a salvare i dati permettono di installarci vari servizi, il famoso Synology DS220+ ad esempio può essere un nas, un mediacenter, un reverse proxy, erogare virtual machines, erogare container docker, con questi ultimi due aspetti praticamente può fornire molteplici servizi, chi più ne ha più ne metta.\nTutto questo però ovviamente ha un costo (alto a mio avviso) e altro aspetto da tenere in conto è che la maggior parte delle volte dovete aggiungerci i dischi, money! cantavano i Pink Floyd.\nLa mia curiosità batte la pigrizia della pappa pronta, quindi ho optato per costruirmelo da me, poi diciamocelo chiaro il bello è proprio quello! e un articolo descrivendo quanto fosse bello il Synology non l’avrei poi mai scritto.\nLa scelta è semplice, trovare un vecchio desktop pc, uno di quelli SFF (small form factor), su ebay e amazon ne trovate a bizzeffe ricondizionati, garantiscono prestazioni adeguate e consumi ridotti considerando che un NAS dovrebbe rimanere acceso H24.\nTra i tanti modelli, dopo varie analisi, la mia scelta è ricaduta su un Dell OPTIPLEX 3050 SFF, quello che ho trovato io ha una cpu non troppo vecchia Intel i3-7100, 8GB di ram DDR4 espandibile e aspetto scatenante la scelta è la presenza di uno slot M2 per storage nvme, dove ci andrà installato l’OS.\nOk abbiamo la macchina, l’altra decisione da prendere è: che tipo di dischi utilizzare? le opzioni sono due, dischi super performanti SSD oppure classici e intramontabili HDD rotativi. Il prezzo dei dischi SSD è sceso tanto negli ultimi anni, ormai sono de facto i dischi da utilizzare su workstation e notebook, non sarebbero male da utilizzare anche sui NAS, c’è però un ma, i dischi SSD hanno una data di “scadenza” in base data dal numero di scritture eseguite, a questo si aggiunge un’aspetto che scoprirete dopo relativo al filesystem che verrà utilizzato ZFS, che per come funziona andrà sicuramente ad accorciarne ulteriormente la loro longevità.\nQuindi si la mia scelta è stata… HDD rotativi! e saranno da 2,5 più facili da inserire dentro alla macchina SFF e meno energivori rispetto agli HDD classici da 3,5 pollici.\nInfine arriva il software da utilizzare, scelta già fatta come avrete intuito dal titolo stesso dell’articolo, Truenas CORE sarà il software, il cuore e la mente del mio nuovo (vecchio) NAS.\nTrueNAS in versione CORE è una piattaforma di storage open-source basata su FreeBSD, progettata per fornire soluzioni di storage e condivisione di dati scalabili e affidabili. Originariamente conosciuta come FreeNAS, TrueNAS è stata ribattezzata per riflettere la sua evoluzione e le sue funzionalità avanzate. Ecco alcune delle sue caratteristiche principali:\nStorage Unificato: Supporta protocolli come SMB/CIFS, NFS, AFP, iSCSI, S3 e altri, consentendo la centralizzazione della gestione dei dati e l’accesso da diversi sistemi operativi. ZFS File System: Utilizza il file system ZFS, noto per la sua robustezza, gestione avanzata dei dati e funzionalità di snapshot e clone. Virtualizzazione e Containerization: Fornisce funzionalità di virtualizzazione e containerization con supporto per VMware, Hyper-V, Bhyve e Docker. Ridondanza e Alta Disponibilità: Offre opzioni avanzate per la ridondanza e l’alta disponibilità, garantendo la continuità degli accessi ai dati e proteggendo da guasti hardware. Sistema di Archiviazione Condiviso: Configurabile come un sistema di archiviazione condiviso in reti aziendali, consentendo a diversi utenti di accedere e condividere dati in modo sicuro. Backup e Ripristino: Include funzionalità di backup e ripristino, consentendo la creazione di snapshot e la pianificazione di backup automatici per garantire la sicurezza dei dati. Interfaccia Web Intuitiva: Dispone di un’interfaccia web intuitiva che semplifica la gestione e la configurazione del sistema, rendendo accessibili molte funzionalità avanzate. TrueNAS CORE e TrueNAS SCALE sono due varianti della piattaforma di storage TrueNAS, entrambe sviluppate da iXsystems. Ecco alcune differenze chiave tra i due:\nTrueNAS CORE:\nFile System ZFS: TrueNAS CORE sfrutta il file system ZFS per avanzate funzionalità di gestione dati, snapshot e resistenza ai guasti. Stabilità e Affidabilità: È noto per la sua stabilità e affidabilità, consigliato per utilizzi in ambienti critici. Orientato agli Utenti Esperti: TrueNAS CORE è più indicato per utenti esperti e amministratori di sistema che desiderano maggiore controllo sulla configurazione. TrueNAS SCALE:\nBasato su Debian: TrueNAS SCALE ha una base Debian Linux, offrendo maggiore flessibilità nell’integrazione con software basato su Linux. Architettura più Moderna: Progettato con un’architettura moderna e scalabile, adatto a carichi di lavoro su larga scala. Interfaccia Kubernetes: TrueNAS SCALE presenta un’interfaccia Kubernetes nativa, semplificando l’implementazione e la gestione di applicazioni in contenitori. Approccio All-in-One: Pensato come soluzione all-in-one con un modello di implementazione semplificato, adatto anche a utenti meno esperti. App Store: TrueNAS SCALE include un App Store integrato con una varietà di applicazioni e servizi aggiuntivi facilmente installabili e gestibili. In breve, mentre TrueNAS CORE offre stabilità consolidata e controllo avanzato, TrueNAS SCALE si orienta verso una flessibilità moderna, con facilità d’uso e integrazione di Kubernetes. La scelta tra i due dipende dalle esigenze specifiche dell’utente e dell’ambiente.\nIo avendo già un homelab con Proxmox, non ho l’esigenza di avere un ulteriore ambiente che sarebbe stato una sorta di “doppione” con Truenas in versione SCALE e poi a me serve un NAS.\nA questo punto dovrei mostrarvi il risultato…… si però dovrete attendere che mi arrivino i componenti!\nCome diceva il grande JeeG Robot “Miwa lanciami i componenti!”\nEcco clicca QUI per la seconda parte dell\u0026rsquo;articolo\n","permalink":"https://marcofanuntza.it/2024/01/01/il-mio-nuovo-nas-con-truenas/","summary":"\u003cp\u003eAcquistai il mio primo NAS nel lontano 2011, uno Zyxel NSA320 che nonostante tutto funziona ancora ma il peso degli anni si sente tutto, è ormai fuori supporto da tempo, nessuna opzione per upgrade o mod varie, versione tls obsoleta e opzioni su share NFS inesistenti. Quest’ultimo aspetto mi sta dando problemi con i backup dei container che ho su Proxmox e per questo ho preso la decisione… nuovo NAS sia!\u003c/p\u003e","title":"Il mio nuovo NAS con Truenas"},{"content":"Problema\nAbbiamo dimenticato la password di un container LXC in esecuzione su Proxmox e non abbiamo alternative se non quella di resettarla.\nSoluzione\n*Effettuare l’accesso sulla web GUI del vostro cluster Proxmox.\n*Individuate il container LXC per il quale si desidera reimpostare la password e ricordarsi l’ID del container. Ad esempio, se vediamo un container chiamato:\n105 (passwdDimenticata) *105 saràil suo ID, passwdDimenticata sarà il suo nome.\n*Avviare il container nel caso non lo fosse già\n*Ora connettersi via SSH sull’host di Proxmox che sta eseguendo il container (come utente root) o aprite una Shell/Console sulla web GUI di Proxmox, sempre sul nodo che sta eseguendo il container\n*Utilizzare il seguente comando per collegare la sessione al container LXC\nlxc-attach -n 105 *A questo punto ci troveremo all’interno della shell del container e potremo eseguire i comandi, nello specifico per cambiare la password di root\npasswd *Digitare la nuova password, premere il tasto Invio, quindi digitare nuovamente la password e premere nuovamente Invio per confermare la nuova password di root del container. -Un a volta completato, è possibile effettuare l’accesso al container con la nuova password.\n","permalink":"https://marcofanuntza.it/2023/12/31/come-reimpostare-la-password-root-in-un-container-lxc-su-proxmox-ve/","summary":"\u003cp\u003e\u003cstrong\u003eProblema\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAbbiamo dimenticato la password di un container LXC in esecuzione su Proxmox e non abbiamo alternative se non quella di resettarla.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSoluzione\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e*Effettuare l’accesso sulla web GUI del vostro cluster Proxmox.\u003c/p\u003e\n\u003cp\u003e*Individuate il container LXC per il quale si desidera reimpostare la password e ricordarsi l’ID del container. Ad esempio, se vediamo un container chiamato:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e105 (passwdDimenticata) \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e*105 saràil suo ID, passwdDimenticata sarà il suo nome.\u003c/p\u003e\n\u003cp\u003e*Avviare il container nel caso non lo fosse già\u003c/p\u003e","title":"Come reimpostare la password root in un container LXC su Proxmox VE"},{"content":"Spesso può capitare la necessità di dover ridimensionare il disco di un container creato troppo grande o relativamente piccolo, a dire la verità spesso si tende ad aumentare lo spazio disco, vuoi crescita dei logs, aggiunta di nuove funzioni, in questo caso specifico però mostreremo come ridurre lo spazio.\nPer chi come me utilizza Proxmox in ambito domestico dovrà fare subito i conti con la mancanza di una LAN a 10GbE, questo si tradurrà in migrazioni tra i nodi eccessivamente lunghe, la regola quindi deve essere: container piccolo = veloce. Quando si crea un container può succedere di non aver ben chiaro in mente quanto dimensionare lo spazio disco, ma niente paura si ridimensiona!\nProxmox di default quando crea il container utilizza LVM che sta per (in lingua inglese logical volume manager) in italiano gestore logico dei volumi, ringraziamo quindi questa scelta perché ci permetterà di ridimensionare i nostri container con dei semplici passaggi che scopo di questo articolo vi elencherò qui di seguito.\nPrima di tutto come sempre vi consiglio di eseguire un backup del container interessato, terminato il backup saremo pronti a partire.\nPer i comandi che seguono sarà necessario operare da \u0026gt;_Shell, la potete attivare da Proxmox e sarà sul nodo che in quel momento sta eseguendo il container interessato. Io personalmente preferisco operare direttamente da shell ssh ma sarà uguale.\nIniziamo con individuare il volume utilizzato dal container eseguendo questo comando:\nlvdisplay | grep \u0026quot;LV Path\\|LV Size\u0026quot; il risultato sarà simile al seguente; simile non uguale sia ben chiaro 😉\nLV Size 141.23 GiB LV Path /dev/pve/swap LV Size 8.00 GiB LV Path /dev/pve/root LV Size 69.37 GiB LV Path /dev/pve/vm-101-disk-0 LV Size 22.00 GiB dovrete prestare attenzione alle ultime due righe, queste infatti si riferiscono al disco (volume) del container che andremo a modificare, che potete intuire sarà vm-101-disk-0 sul percorso /dev/pve/vm-101-disk-0\nintanto procediamo con spegnere il container, si scusate non lo avevo ancora scritto, queste operazioni vanno eseguite con il container spento, eseguite il comando seguente per spegnere il container\npct stop 101 adesso partiamo con ridimensionare il filesystem del volume, nel nostro caso intento è di farlo diventare da 10GB quindi eseguiamo:\nresize2fs /dev/pve/vm-101-disk-0 10G il risultato ci restituirà:\nresize2fs 1.47.0 (5-Feb-2023) Resizing the filesystem on /dev/pve/vm-101-disk-0 to 2621440 (4k) blocks. The filesystem on /dev/pve/vm-101-disk-0 is now 2621440 (4k) blocks long. a questo punto siamo pronti con il comando che effettivamente ridimensionerà il nostro volume, il comando lvreduce fa parte della suite messa a disposizione da LVM, per i più curiosi c’è sempre il MAN di linux per approfondimenti, che detto in Sardo “mai ammanchidi” (mai manchi)\nlvreduce -L 10G /dev/pve/vm-101-disk-0 WARNING: Reducing active logical volume to 10.00 GiB. THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce pve/vm-101-disk-0? y/n]: y confermate con “y” senza paura! avevate fatto il backup si? Il risultato che restituirà il comando sarà il seguente:\nSize of logical volume pve/vm-101-disk-0 changed from 22.00 GiB (5632 extents) to 10.00 GiB (2560 extents). Logical volume pve/vm-101-disk-0 successfully resized. le operazioni a questo punto sono praticamente completate, ma manca un ultimo piccolo passaggio e sarà relativo alla modifica della configurazione del container, dobbiamo semplicemente fargli sapere che le dimensioni del disco sono cambiate\nvi /etc/pve/lxc/101.conf la stringa rootfs dovrà cambiare semplicemente da 22GB a 10GB nel punto “size”\nrootfs: local-lvm:vm-101-disk-0,size=10G ora potete finalmente accendere il container e per avere la certezza della buona riuscita procedete con accedere nel vostro container su console e verificate lo spazio disco con un semplice df -h\nQuesto è tutto, il ridimensionamento è possibile anche in caso contrario, quindi aggiungendo spazio disco, il comando in quel caso sarà lvextend +10G e resize2fs ma comunque sia Proxmox in quel caso vi viene in aiuto permettendovi di eseguirlo direttamente da gui e completamente automizzato.\n","permalink":"https://marcofanuntza.it/2023/12/17/ridimensionare-disco-container-su-proxmox/","summary":"\u003cp\u003eSpesso può capitare la necessità di dover ridimensionare il disco di un container creato troppo grande o relativamente piccolo, a dire la verità spesso si tende ad aumentare lo spazio disco, vuoi crescita dei logs, aggiunta di nuove funzioni, in questo caso specifico però mostreremo come ridurre lo spazio.\u003c/p\u003e\n\u003cp\u003ePer chi come me utilizza Proxmox in ambito domestico dovrà fare subito i conti con la mancanza di una LAN a 10GbE, questo si tradurrà in migrazioni tra i nodi eccessivamente lunghe, la regola quindi deve essere: container piccolo = veloce. Quando si crea un container può succedere di non aver ben chiaro in mente quanto dimensionare lo spazio disco, ma niente paura si ridimensiona!\u003c/p\u003e","title":"Ridimensionare disco container su Proxmox"},{"content":"Introduzione:\nNel corso della mia esperienza nel settore dell’IT, se c’è qualcosa che ho imparato è che l’efficienza e la flessibilità sono fondamentali. È qui che entra in gioco Proxmox, una piattaforma che ho scoperto essere un vero game-changer. In questo primo articolo, voglio condividere la mia esperienza con Proxmox e negli articoli che seguiranno mostrarvi come ho configurato il mio “homedatacenter”.\nChe cos’è Proxmox?\nè una piattaforma di virtualizzazione, un hypervisor di tipo 1 permette di virtualizzare virtual machine e container interfaccia web per il controllo permette configurazione in cluster gestisce storage, snapshot e backup automatizzati basata su Debian, utilizza KVM per le vm e LXC per i container completamente free e open source piani di licenza enterprise attivabili Virtualizzazione e Containerizzazione: La Combo Vincente\nAttraverso l’utilizzo di Proxmox, ho scoperto il potenziale di KVM (Kernel-based Virtual Machine) per una virtualizzazione performante delle VM e con LCX (Linux Containers) una rapida implementazione e gestione dei containers. La sinergia tra queste due tecnologie consente di raggiungere prestazioni elevate e un livello di efficienza senza precedenti.\nInterfaccia Web Intuitiva: Gestione a Portata di Click\nQuello che si apprezza subito di Proxmox è la sua interfaccia web user-friendly. Posso monitorare le risorse in tempo reale, eseguire facilmente operazioni di backup e ripristino. La gestione diventa un’esperienza visiva e accessibile anche ai meno esperti.\nBackup e Ripristino: La Sicurezza dei Miei Dati al Primo Posto\nProxmox garantisce una tranquillità in più con la sua robusta funzionalità di backup e ripristino. Creare snapshot delle VM e dei container è un gioco da ragazzi. In caso di necessità, il ripristino è veloce e indolore.\nCluster e Alta Disponibilità: Non Solo per le Aziende\nIl bello di Proxmox è che non è riservato solo alle grandi aziende. Anche nel mio ambiente domestico, ho potuto creare un cluster in alta affidabilità. La gestione centralizzata, la distribuzione automatica del carico e l’alta disponibilità delle risorse sono diventate una realtà anche nel mio piccolo “data center casalingo”.\nComunità Attiva e Supporto Professionale: Una Rete di Supporto a Portata di Mano\nUnirsi alla comunità Proxmox è stato un passo naturale. La condivisione di esperienze e il supporto degli sviluppatori e degli utenti sono inestimabili. E se mai avessi bisogno di un livello di assistenza più professionale, c’è il supporto dedicato per le esigenze aziendali.\nConclusioni:\nProxmox ha trasformato la mia gestione delle risorse server. Se stai cercando flessibilità, controllo e un’esperienza di gestione che si adatti alle tue esigenze, ti consiglio vivamente di dare un’occhiata a Proxmox. Sia che tu stia gestendo un ambiente aziendale complesso o stia creando un cluster nel tuo salotto, Proxmox offre un controllo senza precedenti sulle tue risorse server. Non potrei essere più soddisfatto della mia scelta.\n","permalink":"https://marcofanuntza.it/2023/12/16/gestione-home-datacenter-con-proxmox/","summary":"\u003cp\u003e\u003cstrong\u003eIntroduzione:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNel corso della mia esperienza nel settore dell’IT, se c’è qualcosa che ho imparato è che l’efficienza e la flessibilità sono fondamentali. È qui che entra in gioco Proxmox, una piattaforma che ho scoperto essere un vero game-changer. In questo primo articolo, voglio condividere la mia esperienza con Proxmox e negli articoli che seguiranno mostrarvi come ho configurato il mio “homedatacenter”.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChe cos’è Proxmox?\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eè una piattaforma di virtualizzazione, un hypervisor di tipo 1\u003c/li\u003e\n\u003cli\u003epermette di virtualizzare virtual machine e container\u003c/li\u003e\n\u003cli\u003einterfaccia web per il controllo\u003c/li\u003e\n\u003cli\u003epermette configurazione in cluster\u003c/li\u003e\n\u003cli\u003egestisce storage, snapshot e backup automatizzati\u003c/li\u003e\n\u003cli\u003ebasata su Debian, utilizza KVM per le vm e LXC per i container\u003c/li\u003e\n\u003cli\u003ecompletamente free e open source\u003c/li\u003e\n\u003cli\u003epiani di licenza enterprise attivabili\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eVirtualizzazione e Containerizzazione: La Combo Vincente\u003c/strong\u003e\u003c/p\u003e","title":"Gestione home datacenter con Proxmox"}]