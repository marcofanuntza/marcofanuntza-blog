[{"content":"Nei miei post precedenti ho mostrato diversi metodi per installare un cluster Kubernetes: utilizzando Kind, K3D, Rancher e infine Kubespray.\nPartendo proprio da quest‚Äôultimo ‚Äî l\u0026rsquo;articolo lo trovate qui: Installiamo Kubernetes con Kubespray ‚Äî oggi voglio completare la configurazione del cluster aggiungendo tre componenti fondamentali per renderlo pronto a gestire ambienti di test e l‚Äôerogazione di servizi:\nIngress Controller con NGINX MetalLB per l‚Äôassegnazione di indirizzi IP ai LoadBalancer Cert-Manager per la gestione automatica dei certificati SSL Tutti questi componenti saranno installati tramite Helm, lo strumento di package management per Kubernetes.\n‚ö†Ô∏è Per chi ha seguito l\u0026rsquo;articolo precedente, consideriamo come punto di partenza un cluster Kubernetes gi√† funzionante.\nSe invece il vostro cluster √® stato creato con altri metodi, il risultato finale potrebbe differire, soprattutto a livello di configurazioni di rete e ruoli del cluster.\nA cosa servono questi componenti? Ingress Controller Un Ingress Controller consente di esporre applicazioni interne al cluster verso l‚Äôesterno, utilizzando risorse di tipo Ingress.\n√à fondamentale per gestire:\nl\u0026rsquo;accesso HTTP/HTTPS alle applicazioni, il routing per host o path, la gestione centralizzata dei certificati TLS. In questo articolo utilizziamo il controller NGINX, uno dei pi√π diffusi e mantenuti.\nMetalLB In ambienti bare-metal o virtualizzati senza cloud provider, Kubernetes non pu√≤ assegnare IP pubblici ai servizi LoadBalancer.\nMetalLB risolve questo problema offrendo funzionalit√† di bilanciamento del carico L2 o BGP e gestendo l‚Äôassegnazione di IP da un pool locale.\nCert-Manager Cert-Manager automatizza:\nla generazione, il rinnovo, e la distribuzione di certificati TLS/SSL per le applicazioni esposte tramite Ingress. Supporta diverse certificate authority tra cui Let\u0026rsquo;s Encrypt, e consente di avere HTTPS funzionante con il minimo sforzo.\nInstallazione con Helm Installiamo Ingress Controller NGINX helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install ingress-nginx ingress-nginx/ingress-nginx \\ --namespace ingress-nginx \\ --create-namespace \\ --set controller.kind=DaemonSet \\ --set controller.service.type=LoadBalancer Utilizziamo DaemonSet per deployare un controller su ogni nodo e LoadBalancer per permettere l‚Äôesposizione tramite MetalLB.\nInstalliamo MetalLB helm repo add metallb https://metallb.github.io/metallb helm repo update helm install metallb metallb/metallb \\ --namespace metallb-system \\ --create-namespace Creiamo ora la configurazione metal-lb-config.yaml con il pool di IP da usare:\napiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: default-address-pool namespace: metallb-system spec: addresses: - 192.168.1.240-192.168.1.250 --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: l2-advertisement namespace: metallb-system Applichiamo il file:\nkubectl apply -f metal-lb-config.yaml üîß Personalizzate il range IP in base alla vostra rete locale.\nInstalliamo Cert-Manager helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set crds.enabled=true Ecco un esempio di ClusterIssuer per ottenere certificati da Let\u0026rsquo;s Encrypt in modalit√† staging:\napiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: tuo@email.it privateKeySecretRef: name: letsencrypt-staging solvers: - http01: ingress: class: nginx Considerazioni finali Con questi tre componenti ‚Äî Ingress NGINX, MetalLB e Cert-Manager ‚Äî il tuo cluster Kubernetes √® finalmente pronto per:\npubblicare applicazioni tramite Ingress, gestire indirizzi IP in ambienti non cloud, ottenere certificati HTTPS in modo automatico. Questo √® un setup ideale per ambienti di test realistici, demo o piccole installazioni on-premise.\nNel prossimo articolo potremmo vedere come deployare una vera applicazione web e renderla accessibile via HTTPS.\nHai dubbi, suggerimenti o vuoi raccontarmi la tua esperienza? Lascia un commento o scrivimi!\n","permalink":"https://marcofanuntza.it/posts/completare-cluster-kubernetes/","summary":"\u003cp\u003eNei miei post precedenti ho mostrato diversi metodi per installare un cluster Kubernetes: utilizzando Kind, K3D, Rancher e infine Kubespray.\u003c/p\u003e\n\u003cp\u003ePartendo proprio da quest‚Äôultimo ‚Äî l\u0026rsquo;articolo lo trovate qui: \u003ca href=\"https://marcofanuntza.it/posts/installiamo-kubernets-con-kubespray\"\u003eInstalliamo Kubernetes con Kubespray\u003c/a\u003e ‚Äî oggi voglio completare la configurazione del cluster aggiungendo tre componenti fondamentali per renderlo pronto a gestire ambienti di test e l‚Äôerogazione di servizi:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eIngress Controller\u003c/strong\u003e con NGINX\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetalLB\u003c/strong\u003e per l‚Äôassegnazione di indirizzi IP ai LoadBalancer\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCert-Manager\u003c/strong\u003e per la gestione automatica dei certificati SSL\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTutti questi componenti saranno installati tramite \u003cstrong\u003eHelm\u003c/strong\u003e, lo strumento di package management per Kubernetes.\u003c/p\u003e","title":"Completare cluster Kubernetes con Ingress NGINX, MetalLB e Cert-Manager"},{"content":"Eseguire un\u0026rsquo;immagine temporanea con Elasticdump Il comando seguente consente di creare un pod temporaneo nel namespace elasticsearch-prod dove risiede l\u0026rsquo;istanza elasticsearch, e nello stesso tempo permette di accedere alla sua shell Bash:\nkubectl run elasticdump-client --rm -it --image=node:18 -n elasticsearch-prod -- bash Installare Elasticdump Una volta nella shell del pod, installiamo elasticdump tramite npm:\nnpm install -g elasticdump Eseguire la sincronizzazione degli indici Ora siamo pronti per eseguire il sync dei dati tra gli indici. Eseguiamo il seguente comando per ogni indice da copiare:\nIndice: prod-index NODE_TLS_REJECT_UNAUTHORIZED=0 elasticdump \\ --input=http://elasticsearch-old-prod.elasticsearch-old-prod.svc.cluster.local:9200/prod-index \\ --output=https://elastic:password@elasticsearch-new-prod.elasticsearch-new-prod.svc.cluster.local:9200/prod-index \\ --type=data Analisi del comando NODE_TLS_REJECT_UNAUTHORIZED=0: disabilita il controllo del certificato SSL. Necessario perch√© l\u0026rsquo;istanza di destinazione ha X-Pack Security abilitato e accetta solo connessioni HTTPS.\n--input=...: URL della sorgente Elasticsearch che contiene l‚Äôindice da copiare.\n--output=...: URL della destinazione Elasticsearch. Verr√† creato (o aggiornato) un indice con lo stesso nome e contenuti.\nConsiderazioni finali La sincronizzazione degli indici tra due istanze di Elasticsearch √® una pratica molto utile in diversi scenari. Ad esempio, pu√≤ servire durante attivit√† di migrazione tra ambienti (da staging a produzione), in caso di ripristino parziale dei dati o per effettuare test su dati reali in ambienti isolati.\nUtilizzare strumenti come elasticdump consente di eseguire questo allineamento in modo semplice e granulare, mantenendo il controllo sui singoli indici. √à una soluzione comoda anche perch√© non richiede downtime, permettendo la copia dei dati in tempo reale, direttamente dal cluster di origine a quello di destinazione.\n","permalink":"https://marcofanuntza.it/posts/sincronizzazione.indici.elasticsearch.con.elasticdump.in.kubernetes/","summary":"\u003ch2 id=\"eseguire-unimmagine-temporanea-con-elasticdump\"\u003eEseguire un\u0026rsquo;immagine temporanea con Elasticdump\u003c/h2\u003e\n\u003cp\u003eIl comando seguente consente di creare un \u003cstrong\u003epod temporaneo\u003c/strong\u003e nel namespace \u003ccode\u003eelasticsearch-prod\u003c/code\u003e dove risiede l\u0026rsquo;istanza elasticsearch, e nello stesso tempo permette di accedere alla sua shell Bash:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ekubectl run elasticdump-client --rm -it --image\u003cspan class=\"o\"\u003e=\u003c/span\u003enode:18 -n elasticsearch-prod -- bash\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"installare-elasticdump\"\u003eInstallare Elasticdump\u003c/h2\u003e\n\u003cp\u003eUna volta nella shell del pod, installiamo \u003ccode\u003eelasticdump\u003c/code\u003e tramite \u003ccode\u003enpm\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003enpm install -g elasticdump\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003chr\u003e\n\u003ch2 id=\"eseguire-la-sincronizzazione-degli-indici\"\u003eEseguire la sincronizzazione degli indici\u003c/h2\u003e\n\u003cp\u003eOra siamo pronti per eseguire il \u003cstrong\u003esync\u003c/strong\u003e dei dati tra gli indici. Eseguiamo il seguente comando per ogni indice da copiare:\u003c/p\u003e","title":"Sincronizzazione Indici Elasticsearch con Elasticdump in Kubernetes"},{"content":"Introduzione In questa guida vedremo come installare Elasticsearch su Kubernetes utilizzando HelmChart di Bitnami. Seguiremo un approccio strutturato, includendo la configurazione personalizzata tramite un file values.yaml\nAggiungere il repository Helm di Bitnami Per prima cosa, aggiungiamo il repository Bitnami alla nostra installazione di Helm:\nhelm repo add bitnami https://charts.bitnami.com/bitnami Se il repository √® gi√† stato aggiunto in precedenza, aggiorniamolo per assicurarci di avere la versione pi√π recente:\nhelm repo update Verificare le versioni disponibili di Elasticsearch Se abbiamo bisogno di installare una versione specifica di Elasticsearch, possiamo controllare le versioni disponibili nel repository:\nhelm search repo bitnami/elasticsearch --versions Creare un namespace dedicato Per una migliore organizzazione, creiamo un namespace dedicato per la nostra installazione di Elasticsearch:\nkubectl create ns elasticsearch-test-stage Configurare Elasticsearch con un file values.yaml Possiamo personalizzare l\u0026rsquo;installazione specificando alcuni parametri in un file YAML. Ecco un esempio di configurazione che utilizzeremo:\ncoordinating: replicaCount: 1 data: replicaCount: 1 global: kibanaEnabled: true ingest: enabled: false ingress: enabled: true ingressClassName: nginx master: replicaCount: 1 ‚ö†Ô∏è Attenzione: Assicurati che il file sia correttamente indentato per evitare errori YAML.\nInstallare Elasticsearch con Helm Ora possiamo eseguire l\u0026rsquo;installazione specificando la versione desiderata e il file di configurazione:\nhelm install elasticsearch-test-stage bitnami/elasticsearch \\ --version 19.9.4 \\ --namespace elasticsearch-test-stage \\ -f values.yaml L\u0026rsquo;installazione potrebbe richiedere alcuni minuti.\nVerificare l\u0026rsquo;installazione Una volta completata l\u0026rsquo;installazione, possiamo verificare che tutto sia stato creato correttamente:\nkubectl get all -n elasticsearch-test-stage Se tutto √® andato a buon fine, Elasticsearch sar√† pronto per l\u0026rsquo;uso! üöÄ\ntips. terminata installazione vi siete resi conto di aver dimenticato un ulteriore parametro necessario? La soluzione √® semplicissima!\nEditate il file values.yaml e invece che utilizzare \u0026ldquo;install\u0026rdquo; eseguirete il comando con \u0026ldquo;upgrade\u0026rdquo;\nhelm upgrade elasticsearch-test-stage bitnami/elasticsearch \\ --version 19.9.4 \\ --namespace elasticsearch-test-stage \\ -f values.yaml Happy helming!\n","permalink":"https://marcofanuntza.it/posts/installiamo-elasticsearch-con-helmchart-bitnami/","summary":"\u003ch2 id=\"introduzione\"\u003eIntroduzione\u003c/h2\u003e\n\u003cp\u003eIn questa guida vedremo come installare \u003cstrong\u003eElasticsearch\u003c/strong\u003e su Kubernetes utilizzando \u003cstrong\u003eHelmChart di Bitnami\u003c/strong\u003e. Seguiremo un approccio strutturato, includendo la configurazione personalizzata tramite un file \u003ccode\u003evalues.yaml\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id=\"aggiungere-il-repository-helm-di-bitnami\"\u003eAggiungere il repository Helm di Bitnami\u003c/h2\u003e\n\u003cp\u003ePer prima cosa, aggiungiamo il repository \u003cstrong\u003eBitnami\u003c/strong\u003e alla nostra installazione di Helm:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehelm repo add bitnami https://charts.bitnami.com/bitnami\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSe il repository √® gi√† stato aggiunto in precedenza, aggiorniamolo per assicurarci di avere la versione pi√π recente:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ehelm repo update\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"verificare-le-versioni-disponibili-di-elasticsearch\"\u003eVerificare le versioni disponibili di Elasticsearch\u003c/h2\u003e\n\u003cp\u003eSe abbiamo bisogno di installare una versione specifica di \u003cstrong\u003eElasticsearch\u003c/strong\u003e, possiamo controllare le versioni disponibili nel repository:\u003c/p\u003e","title":"Installare Elasticsearch con Helm Chart di Bitnami"},{"content":"Installare Kubernetes con Kubespray Installare Kubernetes seguendo la documentazione ufficiale √® noto come \u0026ldquo;the hard way\u0026rdquo; e scoprirete presto quanto questo nome sia appropriato.\nFortunatamente, grazie all\u0026rsquo;open source, sono stati sviluppati diversi strumenti per semplificare questa procedura complessa. Uno dei pi√π validi √® senza dubbio Kubespray.\nCos\u0026rsquo;√® Kubespray? Kubespray √® un progetto open-source che consente di installare e gestire cluster Kubernetes in modo automatizzato tramite Ansible.\n√à una soluzione altamente flessibile che supporta diversi provider cloud e ambienti on-premises, rendendola adatta a molteplici scenari di deployment.\nCaratteristiche principali Basato su Ansible: utilizza playbook per l\u0026rsquo;installazione e la gestione del cluster. Compatibilit√† multi-cloud e on-premises: supporta AWS, GCP, Azure, OpenStack, Proxmox, bare metal e altre piattaforme. Alta personalizzazione: consente di configurare componenti come CNI (Calico, Flannel, Cilium), CRI (containerd, Docker, cri-o) e molto altro. Alta disponibilit√†: permette di creare cluster Kubernetes in modalit√† High Availability con pi√π master e worker. Integrazione con add-on: supporta strumenti come Helm, MetalLB, CoreDNS e altri. Se state cercando un\u0026rsquo;alternativa a kubeadm che vi offra maggiore controllo sulla configurazione del cluster, Kubespray √® la scelta ideale.\nInstallazione di Kubernetes con Kubespray In questa guida, vedremo passo dopo passo come installare un cluster Kubernetes utilizzando Kubespray.\nPrerequisiti\nLa mia installazione √® stata eseguita utilizzando alcune virtual machine create sul mio cluster Proxmox. Nel dettaglio abbiamo bisogno di:\n3 VM per i nodi master\n3 VM per i nodi worker\nworkstation con distribuzione linux, oppure WSL2 se utilizzate Windows.\nLa lista delle VM e relativi IP:\nkubespray-master-01 192.168.1.184 kubespray-master-02 192.168.1.189 kubespray-master-03 192.168.1.190 kubespray-worker-01 192.168.1.191 kubespray-worker-02 192.168.1.192 kubespray-worker-03 192.168.1.193 Su workstation clonare il repository git, io ho utilizzato la versione 2.26.0, creare un \u0026ldquo;env\u0026rdquo; ambiente virtuale per pyrhon e installare i moduli \u0026ldquo;requirements\u0026rdquo; tramite pip3\ngit clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray python3 -m venv kubespray-venv source kubespray-venv/bin/activate pip3 install -U -r requirements.txt Ora editare i files necessari al deploy tramite ansible, copiamo una directory sample e editiamola in base alle nostre esigenze\ncp -rfp inventory/sample inventory/proxmox01 declare -a IPS=(192.168.1.184 192.168.1.189 192.168.1.190 192.168.1.191 192.168.1.192 192.168.1.193) CONFIG_FILE=inventory/proxmox01/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]} Il comando precedente ha creato un file chiamato hosts-yaml e inserito IP, dobbiamo editarlo ulteriormente con i nomi dei nostri nodi\nvim inventory/proxmox01/hosts.yaml Aggiungiamo un\u0026rsquo;ulteriore file dove inseriremo alcune variabili\nvi inventory/proxmox01/cluster-variables.yaml Questo sar√† il contenuto:\nkube_version: v1.30.1 helm_enabled: true kube_proxy_mode: iptables Ora siamo pronti per installare il cluster, prima un\u0026rsquo;ultima verifica eseguendo un test di raggiungibilit√† sui nodi, oltre al comando che segue dobbiamo copiare la nostra chiave SSH sui nodi precedentemente creati il comando che segue eseguir√† un ping\nansible -i inventory/proxmox01/hosts.yaml -m ping all -u service Se tutti i nodi saranno raggiungibili senza problemi siamo finalmente pronti per installare il cluster\nansible-playbook -i inventory/proxmox01/hosts.yaml -e @inventory/proxmox01/cluster-variables.yaml --become --become-user=root -u service cluster.yml Il comando √® abbastanza parlante e chiaro, ora attendete che kubespray tramite ansible installi il cluster kubernetes nei nodi predefiniti, dovrebbe impiegare alcuni minuti.\nTerminato il tutto dovreste avere il messaggio di recap simile a questo\nverifichiamo il cluster kubernetes, eseguiamo un accesso ssh sul primo nodo master e facciamo in modo che il nostro user possa chiamare il cluster tramite kubectl, per fare questo copiamo il file di configurazione come segue:\nsudo mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Ora il pi√π classico dei comandi\nkubectl get nodes ","permalink":"https://marcofanuntza.it/posts/installiamo-kubernets-con-kubespray/","summary":"\u003ch2 id=\"installare-kubernetes-con-kubespray\"\u003eInstallare Kubernetes con Kubespray\u003c/h2\u003e\n\u003cp\u003eInstallare Kubernetes seguendo la documentazione ufficiale √® noto come \u003cstrong\u003e\u0026ldquo;the hard way\u0026rdquo;\u003c/strong\u003e e scoprirete presto quanto questo nome sia appropriato.\u003cbr\u003e\nFortunatamente, grazie all\u0026rsquo;open source, sono stati sviluppati diversi strumenti per semplificare questa procedura complessa. Uno dei pi√π validi √® senza dubbio \u003cstrong\u003eKubespray\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"cos√®-kubespray\"\u003eCos\u0026rsquo;√® Kubespray?\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eKubespray\u003c/strong\u003e √® un progetto open-source che consente di installare e gestire cluster Kubernetes in modo automatizzato tramite \u003cstrong\u003eAnsible\u003c/strong\u003e.\u003cbr\u003e\n√à una soluzione altamente flessibile che supporta diversi provider cloud e ambienti on-premises, rendendola adatta a molteplici scenari di deployment.\u003c/p\u003e","title":"Installare Kubernetes con Kubespray"},{"content":"Nel mio Homelab, come ho descritto in precedenza in altri post, utilizzo un cluster Proxmox.\nSebbene il cluster disponga gi√† di una dashboard integrata, questa risulta piuttosto spartana. Cos√¨, in questo tranquillo pomeriggio delle ferie natalizie, ho colto l\u0026rsquo;occasione per sperimentare una soluzione pi√π avanzata: InfluxDB + Grafana.\nInfluxDB √® un database open-source progettato specificamente per la gestione di serie temporali, ovvero dati che variano nel tempo come metriche, eventi e log. Grazie alla sua architettura ottimizzata, √® in grado di gestire grandi volumi di dati in tempo reale, rendendolo una scelta ideale per applicazioni di monitoraggio e analisi delle prestazioni.\nCon la sua natura scalabile e alla capacit√† di integrarsi con strumenti come Grafana, Prometheus e Telegraf, InfluxDB √® ampiamente utilizzato nel campo DevOps. Con un\u0026rsquo;interfaccia semplice e un set di funzionalit√† avanzate, rappresenta una soluzione potente per chiunque abbia bisogno di analizzare e visualizzare dati basati sul tempo.\nGrafana √® una piattaforma open-source progettata per la visualizzazione e l\u0026rsquo;analisi di dati in tempo reale, utilizzata principalmente per il monitoraggio delle metriche e dei log. Grazie alla sua capacit√† di supportare numerose fonti di dati, come Prometheus, InfluxDB, Elasticsearch e database relazionali, consente agli utenti di creare dashboard altamente personalizzabili e interattive per analizzare informazioni complesse.\nOltre alla visualizzazione, Grafana offre funzionalit√† avanzate di alerting, permettendo di configurare avvisi personalizzati in base a specifiche condizioni e inviarli tramite email, Slack o altri canali.\nIl paradigma √® semplice \u0026raquo; Proxmox genera le metriche, InfluxDB si occupa di salvare le metriche in un database (bucket) e Grafana infine restituir√† le stesse sottoforma di dashboard con grafici dettagliati.\nPer installare InfluxDB e Grafana ho deciso di procedere utilizzando dei container Docker e per comodit√† ho scelto un\u0026rsquo;approccio molto easy \u0026amp; lazy\u0026hellip; tradotto significa.. utilizzo Portainer e suoi templates. Basta semplicemente andare su Templates e nella ricerca scrivere influxdb e poi grafana, troverete rispettivamente: InfluxDB for Edge e Grafana Dashboard\nI template su Portainer sono gi√† strutturati per l\u0026rsquo;inserimento delle variabili, nello specifico per InfluxDB ci sar√† la creazione di un primo bucket (Influx Bucket Name) e una organization (Influx Org Name), questi dati saranno utili nel passaggio successivo.\nDopo aver installato entrambi vai istruito il cluster per indirizzare le metriche su InfluxDB, su Proxmox dalla GUI sulla sezione root del Datacenter si va sul men√π Metric Server ADD \u0026gt; InfluxDB\nCome potete notare viene chiesto anche un Token, questo va creato su InfluxDB, io per comodit√† ho clonato il token admin gi√† presente su sezione Load Data \u0026raquo; API Tokens\nSalvate su Proxmox e se i dati sono stati inseriti correttamente dovreste gi√† vedere su InfluxDB nella sezione Data Explorer il bucket popolarsi con le metriche, nel mio caso il bucket si chiama proxmox-mnt.\nLa parte di configurazione su Proxmox e InfluxDB possiamo considerarla conclusa, ora si potr√† continuare su Grafana.\nSu Grafana iniziamo con \u0026ldquo;agganciare\u0026rdquo; la base dati salvata da InfluxDB, per fare questo rechiamoci sulla sezione Connections e poi Data Sources, qui nella ricerca inseriamo InfluxDB come sorgente e compiliamo i dati necessari\nPrestate attenzione nel linguaggio SQL da scegliere, √® importante selezionare Flux\nCompletiamo inserendo i dati per far raggiungere InfluxDB e relativo bucket, il token che andremo a utilizzare si √® lo stesso di prima. Salvate con test positivo.\nAdesso arriva la parte finale e pi√π soddisfacente, su Grafana dobbiamo creare la nostra Dashboard. Cio√® avvocato\u0026hellip;creare\u0026hellip; c\u0026rsquo;√® gi√† qualcuno pi√π bravo di noi e disponibile che ha gi√† fatto una dashboard!\nGrazie alla community Grafana permette di condividere dashboard gi√† pronte e semplici da importare direttamente sulla vostra istanza, queste si trovano facilmente sul sito grafanalabs.\nProvate a cercare proxmox e scoprirete quante sono gi√† state condivise e disponibili\nIo ho scelto la dashboard Proxmox Cluster Flux e per installarla su Grafana basta semplicemente scaricare il file json e importarlo sulle nostre dashboard\nLa procedura √® talmente semplice che non sto a elencarvela, passo direttamente a mostrarvi il risultato finale, queste sono le metriche generate dal mio cluster Proxmox, salvate da InfluxDB e \u0026ldquo;graficate\u0026rdquo; da Grafana\nVengono mostrati in bella mostra i dati relativi alla CPU e al consumo di memoria dei due nodi presenti, una media del carico di utilizzo, lo storage disponibile, una sezione intermedia elenca invece le VM attualmente accese e i container LXC attivi.\nScendendo pi√π in basso sono presenti tutta una serie di grafici che possiamo zoomare e estendere o ridurre la finestra temporale.\nCredo che al momento possa essere la mia scelta definitiva per monitorare il mio cluster Proxmox, il passo successivo sar√† impostare alcuni alert in base al carico con relative notifiche via mail.\nPer ora √® tutto! Buone feste e felice anno nuovo!\n","permalink":"https://marcofanuntza.it/posts/monitorare-cluster-proxmox-con-influxdb-e-grafana/","summary":"\u003cp\u003eNel mio Homelab, come ho descritto in precedenza in altri post, utilizzo un cluster Proxmox.\u003c/p\u003e\n\u003cp\u003eSebbene il cluster disponga gi√† di una dashboard integrata, questa risulta piuttosto spartana.\nCos√¨, in questo tranquillo pomeriggio delle ferie natalizie, ho colto l\u0026rsquo;occasione per sperimentare una soluzione pi√π avanzata: InfluxDB + Grafana.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInfluxDB\u003c/strong\u003e √® un database open-source progettato specificamente per la gestione di serie temporali, ovvero dati che variano nel tempo come metriche, eventi e log. Grazie alla sua architettura ottimizzata, √® in grado di gestire grandi volumi di dati in tempo reale, rendendolo una scelta ideale per applicazioni di monitoraggio e analisi delle prestazioni.\u003c/p\u003e","title":"Come monitoro il mio cluster Proxmox con InfluxDB e Grafana"},{"content":"Nel precedente articolo avevo mostrato come procedere all\u0026rsquo;installazione di GitLab su un nostro server locale, clicca qui per leggerlo.\nGitLab √® uno strumento leader nel mondo DevOps, oggi √® tra i pi√π diffusi per il versionamento del proprio codice software, ma oltre a questo √® molto di pi√π!\nGitLab tra le tante funzionalit√†, mette a disposizione anche un Container Registry.\nIl GitLab Container Registry √® un registro integrato all\u0026rsquo;interno di GitLab stesso, che consente di archiviare, condividere e distribuire facilmente le immagini dei container all\u0026rsquo;interno dei progetti GitLab.\nQuesta integrazione, insieme a GitLab CI, rende GitLab una piattaforma completa per automatizzare e accelerare i processi DevOps, semplificando ulteriormente l\u0026rsquo;intero ciclo di vita dello sviluppo software.\nSe utilizzate GitLab come servizio direttamente su gitlab.com, troverete il registry gi√† abilitato, se invece lo avete installato su un vostro server locale sar√† necessario abilitarlo.\nLa documentazione ufficiale pu√≤ sembrare molto esaustiva ma spesso ci si perde tra i dettagli.\nIn questa guida vi mostrer√≤ brevemente solo quattro semplici stringhe da modificare per abilitare il registry.\nPartiamo dalla premessa che voi siate l\u0026rsquo;amministratore del sistema e che ne abbiate accesso e pieno controllo, il file da editare sar√† il classico \u0026quot;/etc/gitlab/gitlab.rb\u0026quot;.\nLimitiamoci a editare queste sole stringhe, ovviamente voi adattatele alla vostra url\nregistry_external_url 'https://gitlab.marcofanuntza.it:5050' gitlab_rails['registry_enabled'] = true gitlab_rails['registry_host'] = \u0026quot;gitlab.marcofanuntza.it\u0026quot; gitlab_rails['registry_path'] = \u0026quot;/var/opt/gitlab/gitlab-rails/shared/registry\u0026quot; Dovete sapere che potete impostare una url specifica esclusiva per il registry, come per esempio registry.gitlab.esempio.it ma nel mio caso ho preferito avere un\u0026rsquo;unico indirizzo in comune con il servizio principale. Salvate il file e subito dopo eseguite il comando per applicare la configurazione alla vostra istanza in esecuzione con:\nsudo gitlab-ctl reconfigure sudo gitlab-ctl restart Prima di procedere con ulteriori test assicuratevi che la porta 5050 sia ora in ascolto, √® la porta default che viene utilizzata per contattare il registry, se la porta √® in ascolto possiamo andare avanti.\nOra gi√† per un primo controllo \u0026ldquo;visivo\u0026rdquo; accedete su un vostro project all\u0026rsquo;interno di gitlab, tra le voci presenti sulla sinistra andate su \u0026ldquo;Deploy\u0026rdquo; dovrebbe esservi apparsa nel sottomen√π la voce \u0026ldquo;Container Ragistry\u0026rdquo;\nEntrando sulla pagina vi mostrer√† i comandi da eseguire sul vostro client docker per procedere con creazione e upload del container sul vostro nuovo Gitlab Container Registry\nProcediamo con quest\u0026rsquo;ultima verifica direttamente dal client Docker\u0026hellip;\nDobbiamo istruire il nostro client per considerare il registry come \u0026ldquo;insicuro\u0026rdquo; editando/creando il file /etc/docker/daemon.json\n{ \u0026quot;insecure-registries\u0026quot; : [ \u0026quot;https://gitlab.marcofanuntza.it:5050\u0026quot; ] } Non allarmatevi questa istruzione √® necessaria quando si utilizza un certificato self-signed e la CA non pu√≤ essere verificata da Docker\nPer la successiva build prepariamo un semplice Dockerfile come segue:\nFROM ubuntu RUN apt-get update CMD [\u0026quot;echo\u0026quot;, \u0026quot;Ciao, Gitlab Registry!\u0026quot;] Adesso siamo pronti! eseguiamo il login con il primo comando, le credenziali saranno le stesse del vostro user gitlab che vi permette di accedere al progetto\ndocker login gitlab.marcofanuntza.it:5050 Con il comando che segue eseguiamo la build del nostro container\ndocker build -t gitlab.marcofanuntza.it:5050/utentezero/testregistry . Ora il momento cruciale, con il comando successivo carichiamo l\u0026rsquo;immagine docker finalmente sul nostro Gitlab Container Registry\ndocker push gitlab.marcofanuntza.it:5050/utentezero/testregistry Nel prossimo articolo cercher√≤ di mostrarvi lo stesso concetto per√≤ eseguito in automatico da una pipeline, sempre all\u0026rsquo;interno di Gitlab!\n","permalink":"https://marcofanuntza.it/posts/abilitiamo-il-gitlab-container-registry/","summary":"\u003cp\u003eNel precedente articolo avevo mostrato come procedere all\u0026rsquo;installazione di GitLab su un nostro server locale, \u003ca href=\"https://marcofanuntza.it/posts/installiamo-gitlab/\"\u003eclicca qui per leggerlo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eGitLab √® uno strumento leader nel mondo DevOps, oggi √® tra i pi√π diffusi per il versionamento del proprio codice software, ma oltre a questo √® molto di pi√π!\u003cbr\u003e\nGitLab tra le tante funzionalit√†, mette a disposizione anche un Container Registry.\u003c/p\u003e\n\u003cp\u003eIl \u003cstrong\u003eGitLab Container Registry\u003c/strong\u003e √® un registro integrato all\u0026rsquo;interno di GitLab stesso, che consente di archiviare, condividere e distribuire facilmente le immagini dei container all\u0026rsquo;interno dei progetti GitLab.\u003c/p\u003e","title":"Abilitiamo il Gitlab Container Registry"},{"content":"Come eseguo il backup dei volumi Docker? Niente di pi√π semplice!\nTralasciando la pappardella su quanto sia importante avere dei backup vi mostro come eseguo il backup dei volumi Docker presenti sul mio Raspberry Pi 5\nEseguo applicazioni in self hosting e alcune sono dei container Docker, ho citato il Raspberry ma la stessa procedura pu√≤ essere utilizzata su qualsiasi altra distribuzione Linux\nPartiamo dal presupposto che utilizziate Docker che esegue dei container e che questi abbiano dei volumi persistenti come nel mio caso.\ndocker volume ls DRIVER VOLUME NAME local portainer_data local vaultw-data Se non conoscete il percorso dove Docker posiziona i volumi potete ottenerlo semplicemente con un inspect del volume\ndocker inspect vaultw-data [ { \u0026quot;CreatedAt\u0026quot;: \u0026quot;2024-12-01T08:45:34Z\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Labels\u0026quot;: null, \u0026quot;Mountpoint\u0026quot;: \u0026quot;/var/lib/docker/volumes/vaultw-data/_data\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;vaultw-data\u0026quot;, \u0026quot;Options\u0026quot;: null, \u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot; } ] Ecco lo script\nQuesto √® lo script che utilizzo, ci sono i commenti per ogni fase, spero siano comprensibili per voi.\n#! /bin/sh PATH=/usr/local/bin:/usr/bin: #Script per eseguire backup dei volumi docker #Dichiaro Variabili DATE_BIN=$(command -v date) DATE=`${DATE_BIN} +%y-%m-%d--%H:%M:%S` DESTINATION=/home/marco/backup-docker-volume/backups/backup-docker/$DATE.tar.gz SOURCEFOLDER=/var/lib/docker/volumes/ #Stop dei container in esecuzione, √® importante per avere una situazione senza scritture sui volumi docker stop $(docker ps -q) #Creo un file zip tar.gz della directory dove Docker posiziona i volumi e salvo il file sulla directory di destinazione tar -cpzf $DESTINATION $SOURCEFOLDER #Restart dei container Docker precedentemente fermati docker restart $(docker ps -a -q) #Cancello i file backup pi√π vecchi di 7 giorni find $DESTINATION/ -mtime +7 -type f -delete Salvato il file lo si rende eseguibile, il nome che ho dato allo script √®: script-bck.sh voi potete scegliere il nome che preferite basta che venga mantenuta estensione del file .sh\nchmod a+x script-bck.sh Sarebbe buona norma eseguirlo almeno una volta al giorno e considerando il fatto che i container devono essere momentaneamente fermati vi consiglio di eseguirlo la notte, ci pensa CRON (Obviously numero 1)\n0 0 * * * /usr/bin/sh /home/marco/backup-docker-volume/script-bck.sh Il backup √® importante ma la sua funzione potrebbe essere vana se lasciassimo i files sullo stesso host, muore questo perdiamo anche i backup.. (Obviously numero 2)\nPer questo i files vengono direttamente scritti su uno share NFS presente sul mio server NAS (TruenasCore)\nQuello che infatti non ho specificato sullo script √® che la variabile $DESTINATION √® uno share NFS montato da fstab\nSawadee kap!\n","permalink":"https://marcofanuntza.it/posts/come-eseguo-backup-volumi-docker/","summary":"\u003cp\u003eCome eseguo il backup dei volumi Docker? Niente di pi√π semplice!\u003c/p\u003e\n\u003cp\u003eTralasciando la pappardella su quanto sia importante avere dei backup vi mostro come eseguo il backup dei volumi Docker presenti sul mio Raspberry Pi 5\u003c/p\u003e\n\u003cp\u003eEseguo applicazioni in self hosting e alcune sono dei container Docker, ho citato il Raspberry ma la stessa procedura pu√≤ essere utilizzata su qualsiasi altra distribuzione Linux\u003c/p\u003e\n\u003cp\u003ePartiamo dal presupposto che utilizziate Docker che esegue dei container e che questi abbiano dei volumi persistenti come nel mio caso.\u003c/p\u003e","title":"Come eseguo backup dei volumi Docker"},{"content":"Questo articolo continua la serie denominata \u0026ldquo;Il potere CI/CD\u0026rdquo;, in precedenza abbiamo mostrato come installare ArgoCD, poi siamo passati al registry con Harbor, adesso √® arrivato il momento di Gitlab.\nChe cos\u0026rsquo;√® Gitlab?\nGitLab √® una piattaforma per la gestione del software basata su Git, fornisce un vasto set di strumenti per favorire la collaborazione, automatizzare processi e monitorare lo sviluppo del software durante il suo ciclo di vita.\nEcco alcuni aspetti chiave di GitLab:\nRepository Git: Fornisce repository Git per il controllo delle versioni del codice sorgente del software.\nGestione del Progetto: Include strumenti per la gestione delle attivit√†, la pianificazione, la tracciabilit√† dei problemi e altro ancora.\nIntegrazione Continua e Rilascio Continuo (CI/CD): Supporta la creazione, il test e il rilascio automatici del software attraverso pipeline CI/CD.\nControllo degli Accessi e Autorizzazioni: Consente la definizione precisa dei permessi di accesso per garantire una collaborazione sicura.\nIssue Tracking e Kanban Boards: Offre strumenti avanzati per la gestione delle problematiche e la visualizzazione del lavoro in corso attraverso Kanban boards.\nCollaborazione e Commenti: Favorisce la collaborazione tra membri del team con funzionalit√† di commento e comunicazione integrate.\nWiki e Documentazione: Include un sistema di wiki e strumenti per la documentazione, consentendo la creazione e la condivisione di informazioni.\nRegistri e Monitoraggio: Fornisce funzionalit√† per la registrazione delle attivit√†, il monitoraggio delle performance e la gestione dei log.\nIntegrazioni: Si integra con una variet√† di strumenti di sviluppo di terze parti per una maggiore flessibilit√†.\nGitLab √® disponibile in diverse edizioni, tra cui una versione gratuita e open source, GitLab Community Edition (CE), e una versione a pagamento, GitLab Enterprise Edition (EE), che offre funzionalit√† avanzate e supporto aziendale.\nPrerequisiti\n-VM o server con distribuzione Ubuntu 22.04 -Accesso alla rete -Sul file hosts inserite il nome che esporr√† il servizio, \u0026ldquo;EXTERNAL_URL\u0026rdquo; nel mio caso sar√†: gitlab.marcofanuntza.it\nProcedura\nPer installazione dei pacchetti utilizzeremo il sistema APT, partiamo con installare alcune dipendenze con questo comando che segue\nsudo apt-get install -y curl openssh-server ca-certificates tzdata perl Per l\u0026rsquo;installazione di Gitlab sar√† necessario utilizzare il loro repository specifico, recuperiamolo tramite curl\ncurl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.deb.sh | sudo bash Adesso che abbiamo a disposizione il repository ufficiale possiamo procedere con installazione tramite APT, nel comando possiamo gi√† da ora settare la variabile EXTERNAL_URL\nsudo EXTERNAL_URL=\u0026quot;https://gitlab.marcofanuntza.it\u0026quot; apt-get install gitlab-ee Nota.1 come avrete potuto notare stiamo installando il pacchetto *-ee che sta per enterprise edition, non preoccupatevi, recentemente gitlab distribuisce un solo pacchetto senza pi√π distinguerlo come faceva in precedenza con la versione CE community edition e enterprise edition EE appunto, la gestione della licenza √® facoltativa e successiva per chi ne avr√† l\u0026rsquo;esigenza, il pacchetto sar√† comunque lo stesso.\nNota.2 durante installazione in maniera automatica cercher√† di crearvi un certificato lets\u0026rsquo;encrypt, se la vostra \u0026ldquo;EXTERNAL_URL\u0026rdquo; non √® gi√† esposta su dns pubblici e non sar√† raggiungibile questa procedur√† fallir√†, ma non preoccupatevi l\u0026rsquo;installazione verr√† comunque completata.\nAttendiamo che l\u0026rsquo;installazione tramite APT verr√† completata, oltre a gitlab verranno installati vari servizi, ad esempio anche nginx e postgres. Terminata installazione, errore citato su nota 2 a parte, siamo pronti per chiamare il servizio tramite l\u0026rsquo;interfaccia web.\nLe credenziali per il login saranno root e la password temporanea potete recuperarla in chiaro sul file /etc/gitlab/initial_root_password\nDopo il primo login manco a dirlo procediamo subito con la sostituzione della nostra password di root, dopo averla sostituita ci ritroveremo nella pagina iniziale pronti per eseguire un nuovo login con le nuove credenziali.\nIl file principale che contiene e permette la maggior parte dei settings Gitlab √® \u0026quot;/etc/gitlab/gitlab.rb\u0026quot;\nPer completare la nostra installazione a questo punto possiamo configurare alcuni aspetti, prima di tutto mettere in sicurezza il nostro servizio gitlab configurando il certificato SSL (per chi ha avuto l\u0026rsquo;errore in fase di installazione), poi possiamo passare a configurare le notifiche mail.\nPer le notiche mail possiamo utilizzare un servizio SMTP esterno, la configurazione dei parametri si pu√≤ editare sempre dal file /etc/gitlab/gitlab.rb, qui un\u0026rsquo;esempio dei settings, potete comunque prendere visione della specifica pagina sulla documentazione ufficiale QUI\ngitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \u0026quot;smtp.server\u0026quot; gitlab_rails['smtp_port'] = 465 gitlab_rails['smtp_user_name'] = \u0026quot;smtp user\u0026quot; gitlab_rails['smtp_password'] = \u0026quot;smtp password\u0026quot; gitlab_rails['smtp_domain'] = \u0026quot;example.com\u0026quot; gitlab_rails['smtp_authentication'] = \u0026quot;login\u0026quot; gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['smtp_openssl_verify_mode'] = 'peer' # If your SMTP server does not like the default 'From: gitlab@localhost' you # can change the 'From' with this setting. gitlab_rails['gitlab_email_from'] = 'gitlab@example.com' gitlab_rails['gitlab_email_reply_to'] = 'noreply@example.com' # If your SMTP server is using a self signed certificate or a certificate which # is signed by a CA which is not trusted by default, you can specify a custom ca file. # Please note that the certificates from /etc/gitlab/trusted-certs/ are # not used for the verification of the SMTP server certificate. gitlab_rails['smtp_ca_file'] = '/path/to/your/cacert.pem' Ogni volta che editiamo il file e modifichiamo le impostazioni contenute all\u0026rsquo;interno √® necessario eseguire il seguente comando per applicare le modifiche:\nsudo gitlab-ctl reconfigure La configurazione delle notifiche mail √® di vitale importanza per la gestione degli user per la conferma delle credenziali, cambio password etc..\nL\u0026rsquo;articolo che completer√† la serie mostrer√† una pipeline completa che eseguir√† un classico rilascio software sfruttando il sistema CI/CD e per farlo utilizzer√† gli elementi che sono stato oggetto degli articoli precdenti, Gitlab, ArgoCD, Harbor e Kubernetes\n","permalink":"https://marcofanuntza.it/posts/installiamo-gitlab/","summary":"\u003cp\u003eQuesto articolo continua la serie denominata \u0026ldquo;Il potere CI/CD\u0026rdquo;, in precedenza abbiamo mostrato come installare ArgoCD, poi siamo passati al registry con Harbor, adesso √® arrivato il momento di Gitlab.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChe cos\u0026rsquo;√® Gitlab?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGitLab √® una piattaforma per la gestione del software basata su Git, fornisce un vasto set di strumenti per favorire la collaborazione, automatizzare processi e monitorare lo sviluppo del software durante il suo ciclo di vita.\u003c/p\u003e\n\u003cp\u003eEcco alcuni aspetti chiave di GitLab:\u003c/p\u003e","title":"Installiamo Gitlab"},{"content":"Harbor √® un registry open-source per la gestione delle immagini dei container. Progettato per funzionare con orchestration tools come Kubernetes e Docker Swarm.\nEcco alcune caratteristiche principali di Harbor:\nHarbor offre un registry sicuro e privato per le immagini dei container, permettendo un controllo totale sulla loro archiviazione e distribuzione.\nPolitiche di Sicurezza: Supporta politiche per garantire che solo immagini sicure e approvate vengano utilizzate nell\u0026rsquo;ambiente.\nControllo degli Accessi: Dispone di un sistema robusto di controllo degli accessi, consentendo la definizione precisa di chi pu√≤ accedere e distribuire immagini specifiche.\nIntegrazione Universale: Progettato per integrarsi facilmente con strumenti comuni come Kubernetes, Docker e altri.\nReplicazione di Immagini: Consente la replicazione tra diversi registri Harbor, garantendo la disponibilit√† e la distribuzione su scala geografica.\nScansione delle Vulnerabilit√†: Fornisce strumenti di scansione per identificare e affrontare potenziali problemi di sicurezza nelle immagini dei container.\nGestione del Ciclo di Vita: Supporta l\u0026rsquo;intero ciclo di vita delle immagini, dalla creazione alla distribuzione e alla dismissione.\nHarbor √® particolarmente utile in ambienti aziendali in cui la sicurezza e il controllo dell\u0026rsquo;accesso alle immagini container sono fondamentali. La sua architettura aperta e scalabile lo rende adatto sia a scenari di piccola scala che a implementazioni di grandi dimensioni.\nProviamolo installandolo su un server o su una VM\nPrerequisiti\n-VM o server con distribuzione Linux (in questo articolo ho utilizzato ubuntu-server-22-04-LTS)\n-Docker\n-Docker compose\n-Openssl\nPer installare Docker e Docker Compose posso segnalarvi questo howto che ho scritto in precedenza QUI\nProcedura\nDopo aver installato i prerequisti, dobbiamo assicurarci che il server o la VM soddisfino i requisiti consigliati da Harbor, deve avere a disposizione almeno 4GB di Ram e 2CPU, per lo spazio disco possono bastare 20GB, che comunque possono essere estesi al bisogno se avete utilizzato LVM.\nL\u0026rsquo;installazione di Harbor viene eseguita tramite un\u0026rsquo;installer che possiamo scaricare dal sito ufficiale, noi utilizzeremo la versione 2.10.0\nwget https://github.com/goharbor/harbor/releases/download/v2.10.0/harbor-online-installer-v2.10.0.tgz tar -xzvf harbor-online-installer-v2.10.0.tgz cd harbor cp harbor.yml.tmpl harbor.yml a questo punto dobbiamo creare il nostro certificato SSL, per questa guida inizialmente utilizzeremo un certificato self-signed creato con openssl\nmkdir -p certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout certs/harbor.key -addext \u0026quot;subjectAltName = DNS:harbor.marcofanuntza.it\u0026quot; -x509 -days 365 -out certs/harbor.crt dopo aver terminato copiamo il risultato sul path dei certificati\nsudo cp certs/harbor.* /etc/ssl/certs/ editiamo il file harbor.yml che abbiamo copiato in precedenza, dobbiamo inserire hostname e il path del certificato SSL\nhostname: harbor.marcofanuntza.it # http related config http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 80 https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /etc/ssl/certs/harbor.crt private_key: /etc/ssl/certs/harbor.key adesso sempre all\u0026rsquo;interno della directory harbor possiamo notare due script bash, il primo da eseguire sar√† prepare e in seguito eseguiremo install.sh\n./prepare lo script prepare scaricher√† una prima immagine docker da docker-hub, questa preparer√† una base per lo script successivo che di fatto andr√† a tirare s√π tutto lo stack docker tramite docker compose\n./install.sh mettetevi comodi perche ci vorr√† un p√≤, verranno scaricate le immagini docker per redis, postresql, registry, proxy, core e portal, successivamente verranno tutte avviate in automatico, dovete attendere un output simile a quanto segue\n‚úî Container harbor-log Started ‚úî Container harbor-portal Started ‚úî Container registry Started ‚úî Container redis Started ‚úî Container registryctl Started ‚úî Container harbor-db Started ‚úî Container harbor-core Started ‚úî Container harbor-jobservice Started ‚úî Container nginx Started sembra che tutto sia andato per il verso giusto, per scrupolo comunque potete verificare che tutti i container siano in stato di esecuzione tramite docker ps\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a069f83f6725 goharbor/harbor-jobservice:v2.10.0 \u0026quot;/harbor/entrypoint.‚Ä¶\u0026quot; 3 minutes ago Up About a minute (healthy) harbor-jobservice e96dfe0dc5a0 goharbor/nginx-photon:v2.10.0 \u0026quot;nginx -g 'daemon of‚Ä¶\u0026quot; 3 minutes ago Up 2 minutes (healthy) 0.0.0.0:80-\u0026gt;8080/tcp, :::80-\u0026gt;8080/tcp, 0.0.0.0:443-\u0026gt;8443/tcp, :::443-\u0026gt;8443/tcp nginx 89a0f9fc181b goharbor/harbor-core:v2.10.0 \u0026quot;/harbor/entrypoint.‚Ä¶\u0026quot; 4 minutes ago Up 2 minutes (healthy) harbor-core b0ed82f07593 goharbor/registry-photon:v2.10.0 \u0026quot;/home/harbor/entryp‚Ä¶\u0026quot; 4 minutes ago Up 3 minutes (healthy) registry edabed017c3d goharbor/harbor-db:v2.10.0 \u0026quot;/docker-entrypoint.‚Ä¶\u0026quot; 4 minutes ago Up 3 minutes (healthy) harbor-db d83c114bbebd goharbor/harbor-registryctl:v2.10.0 \u0026quot;/home/harbor/start.‚Ä¶\u0026quot; 4 minutes ago Up 3 minutes (healthy) registryctl 14fc36784402 goharbor/harbor-portal:v2.10.0 \u0026quot;nginx -g 'daemon of‚Ä¶\u0026quot; 4 minutes ago Up 3 minutes (healthy) harbor-portal 7f759eda6cb2 goharbor/redis-photon:v2.10.0 \u0026quot;redis-server /etc/r‚Ä¶\u0026quot; 4 minutes ago Up 3 minutes (healthy) redis 1d24113d3097 goharbor/harbor-log:v2.10.0 \u0026quot;/bin/sh -c /usr/loc‚Ä¶\u0026quot; 4 minutes ago Up 3 minutes (healthy) 127.0.0.1:1514-\u0026gt;10514/tcp harbor-log Si la nostra installazione di Harbor sembra completata senza intoppi, siamo pronti per accedere alla sua interfaccia web, apriamo il broswer e inseriamo la credenziali admin, la password la trovate in chiaro sul file harbor.yml, cambiatela dopo il primo login\necco la pagina principale\nAdesso non ci resta che creare gli user e relativi permessi per poter cos√¨ iniziare a caricare immagini sui rispettivi project!\n","permalink":"https://marcofanuntza.it/posts/harbor-come-registry/","summary":"\u003cp\u003e\u003cstrong\u003eHarbor\u003c/strong\u003e √® un registry open-source per la gestione delle immagini dei container. Progettato per funzionare con orchestration tools come Kubernetes e Docker Swarm.\u003c/p\u003e\n\u003cp\u003eEcco alcune caratteristiche principali di Harbor:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHarbor\u003c/strong\u003e offre un registry sicuro e privato per le immagini dei container, permettendo un controllo totale sulla loro archiviazione e distribuzione.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePolitiche di Sicurezza:\u003c/strong\u003e Supporta politiche per garantire che solo immagini sicure e approvate vengano utilizzate nell\u0026rsquo;ambiente.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eControllo degli Accessi:\u003c/strong\u003e Dispone di un sistema robusto di controllo degli accessi, consentendo la definizione precisa di chi pu√≤ accedere e distribuire immagini specifiche.\u003c/p\u003e","title":"Harbor come Registry"},{"content":"Se siete arrivati a leggere questo articolo do per scontato conosciate gi√† Kubectl, il non plus ultra della riga di comando per Kubernetes!\nSapevate che Kubectl pu√≤ acquisire ancora pi√π potenza grazie a Krew?\nKrew √® uno strumento che semplifica la gestione, l\u0026rsquo;installazione e l\u0026rsquo;aggiornamento di tutta una serie di plugin specifici per Kubectl, estendendone di fatto la gi√† ampia funzionalit√†.\nIl suo funzionamento pu√≤ sembrare molto simile ai tradizionali gestori di pacchetti yum, apt, apk, brew e altri, utilizza un modello basato su repository per distribuire e gestire i plugin. L\u0026rsquo;installazione di nuovi plugin avviene in modo uniforme, fornendo una procedura standardizzata che semplifica il processo.\nKrew semplifica anche il processo di aggiornamento dei plugin, consentendo agli utenti di mantenere facilmente le loro estensioni aggiornate con le ultime funzionalit√† e correzioni di bug.\nTutto questo avviene direttamente dalla stessa interfaccia di comando Kubectl, gli utenti possono accedere a tutte le funzionalit√† aggiuntive avendo a disposizione un\u0026rsquo;ambiente totalmente integrato!\nSe √® la prima volta che sentite parlare di krew, posso consigliarvi di dare un\u0026rsquo;occhiata alla documentazione ufficiale QUI\nIn questo articolo vi mostrer√≤ come installarlo e proveremo alcuni plugin che ho avuto modo di testare.\nPrerequisiti\ndistribuzione linux git installato Procedimento Basandoci sulla documentazione ufficiale utilizzeremo uno script che si occuper√† di installare Krew in autonomia, lo script √® il seguente:\n( set -x; cd \u0026quot;$(mktemp -d)\u0026quot; \u0026amp;\u0026amp; OS=\u0026quot;$(uname | tr '[:upper:]' '[:lower:]')\u0026quot; \u0026amp;\u0026amp; ARCH=\u0026quot;$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\u0026quot; \u0026amp;\u0026amp; KREW=\u0026quot;krew-${OS}_${ARCH}\u0026quot; \u0026amp;\u0026amp; curl -fsSLO \u0026quot;https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\u0026quot; \u0026amp;\u0026amp; tar zxvf \u0026quot;${KREW}.tar.gz\u0026quot; \u0026amp;\u0026amp; ./\u0026quot;${KREW}\u0026quot; install krew ) dopo che lo script avr√† terminato editiamo il file .bashrc inserendo nell\u0026rsquo;ultima riga l\u0026rsquo;export path che segue\nexport PATH=\u0026quot;${KREW_ROOT:-$HOME/.krew}/bin:$PATH\u0026quot; salvate il file e ricaricate la vostra sessione shell, il comando che segue se tutto √® andato bene vi restituir√† lo stesso output\nkubectl krew version OPTION VALUE GitTag v0.4.4 GitCommit 343e657 IndexURI https://github.com/kubernetes-sigs/krew-index.git BasePath /root/.krew IndexPath /root/.krew/index/default InstallPath /root/.krew/store BinPath /root/.krew/bin DetectedPlatform linux/amd64 Ok a questo punto abbiamo Krew installato, possiamo provare qualche plugin. Testiamo ad\u0026rsquo;esempio il plugin Tree, come potete vedere per installare il plugin baster√† un semplice comando\nkubectl krew install tree La funzione principale del plugin Tree √® fornire una rappresentazione visuale ad albero delle risorse all\u0026rsquo;interno di un cluster Kubernetes. Invece di visualizzare le risorse in un formato tabellare o di elenco, il plugin kubectl tree organizza gerarchicamente le risorse in una struttura ad albero, rendendo pi√π facile comprendere la relazione tra di esse.\nEcco un\u0026rsquo;esempio con il deployment di Velero\nkubectl tree deployment velero -n velero NAMESPACE NAME READY REASON AGE velero Deployment/velero - 2y221d velero ‚îú‚îÄReplicaSet/velero-668d7f99cc - 630d velero ‚îÇ ‚îî‚îÄPod/velero-668d7f99cc-cqbpf True 287d velero ‚îú‚îÄReplicaSet/velero-86c77779f6 - 630d velero ‚îî‚îÄReplicaSet/velero-8cb65ddbc - 2y221d Proviamo poi il plugin Clog che banalmente colora l\u0026rsquo;output dei logs quando visualizzati da kubectl\nkubectl krew install clog Proviamo il plugin Outdated che come potete immaginare far√† una scansione delle immagini utilizzate segnalando tag corrente in utilizzo e versione latest, aiutandoci a individuare quali sarebbe meglio aggiornare\u0026hellip;\nkubectl krew install outdated kubectl outdated Come gi√† scritto, sono presenti una mirade di plugin, al momento ne contiamo 231, vi rimando alla pagina ufficiale e scegliete il plugin che desiderate!\n","permalink":"https://marcofanuntza.it/posts/installiamo-krew/","summary":"\u003cp\u003eSe siete arrivati a leggere questo articolo do per scontato conosciate gi√† Kubectl, il non plus ultra della riga di comando per Kubernetes!\u003c/p\u003e\n\u003cp\u003eSapevate che Kubectl pu√≤ acquisire ancora pi√π potenza grazie a Krew?\u003c/p\u003e\n\u003cp\u003eKrew √® uno strumento che semplifica la gestione, l\u0026rsquo;installazione e l\u0026rsquo;aggiornamento di tutta una serie di plugin specifici per Kubectl, estendendone di fatto la gi√† ampia funzionalit√†.\u003c/p\u003e\n\u003cp\u003eIl suo funzionamento pu√≤ sembrare molto simile ai tradizionali gestori di pacchetti yum, apt, apk, brew e altri, utilizza un modello basato su repository per distribuire e gestire i plugin. L\u0026rsquo;installazione di nuovi plugin avviene in modo uniforme, fornendo una procedura standardizzata che semplifica il processo.\u003c/p\u003e","title":"Installiamo Krew"},{"content":"Velero √® uno strumento che aiuta a gestire il backup e il ripristino delle risorse e dei volumi persistenti del tuo cluster Kubernetes.\nVelero √® uno strumento open source molto utilizzato che consente il backup e il ripristino delle risorse Kubernetes e dei volumi persistenti tra cluster in cloud o on-premises. Supporta la maggior parte dei provider di archiviazione, come AWS, Azure, GCP, DigitalOcean e altri. Possiamo utilizzare Velero per creare snapshot del cluster Kubernetes in un determinato momento e ripristinare gli oggetti su un cluster differente o in uno stato diverso. Possiamo utilizzarlo anche per migrare i carichi di lavoro tra cluster in cloud e non, oppure per eseguire il ripristino in caso di guasti o perdita di dati.\nCaratteristiche e funzionalit√† di Velero:\nBackup e ripristino di risorse Kubernetes e volumi persistenti, manualmente o su base programmata.\nSupporto per vari provider di archiviazione come AWS S3, Azure Blob Storage, Google Cloud Storage, DigitalOcean Spaces, e altri.\nCapacit√† di filtrare e selezionare risorse su cui eseguire il backup e il ripristino, basandosi su namespace, label o annotazioni.\nCapacit√† di eseguire comandi o script personalizzati prima e dopo le operazioni di backup e ripristino, utilizzando gli hooks.\nCapacit√† di estendere la funzionalit√† di Velero utilizzando plugin personalizzati, come driver di backup e ripristino, plugin di object store o plugin di snapshot di volumi.\nCapacit√† di monitorare e risolvere problemi durante le operazioni di backup e ripristino, utilizzando log, eventi e metriche.\nSfruttando le funzionalit√† elencate in precedenza possiamo riassumere i casi d\u0026rsquo;uso pi√π comuni di Velero:\nBackup e ripristino: Utilizzare Velero per eseguire il backup e il ripristino delle risorse e dei volumi persistenti del tuo cluster Kubernetes, manualmente o su base programmata.\nDisaster recovery: Utilizzare Velero per eseguire il ripristino di emergenza del tuo cluster Kubernetes, in caso di guasto del cluster o perdita di dati.\nMigrazione: Utilizzare Velero per migrare i tuoi carichi di lavoro tra cluster o cloud, senza perdere lo stato o la configurazione delle tue applicazioni e dati. Ad esempio, puoi utilizzare Velero per eseguire il backup del tuo cluster da un fornitore di servizi cloud e ripristinarlo su un altro fornitore di servizi cloud, nel caso desideri cambiare o ottimizzare i tuoi servizi cloud. Puoi anche utilizzare Velero per eseguire il backup del tuo cluster da una versione di Kubernetes e ripristinarlo su un\u0026rsquo;altra versione di Kubernetes, nel caso desideri eseguire l\u0026rsquo;aggiornamento o il downgrade del tuo cluster.\nVantaggi:\n√à open source e gratuito, con una comunit√† attiva e documentazione.\n√à facile da installare e utilizzare, con un\u0026rsquo;interfaccia a riga di comando semplice e una Helm chart.\n√à compatibile e interoperabile con altri strumenti e componenti Kubernetes, come kubectl, Helm e kustomize.\n√à flessibile e personalizzabile, con varie opzioni e parametri per configurare le operazioni di backup e ripristino.\nSvantaggi:\nNon supporta il backup e il ripristino delle risorse a livello di cluster, come CRD, RBAC o admission controllers. √à necessario utilizzare uno strumento separato, come kubeadm, per eseguire il backup e il ripristino di queste risorse.\nNon supporta il backup e il ripristino di origini dati esterne, come database o code di messaggi, non gestite da Kubernetes. √à necessario utilizzare uno strumento separato, come pg_dump, per eseguire il backup e il ripristino di queste fonti di dati.\nNon fornisce un\u0026rsquo;interfaccia utente grafica o una console di gestione web. √à necessario utilizzare l\u0026rsquo;interfaccia a riga di comando o uno strumento di terze parti, come Arkade, per gestire e monitorare le operazioni di backup e ripristino.\nNel prossimo articolo installeremo e proveremo Velero sul nostro cluster Kubernetes di test utilizzato negli articoli precedenti!\n","permalink":"https://marcofanuntza.it/posts/backup-con-velero/","summary":"\u003cp\u003e\u003cstrong\u003eVelero\u003c/strong\u003e √® uno strumento che aiuta a gestire il backup e il ripristino delle risorse e dei volumi persistenti del tuo cluster Kubernetes.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/velero2.webp#center\" alt=\"Example image\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVelero\u003c/strong\u003e √® uno strumento open source molto utilizzato che consente il backup e il ripristino delle risorse Kubernetes e dei volumi persistenti tra cluster in cloud o on-premises. Supporta la maggior parte dei provider di archiviazione, come AWS, Azure, GCP, DigitalOcean e altri.\nPossiamo utilizzare Velero per creare snapshot del cluster Kubernetes in un determinato momento e ripristinare gli oggetti su un cluster differente o in uno stato diverso. Possiamo utilizzarlo anche per migrare i carichi di lavoro tra cluster in cloud e non, oppure per eseguire il ripristino in caso di guasti o perdita di dati.\u003c/p\u003e","title":"Backup con Velero"},{"content":"ARGO CD\nArgo CD √® uno strumento open-source progettato per implementare e gestire il CD (continous deployment) su infrastrutture Kubernetes. Si basa sui principi GitOps, utilizza repository Git come unica fonte di verit√† per la configurazione dell\u0026rsquo;infrastruttura e delle applicazioni.\nI principali punti chiave di ARGO CD\nContinuous Deployment: ARGO CD automatizza il processo di implementazione delle applicazioni su cluster Kubernetes, garantendo che lo stato attuale corrisponda a quello dichiarato nel repository Git.\nGitOps: Basato sul concetto di GitOps, ARGO CD utilizza un repository Git come fonte unica di verit√† per la configurazione di infrastruttura e applicazioni.\nDashboard Intuitiva: Fornisce un\u0026rsquo;interfaccia web che consente agli utenti di visualizzare e gestire lo stato delle applicazioni nel cluster Kubernetes.\nRilasci: Semplifica i rilasci delle applicazioni tra i diversi ambienti, facilitando il passaggio da sviluppo a produzione attraverso il processo GitOps.\nRollback: Permette di tornare a versioni precedenti delle applicazioni in modo controllato in caso di problemi dopo un rilascio.\nIntegrazione con Strumenti di CI/CD: Pu√≤ essere integrato con strumenti di CI/CD per creare un flusso di lavoro completo dall\u0026rsquo;implementazione iniziale alla gestione continua delle applicazioni.\nMulti-Cluster e Multi-Tenancy: Supporta l\u0026rsquo;implementazione su pi√π cluster Kubernetes e offre funzionalit√† di multi-tenancy per isolare risorse e permessi.\nEstendibile e Personalizzabile: Essendo open-source ARGO CD √® estendibile, consentendo agli utenti di creare estensioni personalizzate per adattarsi alle esigenze specifiche.\nAttraverso queste caratteristiche ARGO CD semplifica e automatizza il ciclo di vita delle applicazioni deployate su Kubernetes.\nPrerequisiti\nAvere a disposizione un cluster Kubernetes (anche locale) Avere una workstation/notebook, il vostro server di sviluppo ad\u0026rsquo;esempio, dove poter installare quanto segue Kubectl Helm Se non avete a disposizione un cluster Kubernetes, per installarlo possiamo seguire molteplici strade, in questo blog ho gi√† scritto alcune guide che potrebbero aiutarvi nel raggiungere lo scopo. Tenete conto che non abbiamo bisogno di un cluster production-grade, basta un semplice cluster locale installato sulla vostra workstation/notebook.\nPer questo articolo io ho utilizzato un cluster Kubernetes installato con Kind, potete seguire questa guida QUI\nProcedimento\nOra.. se avete seguito la guida che vi ho indicato pi√π s√π, diamo per scontato che abbiamo gi√† il cluster Kubernetes pronto con Kubectl, non ci rimane quindi che installare Helm.\nsudo curl -s https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash helm version version.BuildInfo{Version:\u0026quot;v3.14.0\u0026quot;, GitCommit:\u0026quot;3fc9f4b2638e76f26739cd77c7017139be81d0ea\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, GoVersion:\u0026quot;go1.21.5\u0026quot;} Adesso che abbiamo tutti gli elementi per procedere, iniziamo con creare il namespace da dedicare a argocd sul nostro cluster kubernetes\nsudo kubectl create namespace argocd L\u0026rsquo;installazione di Argo-CD verr√† eseguita tramite Helm, nello specifico utilizzeremo il repository sviluppato dal team Bitnami, aggiungiamo il repo\nsudo helm repo add bitnami https://charts.bitnami.com/bitnami sudo helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;bitnami\u0026quot; chart repository Update Complete. ‚éàHappy Helming!‚éà Adattate il comando che segue in base alle vostre esigenze, prestate attenzione alla password e all\u0026rsquo;hostname\nsudo helm install argocd --set config.secret.argocdServerAdminPassword=AB12345 --set server.ingress.enabled=true --set server.ingress.ingressClassName=nginx --set server.service.type=ClusterIP --set server.ingress.pathType=Prefix --set server.ingress.hostname=argocd.marcofanuntza.it bitnami/argo-cd --namespace argocd Riceverete questo output:\nNAME: argocd LAST DEPLOYED: Fri Jan 26 08:33:07 2024 NAMESPACE: argocd STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: argo-cd CHART VERSION: 5.5.0 APP VERSION: 2.9.5 ** Please be patient while the chart is being deployed ** 1. Access your Argo CD installation: Connect to one of the following hosts: http://argocd.marcofanuntza.it 2. Execute the following commands to obtain the Argo CD credentials: echo \u0026quot;Username: \\\u0026quot;admin\\\u0026quot;\u0026quot; echo \u0026quot;Password: $(kubectl -n argocd get secret argocd-secret -o jsonpath=\u0026quot;{.data.clearPassword}\u0026quot; | base64 -d)\u0026quot; Verifichiamo per sicurezza se la password rispecchia quanto abbiamo impostato in fase di installazione\nsudo echo \u0026quot;Password: $(sudo kubectl -n argocd get secret argocd-secret -o jsonpath=\u0026quot;{.data.clearPassword}\u0026quot; | base64 -d)\u0026quot; Password: AB12345 Ok perfetto, possiamo andare avanti.\nAdesso dovete prestare attenzione a questi passaggi perche saranno essenziali per permetterci di raggiungere la Web-Gui di Argo-CD.\nIniziamo con provare a chiamare sul browser la url impostata in fase di installazione, riceverete il warning sul certificato che √® normale essendo self-signed, accettate e andate avanti, dovreste ricevere lo stesso errore che segue\nPerch√® succede questo? Il problema √® che di default Argo-CD gestisce la terminazione TLS in autonomia e reindirizza sempre le richieste HTTP in HTTPS.\nNoi abbiamo l\u0026rsquo;ingress controller che gestisce la terminazione TLS e comunica sempre con il servizio backend tramite HTTP, il risultato √® che il server di Argo-CD risponder√† sempre con un reindirizzamento in HTTPS. Da qu√¨ il nostro errore!\nUna delle soluzioni consiste nel disabilitare HTTPS su Argo-CD, che possiamo impostare utilizzando il flag \u0026ndash;insecure sul deployment argocd-server.\nQuesto problema √® effettivamente documentato qui: link documentazione\nModifichiamo quindi il nostro deployment\nsudo kubectl edit deployment argocd-argo-cd-server -n argocd Editiamo solo questa sezione del file inserendo l\u0026rsquo;istruzione \u0026ndash;insecure, salvate e chiudete il file, Kubernetes in automatico eseguir√† un restart del deployment\ncontainers: - args: - argocd-server - --insecure - --staticassets - /opt/bitnami/argo-cd/app Adesso non dovreste pi√π avere l\u0026rsquo;errore precedente e si presenter√† la pagina per il login sulla web-gui di Argo-CD\nProviamo un login utilizzando lo user admin e la password impostata in fase d\u0026rsquo;installazione\nL\u0026rsquo;installazione di ARGO-CD a questo punto possiamo considerarla conclusa, ora non ci (vi) resta che provare lo strumento, per fare questo abbiamo bisogno per√≤ di una applicazione e di un server git/gitlab da interrogare.. sar√† l\u0026rsquo;oggetto del mio prossimo post su questo blog?\nContenuto Extra\nDimenticavo che sarebbe altrettanto utile avere la CLI di Argo-CD, quindi installiamola! Possiamo installare la CLI sul notebook o sulla stessa macchina dove abbiamo gi√† installato Helm, oppure dove preferite a patto che Argo-CD sia raggiungibile.\nsudo curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 sudo chmod +x /usr/local/bin/argocd argocd version argocd: v2.8.9+3ef5ba7 BuildDate: 2024-01-19T18:33:48Z GitCommit: 3ef5ba76a95d71de88bfa9b6e887c86025a88fdd GitTreeState: clean GoVersion: go1.20.12 Compiler: gc Platform: linux/amd64 FATA[0000] Argo CD server address unspecified Adesso facciamo in modo di agganciare la CLI sul nostro ARGO-CD installato precedentemente sul cluster Kubernetes, niente di pi√π semplice puntiamo la stessa url utilizzata per la web-gui, ci verranno chieste le credenziali admin e finito\nsudo argocd login argocd.marcofanuntza.it WARNING: server certificate had error: tls: failed to verify certificate: x509: certificate is valid for ingress.local, not argocd.marcofanuntza.it. Proceed insecurely (y/n)? y WARN[0008] Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web. Username: admin Password: 'admin:login' logged in successfully Context 'argocd.marcofanuntza.it' updated Con la CLI possiamo eseguire in maniera pi√π diretta le stesse operazioni possibili dalla web-gui\nsudo argocd context CURRENT NAME SERVER * argocd.marcofanuntza.it argocd.marcofanuntza.it sudo argocd cluster list --grpc-web SERVER NAME VERSION STATUS MESSAGE PROJECT https://kubernetes.default.svc in-cluster Unknown Cluster has no applications and is not being monitored. Anche la CLI √® stata installata, per avere una visione completa dei comandi possibili vi lascio il link della documentazione ufficiale QUI\n","permalink":"https://marcofanuntza.it/posts/installiamo-argocd/","summary":"\u003cp\u003e\u003cstrong\u003eARGO CD\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eArgo CD √® uno strumento open-source progettato per implementare e gestire il CD (continous deployment) su infrastrutture Kubernetes.\nSi basa sui principi GitOps, utilizza repository Git come unica fonte di verit√† per la configurazione dell\u0026rsquo;infrastruttura e delle applicazioni.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eI principali punti chiave di ARGO CD\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eContinuous Deployment:\u003c/strong\u003e ARGO CD automatizza il processo di implementazione delle applicazioni su cluster Kubernetes, garantendo che lo stato attuale corrisponda a quello dichiarato nel repository Git.\u003c/p\u003e","title":"Installiamo Argocd"},{"content":"Proviamo Kubernetes con Kind\nQuesta guida √® indicata per tutti coloro che hanno esigenza di interagire con un cluster Kubernetes per meri scopi di test, conoscenza e sviluppo utilizzando una workstation o notebook con risorse limitate.\nKind (Kubernetes IN Docker) √® uno strumento open-source progettato per semplificare la creazione e la gestione di cluster Kubernetes locali utilizzando container Docker come nodi del cluster.\nEcco alcune caratteristiche chiave di kind:\nInstallazione Semplificata: kind semplifica notevolmente il processo di installazione di Kubernetes su una macchina locale, consentendo agli sviluppatori di creare rapidamente e facilmente cluster Kubernetes per scopi di sviluppo o test.\nUtilizzo di Docker come Nodi: kind utilizza container Docker per creare i nodi del cluster Kubernetes. Ogni nodo del cluster viene eseguito come un container Docker specifico, consentendo un\u0026rsquo;implementazione leggera e isolata del cluster locale.\nAmbienti Isolati: kind consente agli sviluppatori di creare cluster Kubernetes completamente isolati, garantendo che le risorse e le configurazioni di un cluster non interferiscano con altri cluster o ambienti.\nConfigurazione Dichiarativa: La configurazione di kind √® dichiarativa e pu√≤ essere definita attraverso file di configurazione in stile YAML. Questo approccio semplifica la creazione e la gestione di cluster con configurazioni complesse.\nIntegrazione con Strumenti di CI/CD: kind √® spesso utilizzato negli ambienti di sviluppo e nei flussi di lavoro (CI/CD) per testare e validare applicazioni Kubernetes.\nAgilit√† nello Sviluppo e nel Test: kind consente agli sviluppatori di eseguire e testare le proprie applicazioni Kubernetes in un ambiente locale, facilitando lo sviluppo, il debug e il test delle applicazioni Kubernetes senza la necessit√† di un cluster remoto.\nEstendibile e Configurabile: In quanto strumento open-source, kind √® estendibile e pu√≤ essere configurato per soddisfare esigenze specifiche. Gli sviluppatori possono personalizzare i cluster creati con kind per rispecchiare le configurazioni desiderate.\nIn sintesi, kind √® uno strumento che semplifica l\u0026rsquo;installazione di cluster Kubernetes locali, facilitando il processo di sviluppo, test e debug delle applicazioni Kubernetes in un ambiente controllato e isolato.\nPrerequisiti\nVirtual Machine, Workstation o Notebook con distribuzione Linux (si pu√≤ utilizzare anche WSL2 su Windows con Docker Desktop installati) Docker Kubectl per interagire con il cluster Procedimento\nPer questo esempio io utilizzo una VM installata sul mio cluster Proxmox, la distribuzione utilizzata √® Ubuntu 22-04-LTS, iniziamo con installare Docker\n# Aggiungiamo la chiave **GPG key** ufficiale del repository Docker sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \u0026quot;deb [arch=\u0026quot;$(dpkg --print-architecture)\u0026quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \u0026quot;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026quot;$VERSION_CODENAME\u0026quot;)\u0026quot; stable\u0026quot; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Installiamo Kind, sostanzialmente si tratta di scaricare il file binario eseguibile, impostare i giusti permessi e spostarlo sul path riservato ai file eseguibili\n# For AMD64 / x86_64 [ $(uname -m) = x86_64 ] \u0026amp;\u0026amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64 sudo chmod +x ./kind sudo mv ./kind /usr/local/bin/kind Installiamo Kubectl, il concetto √® lo stesso precedente\nsudo curl -LO \u0026quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x kubectl sudo mv kubectl /usr/local/bin Verifichiamo le versioni appena installate di docker, Kind e Kubectl\ndocker --version Docker version 25.0.1, build 29cf629 kind --version kind version 0.20.0 kubectl version Client Version: v1.29.1 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Adesso siamo pronti per creare il nostro primo cluster Kubernetes utilizzando Kind, decidiamo prima come deve essere composto il nostro cluster, per esempio per le nostre esigenze desideriamo un cluster con 1 nodo control-plane e 3 nodi worker. Per istruire Kind a fare questo dobbiamo scrivere un file di configurazione.\nFile esempio in formato yaml, cluster-config.yaml\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 # patch the generated kubeadm config with some extra settings kubeadmConfigPatches: - | apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration evictionHard: nodefs.available: \u0026quot;0%\u0026quot; # patch it further using a JSON 6902 patch kubeadmConfigPatchesJSON6902: - group: kubeadm.k8s.io version: v1beta2 kind: ClusterConfiguration patch: | - op: add path: /apiServer/certSANs/- value: my-hostname # 1 control plane node and 3 workers nodes: # the control plane node config - role: control-plane image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72 kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \u0026quot;ingress-ready=true\u0026quot; extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP # the three workers - role: worker image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72 - role: worker image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72 - role: worker image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72 Il file oltre a dichiarare i tre nodi, 1 control-plane + 3 worker pre-abilita anche l\u0026rsquo;ingress controller con le porte 80 e 443. Mi raccomando sul file prestate attenzione alla corretta identazione yaml, altrimenti riceverete errore.\nEseguiamo il seguente comando che ci permetter√† di creare il cluster con la configurazione desiderata.\nsudo kind create cluster --name kube-kind-test --config cluster-config.yaml Il comando completer√† i task in circa due minuti, questo potrebbe variare in base alla vostra connessione internet, tenete conto che Docker dovr√† scaricare le apposite immagini.\nDovreste ritrovarvi con un\u0026rsquo;output simile\nCreating cluster \u0026quot;kube-kind-test\u0026quot; ... ‚úì Ensuring node image (kindest/node:v1.27.3) üñº ‚úì Preparing nodes üì¶ üì¶ üì¶ üì¶ ‚úì Writing configuration üìú ‚úì Starting control-plane üïπÔ∏è ‚úì Installing CNI üîå ‚úì Installing StorageClass üíæ ‚úì Joining worker nodes üöú Set kubectl context to \u0026quot;kind-kube-kind-test\u0026quot; You can now use your cluster with: kubectl cluster-info --context kind-kube-kind-test Thanks for using kind! üòä A questo punto possiamo utilizzare kind per verificare il cluster appena creato e kubectl per interagire con esso\nsudo kind get clusters kube-kind-test sudo kubectl get nodes NAME STATUS ROLES AGE VERSION kube-kind-test-control-plane Ready control-plane 5m47s v1.27.3 kube-kind-test-worker Ready \u0026lt;none\u0026gt; 5m21s v1.27.3 kube-kind-test-worker2 Ready \u0026lt;none\u0026gt; 5m27s v1.27.3 kube-kind-test-worker3 Ready \u0026lt;none\u0026gt; 5m26s v1.27.3 sudo kubectl get ns NAME STATUS AGE default Active 2m kube-node-lease Active 2m kube-public Active 2m kube-system Active 2m local-path-storage Active 2m sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-5d78c9869d-6dgr5 1/1 Running 0 7m48s kube-system coredns-5d78c9869d-fchdj 1/1 Running 0 7m48s kube-system etcd-kube-kind-test-control-plane 1/1 Running 0 8m2s kube-system kindnet-6p787 1/1 Running 0 7m44s kube-system kindnet-7cwt7 1/1 Running 0 7m45s kube-system kindnet-fx5lg 1/1 Running 0 7m39s kube-system kindnet-tcwbt 1/1 Running 0 7m48s kube-system kube-apiserver-kube-kind-test-control-plane 1/1 Running 0 8m2s kube-system kube-controller-manager-kube-kind-test-control-plane 1/1 Running 0 8m2s kube-system kube-proxy-cjs8z 1/1 Running 0 7m45s kube-system kube-proxy-n97zk 1/1 Running 0 7m44s kube-system kube-proxy-qhmvm 1/1 Running 0 7m48s kube-system kube-proxy-vzxpr 1/1 Running 0 7m39s kube-system kube-scheduler-kube-kind-test-control-plane 1/1 Running 0 8m2s local-path-storage local-path-provisioner-6bc4bddd6b-zzw4d 1/1 Running 0 7m48s Infine per completare il discorso ingress-controller procederemo con installazione dell\u0026rsquo;ingress controller NGINX, il deployment verr√† eseguito con kubectl scaricando il file direttamente dai repository ufficiali NGINX\nsudo kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml Verifichiamo sempre tramite kubectl\nsudo kubectl get deployment -n ingress-nginx NAME READY UP-TO-DATE AVAILABLE AGE ingress-nginx-controller 1/1 1 1 114s sudo kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-jk94q 0/1 Completed 0 2m44s ingress-nginx-admission-patch-qwc96 0/1 Completed 0 2m44s ingress-nginx-controller-864894d997-48cn4 1/1 Running 0 2m44s Assicuriamoci che l\u0026rsquo;ingress controller stia funzionando creando questo pod con service e ingress di esempio, al termine provate i due comandi curl\nsudo kubectl apply -f https://kind.sigs.k8s.io/examples/ingress/usage.yaml # should output \u0026quot;foo-app\u0026quot; curl localhost/foo/hostname # should output \u0026quot;bar-app\u0026quot; curl localhost/bar/hostname Bonus extra Installiamo Metal-lb - metal-lb √® un servizio che simula la presenza di un load balancer sul nostro cluster Kubernetes, quando attivo si occuper√† di assegnare un indirizzo IP al service di tipo LoadBalancer\nsudo kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml eseguiamo wget di questo file che poi andremo a editare\nwget https://kind.sigs.k8s.io/examples/loadbalancer/metallb-config.yaml editate il file inserendo un range di indirizzi IP consentiti al loadbalancer metallb, questo range deve appartenere alla vostra rete docker, nel mio caso ho editato solo questa parte del file lasciando il resto invariato\nspec: addresses: - 172.18.255.200-172.18.255.250 tradotto, metal-lb ogni volta che voi creerete un service di tipo LoadBalncer dentro Kubernetes, assegner√† un IP estrapolato da quel range, come potete vedere io ho assegnato 51 indirizzi disponibili. Applichiamo la configurazione eseguendo il seguente comando\nsudo kubectl apply -f metallb-config.yaml Adesso avete a disposizione tutti gli elementi per provare, testare e utilizzare un cluster Kubernetes. La guida termina qu√¨, spero sia stata semplice e chiara da seguire!\nPs. immagine in copertina creata da Dall-E tramite Microsoft Co-pilot (peccato per il testo :P ) sta AI √® talmente intelligente che ha pure la dislessia! LOL\n","permalink":"https://marcofanuntza.it/posts/proviamo-kubernetes-con-kind/","summary":"\u003cp\u003e\u003cstrong\u003eProviamo Kubernetes con Kind\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eQuesta guida √® indicata per tutti coloro che hanno esigenza di interagire con un cluster Kubernetes per meri scopi di test, conoscenza e sviluppo utilizzando una workstation o notebook con risorse limitate.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKind (Kubernetes IN Docker)\u003c/strong\u003e √® uno strumento open-source progettato per semplificare la creazione e la gestione di cluster Kubernetes locali utilizzando container Docker come nodi del cluster.\u003c/p\u003e\n\u003cp\u003eEcco alcune caratteristiche chiave di kind:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInstallazione Semplificata:\u003c/strong\u003e kind semplifica notevolmente il processo di installazione di Kubernetes su una macchina locale, consentendo agli sviluppatori di creare rapidamente e facilmente cluster Kubernetes per scopi di sviluppo o test.\u003c/p\u003e","title":"Proviamo Kubernetes con Kind"},{"content":"Automatic for the people √® un album dei R.E.M. mi √® venuto in mente quando ho pensato che Ansible √® un prodotto di \u0026ldquo;automation\u0026rdquo; IT.\nAnsible √® una potente e flessibile piattaforma di automazione IT progettata per semplificare e automatizzare una vasta gamma di compiti, processi e operazioni legate all\u0026rsquo;infrastruttura e allo sviluppo del software.\nDi seguito alcuni aspetti salienti:\nAnsible si distingue per la sua facilit√† d\u0026rsquo;uso, grazie a una sintassi dichiarativa basata su YAML √® accessibile anche a chi ha una conoscenza limitata della programmazione.\nAnsible opera tramite connessioni SSH, eliminando la necessit√† di installare agenti o client aggiuntivi sulle macchine di destinazione.\nAnsible consente di definire la configurazione di un\u0026rsquo;intera infrastruttura (Infrastracture as a Code) attraverso un semplice file di testo, migliorandone riproducibilit√† e versioning.\nOltre alla gestione delle configurazioni pu√≤ automatizzare una vasta gamma di operazioni inclusi il rilascio di applicazioni, il monitoraggio e la scalabilit√†.\nAnsible √® progettato per funzionare su diverse piattaforme e sul cloud, offrendo flessibilit√† e portabilit√†.\nAnsible inoltre √® in grado di gestire ambienti di varie dimensioni, rendendolo adatto per piccoli progetti fino a progetti pi√π articolati e complessi.\nNel mio ambito lavorativo Ansible era da un p√≤ che ne sentivo parlare ma personalmente non avevo mai avuto modo di utilizzarlo, con questo post sul mio blog unisco l\u0026rsquo;utile al dilettevole cogliendo l\u0026rsquo;occasione per conoscerlo meglio!\nPartiamo dalla basi prima di tutto installandolo, eseguir√≤ i test su una VM con distribuzione Ubuntu 22-04, ansible e i relativi moduli python verranno installati tramite APT\n#Installo ansible sudo add-apt-repository --yes --update ppa:ansible/ansible sudo apt install ansible-core #Verifico la versione installata sudo ansible --version ansible [core 2.15.8] config file = /etc/ansible/ansible.cfg configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python3/dist-packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/bin/ansible python version = 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (/usr/bin/python3) jinja version = 3.0.3 libyaml = True Adesso che Ansible √® installato dobbiamo avere a disposizione dei server o delle VM con cui utilizzarlo, per fare questo io personalmente ho creato 3 istanze LXC sul mio cluster Proxmox, anche in questo caso la distribuzione utilizzata √® ubuntu, giusto a titolo d\u0026rsquo;esempio qualsiasi altra distribuzione linux andrebbe bene.\nQuesto √® il recap delle macchine che ho a disposizione:\nans-serv-01 192.168.1.149\nans-serv-02 192.168.1.150\nans-serv-03 192.168.1.154\nOra i punti cardine principali di Ansible sono i file di configurazione \u0026ldquo;inventario\u0026rdquo; e \u0026ldquo;playbook\u0026rdquo;\nIl file inventario di Ansible contiene informazioni su tutti gli host che Ansible andr√† a gestire, in questo file si ha la possibilit√† di organizzare gli host in diversi gruppi in base ai loro ruoli o funzioni, come ad esempio web-server, database, server di frontend, oppure possiamo categorizzarli in base al sistema operativo. Il file in questione lo troviamo nel path /etc/ansible ed\u0026rsquo;√® chiamato hosts, andando a editarlo scoprirete che in parte √® gi√† strutturato per aiutarne la comprensione, baster√† quindi eliminare i commenti e adattarlo a piacimento.\nNoi in questo esempio andiamo a editarlo cos√¨ per questa sola parte\n[webservers] ## alpha.example.org ## beta.example.org 192.168.1.149 192.168.1.150 192.168.1.154 come potete intuire sono gli IP delle macchine elencate in precedenza, le ho inserite nel gruppo \u0026ldquo;webserver\u0026rdquo;. Verifichiamo con il comando seguente\nsudo ansible-inventory --list -y all: children: webservers: hosts: 192.168.1.149: {} 192.168.1.150: {} 192.168.1.154: {} Come annunciato in precedenza Ansible non utilizza client o agent all\u0026rsquo;interno dei server da gestire ma si avvale della sola connessione SSH, per utilizzarla per√≤ deve essere in grado di connettersi sui server di destinazione, per fare questo √® necessario copiare la chiave SSH dal server Ansible verso i server da gestire. Questo inoltre eviter√† di dover utilizzare la passwd per connettersi che andrebbe a inficiare sull\u0026rsquo;automatismo non rendendolo possibile\nIniziamo con creare la chiave sul server Ansible\n#Date sempre invio dopo il comando sudo ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:ZRAsk+qjBjQjpsLdEF39qdUmOa5ARx7QPFr6EEQhbDY root@ubuntu-22-04-lts The key's randomart image is: +---[RSA 3072]----+ | o.+OO. | | . E=..X | | +..oB * + | |.= .. = = B o | |* +.o . S + + | |+. .o. . o . | |.. . . . . | | o . | | . | +----[SHA256]-----+ la chiave pubblica adesso va copiata sui server da gestire, il comando che segue va eseguito su tutti e tre i server, la password andr√† inserita la prima volta.\nsudo ssh-copy-id root@192.168.1.149 sudo ssh-copy-id root@192.168.1.150 sudo ssh-copy-id root@192.168.1.154 Verifichiamo il buon esito eseguendo questo comando\nsudo ansible all -m ping -u root 192.168.1.149 | SUCCESS =\u0026gt; { \u0026quot;ansible_facts\u0026quot;: { \u0026quot;discovered_interpreter_python\u0026quot;: \u0026quot;/usr/bin/python3\u0026quot; }, \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } 192.168.1.150 | SUCCESS =\u0026gt; { \u0026quot;ansible_facts\u0026quot;: { \u0026quot;discovered_interpreter_python\u0026quot;: \u0026quot;/usr/bin/python3\u0026quot; }, \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } 192.168.1.154 | SUCCESS =\u0026gt; { \u0026quot;ansible_facts\u0026quot;: { \u0026quot;discovered_interpreter_python\u0026quot;: \u0026quot;/usr/bin/python3\u0026quot; }, \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } Adesso Ansible √® pronto!\nDopo il file inventario adesso √® il momento di mettere mano al file \u0026ldquo;playbook\u0026rdquo;. Rispetto al file inventario che definisce le macchine nel concetto, passatemi il termine hardware, nel file playbook invece vengono definiti gli aspetti software, quindi si andranno a dichiarare tutti gli elementi software e correlati.\nNel nostro esempio abbiamo deciso di installare Nginx, il nostro primo file playbook quindi sar√†: /etc/ansible/nginx-playbook.yml\nEcco il contenuto, √® un\u0026rsquo;esempio trovato in rete\n--- - hosts: webservers tasks: - ping: ~ - name: Update APT package manager repositories cache become: true apt: update_cache: yes - name: Upgrade installed packages become: true apt: upgrade: safe - name: Install Nginx web server become: true apt: name: nginx state: latest riuscite a intuire che operazioni eseguir√†? dai √® semplice e mi raccomando prestate attenzione all\u0026rsquo;identazione del file, √® YAML lo sai\u0026hellip;.\nSiamo pronti per provare il nostro primo playbook, per eseguirlo utilizzeremo questo comando semplice e basilare\nsudo ansible-playbook nginx-playbook.yml Ecco l\u0026rsquo;output che riceveremo\nPLAY [webservers] ****************************************************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************************************************* ok: [192.168.1.154] ok: [192.168.1.150] ok: [192.168.1.149] TASK [ping] ************************************************************************************************************************************************************ ok: [192.168.1.154] ok: [192.168.1.150] ok: [192.168.1.149] TASK [Update APT package manager repositories cache] ******************************************************************************************************************* changed: [192.168.1.150] changed: [192.168.1.154] changed: [192.168.1.149] TASK [Upgrade installed packages] ************************************************************************************************************************************** ok: [192.168.1.150] ok: [192.168.1.154] ok: [192.168.1.149] TASK [Install Nginx web server] **************************************************************************************************************************************** changed: [192.168.1.149] changed: [192.168.1.154] changed: [192.168.1.150] PLAY RECAP ************************************************************************************************************************************************************* 192.168.1.149 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.150 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.1.154 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Grande Giove!\nEntriamo in uno dei server e verifichiamo la presenza di Nginx\nssh root@192.168.1.154 Welcome to Ubuntu 23.10 (GNU/Linux 6.5.11-7-pve x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/pro Last login: Fri Jan 19 15:37:04 2024 from 192.168.1.147 root@ans-serv-03:~# ps -ef | grep nginx root 1181 1 0 15:37 ? 00:00:00 nginx: master process /usr/sbin/nginx -g daemon on; master_process on; www-data 1183 1181 0 15:37 ? 00:00:00 nginx: worker process root 1241 1231 0 15:44 pts/3 00:00:00 grep --color=auto nginx E\u0026rsquo; tutto vero! Eh si √®!\nBuon Ansible a tutti! Questo articolo termina qu√¨, √® solo un semplice esempio di quanto sia in grado di fare questo grande strumento di automazione, l\u0026rsquo;unico peccato √® non averlo utilizzato prima!\n","permalink":"https://marcofanuntza.it/posts/automazione-con-ansible/","summary":"\u003cp\u003eAutomatic for the people √® un album dei R.E.M. mi √® venuto in mente quando ho pensato che Ansible √® un prodotto di \u0026ldquo;automation\u0026rdquo; IT.\u003c/p\u003e\n\u003cp\u003eAnsible √® una potente e flessibile piattaforma di automazione IT progettata per semplificare e automatizzare una vasta gamma di compiti, processi e operazioni legate all\u0026rsquo;infrastruttura e allo sviluppo del software.\u003c/p\u003e\n\u003cp\u003eDi seguito alcuni aspetti salienti:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAnsible si distingue per la sua facilit√† d\u0026rsquo;uso, grazie a una sintassi dichiarativa basata su YAML √® accessibile anche a chi ha una conoscenza limitata della programmazione.\u003c/p\u003e","title":"Automazione con Ansible"},{"content":"Rancher come eseguire un reset della password dell\u0026rsquo;utente Admin\nSi pu√≤ capitare a tutti di dimenticare una password, ad alcuni spesso, ma niente paura possiamo eseguire un reset eseguendo questi semplici comandi che seguono..\nL\u0026rsquo;esempio che segue mostra come eseguire il reset della password dell\u0026rsquo;utente admin di Rancher installato all\u0026rsquo;interno di un cluster Kubernetes, le operazioni in parte sono valide anche nel caso il vostro Rancher fosse stato installato su un semplice container docker.\nPrerequisiti\navere accesso al cluster Kubernetes tramite kubectl avere accesso al server che esegue docker (nel secondo caso) Procedura\nTramite kubectl individuiamo il pod che sta erogando il servizio Rancher\nkubectl get pods -n cattle-system | grep Running rancher-webhook-7476c74c6c-scbvs 1/1 Running 0 42m rancher-7484b7b4c5-jb9dt 1/1 Running 0 42m a questo punto dobbiamo entrare all\u0026rsquo;interno della console del pod rancher-7484b7b4c5-jb9dt, per farlo eseguiremo\nkubectl exec --stdin --tty rancher-7484b7b4c5-jb9dt -n cattle-system -- /bin/bash noterete che il prompt dei comandi sar√† cambiato, siamo all\u0026rsquo;interno del pod e possiamo eseguire dei comandi diretti, quello specifico che far√† al caso nostro √® un semplice \u0026ldquo;reset password\u0026rdquo;\nreset-password New password for default admin user (user-m258d): tQXngoMxYBtugFp4Xcjg dopo aver copiato la password potete uscire dalla console con \u0026ldquo;exit\u0026rdquo;.\nCon questi semplici passaggi la vostra password admin √® stata rigenereata e sarete nuovamente in grado di accedere al vostro Rancher con utenza admin!\n","permalink":"https://marcofanuntza.it/posts/reset-password-admin-rancher/","summary":"\u003cp\u003e\u003cstrong\u003eRancher come eseguire un reset della password dell\u0026rsquo;utente Admin\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSi pu√≤ capitare a tutti di dimenticare una password, ad alcuni spesso, ma niente paura possiamo eseguire un reset eseguendo questi semplici comandi che seguono..\u003c/p\u003e\n\u003cp\u003eL\u0026rsquo;esempio che segue mostra come eseguire il reset della password dell\u0026rsquo;utente admin di Rancher installato all\u0026rsquo;interno di un cluster Kubernetes, le operazioni in parte sono valide anche nel caso il vostro Rancher fosse stato installato su un semplice container docker.\u003c/p\u003e","title":"Reset password utente admin su Rancher"},{"content":"Kubernetes √® un sistema di gestione (orchestratore) di container che √® diventato di fatto lo standard per distribuire applicazioni containerizzate.\nQuesto perch√© Kubernetes √® potente, affidabile, flessibile e per lo pi√π facile da usare (come no).\nSi facile da utilizzare dopo che si supera il primo scoglio iniziale..\nIo personalmente ho avuto difficolt√† nel visualizzare mentalmente il cluster e tutti i componenti che ne facevano parte utilizzando solo gli strumenti della riga di comando finch√© non ho familiarizzato con la sua struttura.\nPer fare ci√≤ √® stato di grandissimo aiuto (per me) l\u0026rsquo;utilizzo di Rancher!\nCon questo articolo spero di aiutare tutti gli interessati che vogliono conoscere e sperimentare Kubernetes, il risultato finale di questa guida vi permetter√† di avere una sorta di laboratorio sulla vostra workstation/server e toccare con mano un cluster Kubernetes.\nPer raggiungere le scopo verranno installati i seguenti elementi:\nDocker: Il sistema pi√π diffuso per gestire container\nKubectl: Strumento a riga di comando utilizzato per controllare il cluster Kubernetes\nHelm: Gestore di pacchetti per Kubernetes. Consente di installare, aggiornare e gestire applicazioni su un cluster Kubernetes.\nK3d: k3d √® un progetto guidato dalla community, supportato da Rancher (SUSE). √à un wrapper per eseguire k3s in Docker.\nK3s: √à una distribuzione Kubernetes pronta per la produzione, molto leggera, sviluppata da Rancher.\nRancher: Banalmente potrebbe essere considerata una GUI per Kubernetes ma fa molto di pi√π! Permette di gestire e configurare pi√π cluster Kubernetes da un unico punto di controllo.\nPremessa\navere a disposizione una workstation o server con distribuzione Linux (si potrebbe utilizzare anche WSL2 su Windows ma non lo tratteremo in questo articolo) accesso alla rete per scaricare pacchetti Procedura parte 1\nQui procediamo con installazione degli elementi Docker, K3D, Helm e Kubectl\nDocker\n# Aggiungiamo la chiave **GPG key** ufficiale del repository Docker sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \u0026quot;deb [arch=\u0026quot;$(dpkg --print-architecture)\u0026quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \u0026quot;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026quot;$VERSION_CODENAME\u0026quot;)\u0026quot; stable\u0026quot; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io K3D\nsudo wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash Helm\nsudo curl -s https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash Kubectl\nsudo curl -LO \u0026quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x kubectl sudo mv kubectl /usr/local/bin Procedura parte 2\nAdesso procederemo con la creazione del cluster Kubernetes tramite K3D utilizzando K3S e infine completeremo installando Rancher tramite Helm.\nCreazione cluster Kubernetes\nsudo k3d cluster create k8s-test-rancher -p \u0026quot;8900:30080@agent:0\u0026quot; -p \u0026quot;8901:30081@agent:0\u0026quot; -p \u0026quot;8902:30082@agent:0\u0026quot; --agents 2 Il comando andr√† a creare un cluster chiamato \u0026ldquo;k8s-test-rancher\u0026rdquo; con 3 porte esposte: 30080, 30081 e 30082. Verranno mappate rispettivamente alle porte 8900, 8901 e 8902 della workstation o server che state utilizzando. Il cluster sa√† composto da 1 nodo master e 2 nodi worker.\nOra facciamo in modo che Kubectl conosca il file di configurazione del cluster eseguendo il seguente comando\nsudo k3d kubeconfig merge k8s-test-rancher --kubeconfig-switch-context Eseguiamo ora una prima verifica della presenza del cluster e dei nodi elencati\nsudo kubectl get nodes Se tutto procede come dovrebbe dovreste avere lo stesso risultato di sotto\nNAME STATUS ROLES AGE VERSION k3d-k8s-test-rancher-server-0 Ready control-plane,master 36s v1.27.4+k3s1 k3d-k8s-test-rancher-agent-0 Ready \u0026lt;none\u0026gt; 32s v1.27.4+k3s1 k3d-k8s-test-rancher-agent-1 Ready \u0026lt;none\u0026gt; 32s v1.27.4+k3s1 Adesso siamo pronti per l\u0026rsquo;ultimo passaggio fondamentale, installare Rancher tramite Helm\nsudo helm repo add rancher-latest https://releases.rancher.com/server-charts/latest sudo helm install rancher rancher-latest/rancher --namespace cattle-system --create-namespace --set ingress.enabled=false --set tls=external --set replicas=1 Nel frattempo che Helm completer√† l\u0026rsquo;installazione, se siete cusiosi potete interagire con il cluster e verificare cosa stia installando\nsudo kubectl get deployment -n cattle-system sudo kubectl get pods -n cattle-system A questo punto dobbiamo metterci nelle condizioni di poter raggiungere Rancher sulla propria web-gui, per farlo √® necessario creare un NodePort. Create questo file yaml\nsudo vi rancher.yaml al suo interno incollate questa dichiarazione, che sostanzialmente eseguir√† un match con le porte settate in precedenza\napiVersion: v1 kind: Service metadata: labels: app: rancher name: ranchernodeport namespace: cattle-system spec: ports: - name: http nodePort: 30080 port: 80 protocol: TCP targetPort: 80 - name: https nodePort: 30081 port: 443 protocol: TCP targetPort: 443 selector: app: rancher type: NodePort Attenzione √® importante sia identato correttamente, dovete rispettare gli spazi, yaml non perdona, lo scoprirete presto :D\nAttiviamo il NodePort eseguendo il comando\nsudo kubectl apply -f rancher.yaml Adesso potete utilizzare il vostro browser e chiamare la seguente url\nhttps://vostroip:8901/dashboard/auth/login Non date importanza al warning sul certificato √® perche utilizza un self signed, andate avanti e vi ritroverete la pagina web di Rancher, vi verr√† suggerito come recuperare la passwd seguite le indicazioni con il comando kubectl\nBenvenuti su Rancher il vostro cluster Kubernetes √® pronto! la guida termina qu√¨, spero sia stata semplice e chiara da seguire!\nps. immagine in copertina creata da Dall-E tramite co-pilot\n","permalink":"https://marcofanuntza.it/posts/proviamo-kubernetes-e-rancher-con-k3d/","summary":"\u003cp\u003eKubernetes √® un sistema di gestione (orchestratore) di container che √® diventato di fatto lo standard per distribuire applicazioni containerizzate.\u003c/p\u003e\n\u003cp\u003eQuesto perch√© Kubernetes √® potente, affidabile, flessibile e per lo pi√π facile da usare (come no).\u003c/p\u003e\n\u003cp\u003eSi facile da utilizzare dopo che si supera il primo scoglio iniziale..\u003c/p\u003e\n\u003cp\u003eIo personalmente ho avuto difficolt√† nel visualizzare mentalmente il cluster e tutti i componenti che ne facevano parte utilizzando solo gli strumenti della riga di comando finch√© non ho familiarizzato con la sua struttura.\u003c/p\u003e","title":"Proviamo Kubernetes con Rancher"},{"content":"Iniziamo con capire che cos\u0026rsquo;√® K3D e non confondiamolo con K3S\nK3D\nK3D √® un \u0026ldquo;wrapper\u0026rdquo; che come scrive Wikipedia \u0026ldquo;√® un\u0026rsquo;avvolgitore, un modulo software che ne \u0026ldquo;riveste\u0026rdquo; un altro\u0026rdquo; Si la traduzione dall\u0026rsquo;inglese non √® felicissima, in questo caso specifico consente di eseguire K3S, che √® la distribuzione minimale di Kubernetes sviluppata da Rancher Labs, all\u0026rsquo;interno di Docker.\nIn altre parole, K3D semplifica la creazione e la gestione di cluster Kubernetes leggeri e portatili che utilizzano K3S, rendendo il processo pi√π agevole, specialmente per coloro che sviluppano in locale utilizzando tecnologie Kubernetes.\n√à importante notare che K3D √® un progetto guidato dalla community, non √® un prodotto ufficiale di Rancher (SUSE) che ha creato e mantiene K3S. Tuttavia, K3D offre un\u0026rsquo;opzione comoda e flessibile per coloro che desiderano utilizzare K3S all\u0026rsquo;interno di container Docker.\nPrerequisiti\navere a disposizione un server o una VM con distribuzione linux (si pu√≤ installare anche su Windows o Mac ma non lo tratteremo qu√¨) accesso alla rete per scaricare files curl e/o wget installati avere docker installato, per installarlo vi ricordo un precedente articolo QUI Procedura\nPer installare K3D sostanzialmente si tratter√† semplicemente di scaricare uno script bash che si occuper√† in totale autonomia dell\u0026rsquo;installazione, abbiamo due alternative, utilizzare curl oppure wget, c\u0026rsquo;√® da dire che lo script utilizzer√† curl per scaricare i files quindi curl dovete installarlo comunque.\nsudo apt install curl -y sudo wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash oppure tramite curl\nsudo curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash Attendete che lo script completi e poi verificatene il risultato con\nk3d version k3d version v5.6.0 k3s version v1.27.4-k3s1 (default) Ora che K3D √® installato possiamo provare a creare il nostro primo cluster Kubernetes (K3S) eseguendo questo comando\nsudo k3d cluster create mycluster Attendete il completamento, nel mio caso ha completato in poco pi√π di 30 secondi\nINFO[0000] Prep: Network INFO[0000] Created network 'k3d-mycluster' INFO[0000] Created image volume k3d-mycluster-images INFO[0000] Starting new tools node... INFO[0001] Creating node 'k3d-mycluster-server-0' INFO[0001] Pulling image 'ghcr.io/k3d-io/k3d-tools:5.6.0' INFO[0003] Pulling image 'docker.io/rancher/k3s:v1.27.4-k3s1' INFO[0003] Starting Node 'k3d-mycluster-tools' INFO[0011] Creating LoadBalancer 'k3d-mycluster-serverlb' INFO[0012] Pulling image 'ghcr.io/k3d-io/k3d-proxy:5.6.0' INFO[0017] Using the k3d-tools node to gather environment information INFO[0017] HostIP: using network gateway 172.18.0.1 address INFO[0017] Starting cluster 'mycluster' INFO[0017] Starting servers... INFO[0017] Starting Node 'k3d-mycluster-server-0' INFO[0022] All agents already running. INFO[0022] Starting helpers... INFO[0022] Starting Node 'k3d-mycluster-serverlb' INFO[0028] Injecting records for hostAliases (incl. host.k3d.internal) and for 2 network members into CoreDNS configmap... INFO[0030] Cluster 'mycluster' created successfully! INFO[0030] You can now use it like this: kubectl cluster-info Ora come suggerisce l\u0026rsquo;ultima riga dell\u0026rsquo;output, per interagire con il cluster dobbiamo utilizzare kubectl, installiamolo quindi\nsudo curl -LO \u0026quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x kubectl sudo mv kubectl /usr/local/bin Dobbiamo fare in modo che kubectl conosca il file di configurazione del cluster\nsudo k3d kubeconfig merge mycluster --kubeconfig-switch-context Adesso siamo pronti per interagire con il cluster\nsudo kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k3d-mycluster-server-0 Ready control-plane,master 12m v1.27.4+k3s1 172.18.0.2 \u0026lt;none\u0026gt; K3s dev 5.15.0-91-generic containerd://1.7.1-k3s1 Come potrete notare il cluster √® composto da un unico nodo K3S, ma questo sar√† comunque sufficiente per eseguire tutti i test e le esigenze di sviluppo.\nSia comunque ben chiaro che questo √® solo un esempio, K3D tramite ulteriori opzioni e file di configurazione vi permette di creare cluster con pi√π nodi. QUI la documentazione ufficiale.\nContinuando a curiosare potete vedere cosa ha installato K3D\nsudo kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 20m kube-system kube-dns ClusterIP 10.43.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 20m kube-system metrics-server ClusterIP 10.43.151.46 \u0026lt;none\u0026gt; 443/TCP 20m kube-system traefik LoadBalancer 10.43.61.96 172.18.0.2 80:32526/TCP,443:32411/TCP 19m sudo kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-957fdf8bc-g6vsb 1/1 Running 0 20m kube-system coredns-77ccd57875-h6krb 1/1 Running 0 20m kube-system metrics-server-648b5df564-nn5kk 1/1 Running 0 20m kube-system helm-install-traefik-crd-52bfs 0/1 Completed 0 20m kube-system helm-install-traefik-nzmhv 0/1 Completed 1 20m kube-system svclb-traefik-caab8633-t7m98 2/2 Running 0 19m kube-system traefik-64f55bb67d-gnvf8 1/1 Running 0 19m Personalmente conosco anche un altra alternativa per creare velocemente un cluster kubernetes, forse scriver√≤ un prossimo articolo, l\u0026rsquo;alternativa cmq √® kind\nAggiornamento Ho scritto un\u0026rsquo;articolo specifico su Kind QUI\n","permalink":"https://marcofanuntza.it/posts/come-installare-k3d/","summary":"\u003cp\u003eIniziamo con capire che cos\u0026rsquo;√® \u003cstrong\u003eK3D\u003c/strong\u003e e non confondiamolo con \u003cstrong\u003eK3S\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eK3D\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eK3D √® un \u0026ldquo;wrapper\u0026rdquo; che come scrive \u003ca href=\"https://it.wikipedia.org/wiki/Wrapper\"\u003eWikipedia\u003c/a\u003e \u0026ldquo;√® un\u0026rsquo;avvolgitore, un modulo software che ne \u0026ldquo;riveste\u0026rdquo; un altro\u0026rdquo; Si la traduzione dall\u0026rsquo;inglese non √® felicissima, in questo caso specifico consente di eseguire K3S, che √® la distribuzione minimale di Kubernetes sviluppata da Rancher Labs, all\u0026rsquo;interno di Docker.\u003c/p\u003e\n\u003cp\u003eIn altre parole, K3D semplifica la creazione e la gestione di cluster Kubernetes leggeri e portatili che utilizzano K3S, rendendo il processo pi√π agevole, specialmente per coloro che sviluppano in locale utilizzando tecnologie Kubernetes.\u003c/p\u003e","title":"Come installare K3D"},{"content":"Premessa\nQuesta guida mostra i comandi da eseguire per creare un template di una VM da utilizzare su Proxmox, la distro utilizzata √® Ubuntu e l\u0026rsquo;immagine sar√† una versione specifica per il cloud.\nLe Immagini Cloud sono piccole immagini certificate e pronte per il cloud, hanno Cloud Init preinstallato e pronto per accettare una Cloud Config.\nI comandi verranno tutti eseguiti da shell all\u0026rsquo;interno di un nodo Proxmox\nProcedura\nIniziamo scaricando l\u0026rsquo;immagine Ubuntu dalla pagina specifica Ubuntu Cloud Images per questa guida utilizzeremo Ubuntu Server 24.04 LTS (Noble Numbat)\nwget https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img Creiamo la VM che diventer√† la base per il successivo template\nqm create 2000 --memory 2048 --core 2 --name ubuntu-cloud-noble --net0 virtio,bridge=vmbr0 Importiamo l\u0026rsquo;immagine precedentemente scaricata sul volume locale mettendola a disposizione della VM\nqm importdisk 2000 noble-server-cloudimg-amd64.img local-lvm Attendiamo il trasferimento e poi procederemo con collegare il nuovo disco alla VM come un\u0026rsquo;unit√† SCSI sul controller SCSI\nqm set 2000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-2000-disk-0 Adesso aggiungiamo il cloud-init drive, questo √® una parte indispensabile per la creazione del template, permette di cambiare la configurazione dell\u0026rsquo;immagine ogni volta che verr√† utilizzato il template\nqm set 2000 --ide2 local-lvm:cloudinit Rendiamo l\u0026rsquo;unit√† cloud-init avviabile e limitiamo i permessi al solo BIOS\nqm set 2000 --boot c --bootdisk scsi0 Aggiungiamo infine serial console\nqm set 2000 --serial0 socket --vga serial0 ATTENZIONE a questo punto non avviate la VM appena creata, siamo pronti per creare il template eseguendo quest\u0026rsquo;ultimo semplice comando\nqm template 2000 Renamed \u0026quot;vm-2000-disk-0\u0026quot; to \u0026quot;base-2000-disk-0\u0026quot; in volume group \u0026quot;pve\u0026quot; Logical volume pve/base-2000-disk-0 changed. WARNING: Combining activation change with other commands is not advised. Adesso avete a disposizione un template che vi permetter√† di generare N immagini Ubuntu senza il bisogno di dover ogni volta rieseguire una nuova installazione, insomma √® un template!\nChiudiamo la guida con un semplice esempio creando una nuova VM partendo dal template appena creato\nqm clone 2000 110 --name ubuntu-noble-01 --full Verificando possiamo notare la VM appena creata con ID 110 e nome ubuntu-noble-01\nqm list VMID NAME STATUS MEM(MB) BOOTDISK(GB) PID 103 haos11.2 stopped 4096 32.00 0 110 ubuntu-noble-01 stopped 2048 3.50 0 2000 ubuntu-cloud-noble stopped 2048 3.50 0 Noterete anche che l\u0026rsquo;immagine ha un disco di soli 3.5GB, nessun problema in base alle vostre esigenze potete ridimensionare il disco eseguendo questo successivo comando\nqm resize 110 scsi0 15G Size of logical volume pve/vm-110-disk-0 changed from 3.50 GiB (896 extents) to 15.00 GiB (3840 extents). Logical volume pve/vm-110-disk-0 successfully resized. Adesso siamo pronti per far partire la nostra nuova VM creata dal template.. ma manca qualcosa, spero i pi√π attenti ci abbiano fatto caso, che user e passwd dobbiamo utilizzare per il login?\nE\u0026rsquo; qui che entra in gioco cloud-init, aprite la web-gui di Proxmox e posizionatevi sulle voci di men√π relative alla VM, cliccate sulla voce Cloud-init e noterete le voci che si possono editare, modificatele e provate a far partire la VM.\nPer l\u0026rsquo;accesso tramite SSH da remoto dovete utilizzare direttamente la vostra chiave SSH, si potete inserirla direttamente come sopra.\nLa guida termina qui, se velete utilizzare una distribuzione diversa da Ubuntu niente vi vieta di farlo, la guida va semplicemente adattata utilizzando un\u0026rsquo;immagine diversa.\n","permalink":"https://marcofanuntza.it/posts/come-creare-template-ubuntu-su-proxmox/","summary":"\u003cp\u003e\u003cstrong\u003ePremessa\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eQuesta guida mostra i comandi da eseguire per creare un template di una VM da utilizzare su Proxmox, la distro utilizzata √® Ubuntu e l\u0026rsquo;immagine sar√† una versione specifica per il cloud.\u003c/p\u003e\n\u003cp\u003eLe Immagini Cloud sono piccole immagini certificate e pronte per il cloud, hanno Cloud Init preinstallato e pronto per accettare una Cloud Config.\u003c/p\u003e\n\u003cp\u003eI comandi verranno tutti eseguiti da shell all\u0026rsquo;interno di un nodo Proxmox\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProcedura\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIniziamo scaricando l\u0026rsquo;immagine Ubuntu dalla pagina specifica \u003ca href=\"https://cloud-images.ubuntu.com/\"\u003eUbuntu Cloud Images\u003c/a\u003e per questa guida utilizzeremo Ubuntu Server 24.04 LTS (Noble Numbat)\u003c/p\u003e","title":"Come creare template Ubuntu su Proxmox"},{"content":"Come installare Docker e Docker compose su Ubuntu\nQuesta guida elenca passo per passo la procedura da seguire per installare docker, docker compose e containerd su distribuzione Ubuntu.\nPrerequisiti:\nserver o workstation con distribuzione ubuntu accesso alla rete per scaricare i pacchetti Procedura\nTutti i comandi verranno eseguiti da terminale, se in precedenza avevate gi√† provato un\u0026rsquo;installazione di Docker sarebbe opportuno rimuoverla eseguendo il comando che segue:\nsudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras adesso si pu√≤ procedere con l\u0026rsquo;installazione, si parte prima di tutto aggiungendo il repository ufficiale Docker\n# Aggiungiamo la chiave **GPG key** ufficiale del repository Docker: sudo apt-get update sudo apt-get install ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg aggiungiamo il repository ufficiale Docker al sistema APT:\necho \\ \u0026quot;deb [arch=\u0026quot;$(dpkg --print-architecture)\u0026quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \u0026quot;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026quot;$VERSION_CODENAME\u0026quot;)\u0026quot; stable\u0026quot; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update qui il comando per installazione dei pacchetti necessari:\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin verifichiamo le versioni installate di Docker e Docker compose:\ndocker -v docker compose a questo punto possiamo considerare completata l\u0026rsquo;installazione, in via opzionale ci resta solamente abilitare il nostro user per utilizzo del comando docker senza il bisogno di utilizzare ogni volta sudo, per farlo aggiungiamo semplicemente lo user al gruppo docker\nsudo usermod -aG docker $USER that\u0026rsquo;all folks!\n","permalink":"https://marcofanuntza.it/posts/come-installare-docker-e-docker-compose-su-ubuntu/","summary":"\u003cp\u003eCome installare Docker e Docker compose su Ubuntu\u003c/p\u003e\n\u003cp\u003eQuesta guida elenca passo per passo la procedura da seguire per installare docker, docker compose e containerd su distribuzione Ubuntu.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePrerequisiti:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eserver o workstation con distribuzione ubuntu\u003c/li\u003e\n\u003cli\u003eaccesso alla rete per scaricare i pacchetti\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eProcedura\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTutti i comandi verranno eseguiti da terminale, se in precedenza avevate gi√† provato un\u0026rsquo;installazione di Docker sarebbe opportuno rimuoverla eseguendo il comando che segue:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eadesso si pu√≤ procedere con l\u0026rsquo;installazione, si parte prima di tutto aggiungendo il repository ufficiale Docker\u003c/p\u003e","title":"Come installare Docker e Docker Compose su Ubuntu"},{"content":"Questo articolo continua il log partito da QUI dove spiegavo la scelta e il perch√® .\nI componenti che attendevo sono arrivati, oltre la lista iniziale ho apportato alcune integrazioni aggiungendo una scheda PCI che di fatto mette a disposizione due porte SATA III aggiuntive. A questa scheda sono direttamente connessi i due dischi da 2,5 pollici, 2TB cadauno. Altra integrazione √® un ulteriore banco di ram da 8GB.\nLa prima installazione √® abbastanza semplice, se avete gi√† avuto modo di installare una distro linux da pendrive USB sar√† una passeggiata. Per creare la pendrive USB bootabile ho utilizzato il software Balena Etcher e la ISO di Truenas Core potete ovviamente trovarla sui repository ufficiali. La procedura guidata vi chieder√† su quale disco installare l\u0026rsquo;OS, imposterete una password e successivamente sar√† il turno della rete, finito! nel mio caso ha completato l\u0026rsquo;installazione in pochi minuti. A questo link comunque potete seguire la pagina ufficiale con immagini passo per passo.\nOra potete anche scollegare monitor e tastiera dal server, tutto il resto potr√† essere eseguito dalla web-gui di Truenas!\nIn questa galleria di immagini seguente potete vedere la dashboard iniziale che presenta con Truens CORE\n.\nPrime operazioni basilari che ho eseguito su TrueNAS CORE\nImpostato default gateway: questo √® indispensabile per cercare/installare aggiornamenti e installare plugin jail Impostato DNS: ovviamente, no dns no party Creato primo pool: primo passaggio per inizializzare lo storage a disposizione Le mie esigenze\nIn aggiornamento‚Ä¶..\n","permalink":"https://marcofanuntza.it/2024/01/13/il-mio-nuovo-nas-con-truenas-part2/","summary":"\u003cp\u003eQuesto articolo continua il log partito da \u003ca href=\"https://marcofanuntza.it/2024/01/01/il-mio-nuovo-nas-con-truenas/\"\u003eQUI\u003c/a\u003e dove spiegavo la scelta e il perch√® .\u003c/p\u003e\n\u003cp\u003eI componenti che attendevo sono arrivati, oltre la lista iniziale ho apportato alcune integrazioni aggiungendo una scheda PCI che di fatto mette a disposizione due porte SATA III aggiuntive. A questa scheda sono direttamente connessi i due dischi da 2,5 pollici, 2TB cadauno. Altra integrazione √® un ulteriore banco di ram da 8GB.\u003c/p\u003e\n\u003cp\u003eLa prima installazione √® abbastanza semplice, se avete gi√† avuto modo di installare una distro linux da pendrive USB sar√† una passeggiata. Per creare la pendrive USB bootabile ho utilizzato il software \u003cstrong\u003eBalena Etcher\u003c/strong\u003e e la ISO di Truenas Core potete ovviamente trovarla sui repository ufficiali. La procedura guidata vi chieder√† su quale disco installare l\u0026rsquo;OS, imposterete una password e successivamente sar√† il turno della rete, finito! nel mio caso ha completato l\u0026rsquo;installazione in pochi minuti. A questo \u003ca href=\"https://www.truenas.com/blog/how-to-install-truenas-core/\"\u003elink\u003c/a\u003e  comunque potete seguire la pagina ufficiale con immagini passo per passo.\u003c/p\u003e","title":"Il mio nuovo NAS con Truenas part II"},{"content":"Introduzione:\nQuando ho deciso di riscrivere sul blog il cms scelto inizialmente era stato Wordpress, per ambito lavorativo avevo gi√† gestito server wordpress decine di volte, avevo avuto anche un\u0026rsquo;esperienza come writer assiduo sul defunto blog actioncamitalia, la scelta quindi si era basata esclusivamente sull\u0026rsquo;esperienza passata. Dopo un p√≤ mi sono accorto per√≤ che per le mie esigenze, per le esigenze di questo blog specifico, l\u0026rsquo;utilizzo delle risorse necessarie per worpress erano sprecate, insomma non ne avevo bisogno.\nDa qui √® partita la ricerca su un nuovo CMS per il mio blog.\nCi sono molte piattaforme di blogging. Essendo un sistemista, le mie esigenze per una piattaforma di blogging potrebbero differire da quelle della maggior parte dei blogger. Io vorrei che il mio blog sia:\nFacile da mantenere per quanto riguarda gli aggiornamenti del software Semplice nelle sue funzionalit√† Facile per me da configurare Trasparente su ci√≤ che sta accadendo sotto il cofano Che cos\u0026rsquo;√® Hugo?\nHugo √® un framework open source per la generazione di siti web statici. Creato utilizzando il linguaggio di programmazione Go (o Golang), Hugo si distingue per la sua velocit√† straordinaria nella generazione di contenuti statici. Alcune caratteristiche chiave di Hugo includono:\nVelocit√†: Grazie alla sua implementazione in Go, Hugo √® notevolmente veloce nella generazione di siti web statici, rendendo il processo efficiente e immediato.\nSemplicit√†: Hugo √® progettato per essere facile da usare e comprendere. La sua struttura chiara e la documentazione completa facilitano la creazione e la gestione di siti web.\nFlessibilit√†: Supporta temi e layout personalizzabili, consentendo agli sviluppatori di adattare l\u0026rsquo;aspetto e la struttura del sito secondo le proprie esigenze.\nGenerazione di Contenuti Statici: Hugo crea siti web completamente statici, eliminando la necessit√† di un server di backend. Ci√≤ li rende sicuri, facili da distribuire e veloci da caricare.\nInstallare Hugo\nSu qualsiasi distribuzione linux √® abbastanza semplice installare Hugo, i vari snap, apt e yum hanno nei loro repository i pacchetti necessari, ma c\u0026rsquo;√® da dire che spesso non sono aggiornati. Il mio consiglio √® scaricarvi il pacchetto pi√π recente e installarlo a manina.\nNel mio caso specifico ho deciso di installare Hugo su un container LXC erogato dal mio cluster Proxmox, distribuzione ho scelto una ubuntu 23-10, dal sito di Hugo ho scaricato l\u0026rsquo;ultima release disponibile del pacchetto deb\nnota. scegliete la versione \u0026ldquo;extended\u0026rdquo;\nwget https://github.com/gohugoio/hugo/releases/download/v0.121.2/hugo_extended_0.121.2_linux-amd64.deb eseguito il comando di installazione:\ndpkg -i hugo_extended_0.121.2_linux-amd64.deb qui il risultato:\nhugo version hugo v0.121.2-6d5b44305eaa9d0a157946492a6f319da38de154+extended linux/amd64 BuildDate=2024-01-05T12:21:15Z VendorInfo=gohugoio Primi Passi\nPrima di tutto vi consiglio di leggere la documentazione di Hugo e consiglio di partire dalla quick-start\nIo non l\u0026rsquo;ho seguita alla lettera ma ho apportato alcune modifiche, procediamo con la creazione del nostro primo sitoweb con Hugo, posizionatevi in un path qualunque sul vostro server e eseguite il seguente comando:\nhugo new site mio-nuovo-blog --format yaml descrivendo nello specifico il comando, la prima parte istruisce hugo per creare un sito, mio-nuovo-blog sar√† il nome del sito e l\u0026rsquo;opzione -format yaml far√† in modo che i file di configurazione vengano formattati in formato yaml invece che toml, a mio avviso pi√π semplice e intuitivo a prima vista.\nScoprirete che hugo ha creato una nuova directory con il nome del sito e posizionato al suo interno tutti i files necessari, dovreste ritrovarvi in questa simile situazione\nIl passo successivo sar√† scegliere e abiltare un tema da utilizzare, date un\u0026rsquo;occhiata sul sito ufficiale e scegliete quello che preferite. Sono quasi tutti ben documentati e la procedura per attivarli √® quasi sempre la stessa, sostanzialmente va utilizzato git e scaricato il repository all\u0026rsquo;interno del path creato in precedenza da Hugo.\nIo ho scelto il tema PaperMod e l\u0026rsquo;ho abilitato come submodulo di git con i seguenti comandi:\ngit init questo vi servir√† anche nel caso voi decidiate di versionare il vostro sito su GitHub, procediamo poi con:\ngit submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive # needed when you reclone your repo (submodules may not get cloned automatically) Ora troverete il tema installato e a disposizione per il vostro nuovo sito, i files del tema sono posizionati all\u0026rsquo;interno del path themes/PaperMod.\nIl file che editerete principalmente sar√† config.yaml, questo √® di fatto il punto principale di controllo per il vostro sito Hugo.\nMa finiamo con un\u0026rsquo;ultimo comando poi lascer√≤ a voi il bello di configurarvi il vostro sito Hugo!\nCome creo il mio primo post?\nBene niente di pi√π semplice, per farlo utilizziamo nuovamente un comando hugo\nhugo new content posts/il-mio-primo-post.md questo comando creer√† un nuovo post vuoto e come potrete intuire sar√† sul path content/posts/\nA questo punto credo sia arrivato il momento di lasciarvi scoprire come andare avanti con Hugo in base alle vostre esigenze, se volete comunque dare un\u0026rsquo;occhiata al codice del mio blog, questo stesso che sta leggendo, vi lascio il link al mio repository su GitHub\nPeace \u0026amp; Love!\n","permalink":"https://marcofanuntza.it/2024/01/12/ho-scelto-hugo/","summary":"\u003cp\u003e\u003cstrong\u003eIntroduzione:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eQuando ho deciso di riscrivere sul blog il cms scelto inizialmente era stato Wordpress, per ambito lavorativo avevo gi√† gestito server wordpress decine di volte, avevo avuto anche un\u0026rsquo;esperienza come writer assiduo sul defunto blog actioncamitalia, la scelta quindi si era basata esclusivamente sull\u0026rsquo;esperienza passata.\nDopo un p√≤ mi sono accorto per√≤ che per le mie esigenze, per le esigenze di questo blog specifico, l\u0026rsquo;utilizzo delle risorse necessarie per worpress erano sprecate, insomma non ne avevo bisogno.\u003c/p\u003e","title":"Ho scelto Hugo!"},{"content":"Che ne sar√† di noi mi verrebbe da dire.. oggi stavo continuando con il tuning del nuovo tema Hugo che sto utilizzando e ho notato che √® possibile inserire un logo in homepage, quindi ho avuto la brillante idea di chiedere a bing-chat e alla nuova funzione di Microsoft chiamata Co-Pilot di generare per me un\u0026rsquo;immagine logo con l\u0026rsquo;ausilio della intelligenza artificiale. Come avrete potuto gi√† intuire la situazione mi √® leggermente sfuggita di mano.\nCo-pilot √® un assistente virtuale basato sull‚Äôintelligenza artificiale che ti aiuta a svolgere diverse attivit√† con una semplice chat. Puoi chiedere a Co-pilot di suggerirti codici, migliorare la tua scrittura, fornirti informazioni utili e molto altro. Co-pilot √® integrato in vari prodotti Microsoft, come Edge, Windows 11, Word, Excel e Teams.\nMicrosoft come suo solito fare ha acquisito DALL-E che √® un algoritmo di intelligenza artificiale capace di generare immagini a partire da descrizioni testuali attraverso la sintografia.\nSe anche voi volete provarlo vi baster√† utilizzare il browser nativo di Microsoft Edge, cercate Bing Chat, tra i risultati vi verr√† proposto di attivare e provare Co-Pilot. Sulla parte destra della pagina apparir√† una finestra dove sar√† possibile interagire con Co-Pilot, baster√† scrivere:\n\u0026quot;Genera immagine che mostra una gatto con lo sfondo di un bosco incantato, il gatto deve essere terrificante\u0026quot; Che esempio del piffero che ho fatto, comunque tutto questo per farvi capire che il vostro unico limite sar√† la fantasia!\nA me stranamente invece √® venuta idea di creare immagini con ragazze asiatiche con sullo sfondo citt√† Asiatiche. Sawadee Kaaap!\n","permalink":"https://marcofanuntza.it/2024/01/01/pensare-che-cercavo-un-logo/","summary":"\u003cp\u003eChe ne sar√† di noi mi verrebbe da dire.. oggi stavo continuando con il tuning del nuovo tema Hugo che sto utilizzando e ho notato che √® possibile inserire un logo in homepage, quindi ho avuto la brillante idea di chiedere a bing-chat e alla nuova funzione di Microsoft chiamata Co-Pilot di generare per me un\u0026rsquo;immagine logo con l\u0026rsquo;ausilio della intelligenza artificiale.\nCome avrete potuto gi√† intuire la situazione mi √® leggermente sfuggita di mano.\u003c/p\u003e","title":"E pensare che cercavo un logo.."},{"content":"Quanto vi infastidiscono gli annunci pubblicitari navigando sul web? Ormai ci sono pagine web piene di annunci, pop-up fastidiosissimi che non fanno altro che farci perdere tempo e voglia di visitarne il sito, i quotidiani con le news e le notizie sportive su tutti sono i pi√π scassa bit.\nPer fortuna ci viene in aiuto Pi-Hole!\nPi-hole √® un software open-source progettato per il controllo e la gestione della rete orientato nello specifico proprio per combattere la pubblicit√† e gli annunci correlati, sostanzialmente agisce come un filtro DNS, offrendo funzionalit√† avanzate per bloccare tutti gli annunci pubblicitari, tracker e tutti i contenuti indesiderati ancor prima che raggiungano i nostri dispositivi connessi alla rete.\nCaratteristiche Principali:\nBlocco degli Annunci: Pi-hole utilizza liste di blocco per intercettare le richieste DNS associate agli annunci pubblicitari, consentendo di eliminare la visualizzazione di annunci indesiderati su tutti i dispositivi connessi alla rete.\nFiltro DNS: Attraverso il blocco delle richieste DNS indesiderate, Pi-hole inoltre impedisce l‚Äôaccesso a siti malevoli contenenti malware o altri contenuti indesiderati, contribuendo a migliorare la sicurezza della navigazione.\nMonitoraggio delle Prestazioni: Pi-hole fornisce statistiche dettagliate sulle richieste DNS e sui domini bloccati, permettendo agli utenti di monitorare l‚Äôattivit√† di rete e l‚Äôefficacia dei blocchi.\nInterfaccia Web: Il software √® dotato di un‚Äôinterfaccia utente web che semplifica la configurazione e la gestione. Attraverso questa interfaccia, gli utenti possono monitorare le statistiche, aggiornare liste di blocco e personalizzare le impostazioni.\nListe di Blocco Personalizzabili: Pi-hole consente agli utenti di aggiungere o rimuovere domini dalle liste di blocco in base alle proprie preferenze e esigenze.\nSupporto per IPv6: Il software supporta IPv6, garantendo una copertura completa delle richieste DNS e dei blocchi su entrambi gli standard di indirizzamento IP.\nIntegrazione con DHCP: Pi-hole pu√≤ fungere anche da server DHCP, semplificando ulteriormente la gestione degli indirizzi IP e la distribuzione delle configurazioni di rete.\nViste le caratteristiche non poteva mancare nella suite di servizi installati sul mio homelab, l‚Äôinstallazione di per s√® √© molto semplice infatti baster√† eseguire un semplice script che vi auto guider√† nelle varie impostazioni\ncurl -sSL https://install.pi-hole.net | bash Io personalmente per ora l‚Äôho installato su un raspberry Pi 3 dove sono gi√† presenti le istanze di Nginx Proxy Manager e il container per la gestione del tunnel cloudflare, scriver√≤ un‚Äôarticolo specifico su questa macchina in futuro.\nVoi potete installarlo dove meglio credete, un server linux, un container docker o su una VM, up to you! La documentazione √® molto chiara e completa per ogni dettaglio specifico non posso che invitarvi a leggerla cliccando QUI\nAttenzione √® importante che il server abbia l‚Äôindirizzo IP statico, questo non dovr√† cambiare mai perch√© verr√† chiamato da tutti gli altri device, se avete il DHCP attivo baster√† impostare una reservation.\nEcco l\u0026rsquo;aspetto che ha la comodissima interfaccia WEB Oltre a liberami dalla pubblicit√† Pi-Hole mi √® stato utilissimo anche come DNS locale per il mio homelab\nPer completare il tutto non dovete fare altro che modificare i parametri network sui vostri device facendo in modo che il DNS utilizzato sia l‚ÄôIP della vostra installazione Pi-Hole, in questo modo ogni richiesta verr√† gestita da Pi-Hole e addio alla pubblicit√†!\nPer chi ne avesse la possibilit√† la modifica DNS si potrebbe eseguire gi√† a livello del vostro router, baster√† editare una sola volta sull‚Äôapparato per avere cos√¨ di conseguenza tutti i device gi√† configurati.\nEvviva l‚Äôopensource!\n","permalink":"https://marcofanuntza.it/2024/01/06/grazie-pihole-basta-pubblicita/","summary":"\u003cp\u003eQuanto vi infastidiscono gli annunci pubblicitari navigando sul web? Ormai ci sono pagine web piene di annunci, pop-up fastidiosissimi che non fanno altro che farci perdere tempo e voglia di visitarne il sito, i quotidiani con le news e le notizie sportive su tutti sono i pi√π scassa bit.\u003c/p\u003e\n\u003cp\u003ePer fortuna ci viene in aiuto Pi-Hole!\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/pi-hole-pagina0.webp\" alt=\"Example image\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003ePi-hole √® un software open-source progettato per il controllo e la gestione della rete orientato nello specifico proprio per combattere la pubblicit√† e gli annunci correlati, sostanzialmente agisce come un filtro DNS, offrendo funzionalit√† avanzate per bloccare tutti gli annunci pubblicitari, tracker e tutti i contenuti indesiderati ancor prima che raggiungano i nostri dispositivi connessi alla rete.\u003c/p\u003e","title":"Grazie a Pi-Hole basta con la pubblicit√†!"},{"content":"Acquistai il mio primo NAS nel lontano 2011, uno Zyxel NSA320 che nonostante tutto funziona ancora ma il peso degli anni si sente tutto, √® ormai fuori supporto da tempo, nessuna opzione per upgrade o mod varie, versione tls obsoleta e opzioni su share NFS inesistenti. Quest‚Äôultimo aspetto mi sta dando problemi con i backup dei container che ho su Proxmox e per questo ho preso la decisione‚Ä¶ nuovo NAS sia!\nIl primo aspetto preso in considerazione √®: pappa pronta acquistando un prodotto commerciale dove basta inserire i dischi accendere e via, oppure costruirmelo da me?\nDevo ammettere che i nuovi NAS in commercio hanno tutta una serie di features veramente interessanti, sono praticamente dei mini computer e oltre a salvare i dati permettono di installarci vari servizi, il famoso Synology DS220+ ad esempio pu√≤ essere un nas, un mediacenter, un reverse proxy, erogare virtual machines, erogare container docker, con questi ultimi due aspetti praticamente pu√≤ fornire molteplici servizi, chi pi√π ne ha pi√π ne metta.\nTutto questo per√≤ ovviamente ha un costo (alto a mio avviso) e altro aspetto da tenere in conto √® che la maggior parte delle volte dovete aggiungerci i dischi, money! cantavano i Pink Floyd.\nLa mia curiosit√† batte la pigrizia della pappa pronta, quindi ho optato per costruirmelo da me, poi diciamocelo chiaro il bello √® proprio quello! e un articolo descrivendo quanto fosse bello il Synology non l‚Äôavrei poi mai scritto.\nLa scelta √® semplice, trovare un vecchio desktop pc, uno di quelli SFF (small form factor), su ebay e amazon ne trovate a bizzeffe ricondizionati, garantiscono prestazioni adeguate e consumi ridotti considerando che un NAS dovrebbe rimanere acceso H24.\nTra i tanti modelli, dopo varie analisi, la mia scelta √® ricaduta su un Dell OPTIPLEX 3050 SFF, quello che ho trovato io ha una cpu non troppo vecchia Intel i3-7100, 8GB di ram DDR4 espandibile e aspetto scatenante la scelta √® la presenza di uno slot M2 per storage nvme, dove ci andr√† installato l‚ÄôOS.\nOk abbiamo la macchina, l‚Äôaltra decisione da prendere √®: che tipo di dischi utilizzare? le opzioni sono due, dischi super performanti SSD oppure classici e intramontabili HDD rotativi. Il prezzo dei dischi SSD √® sceso tanto negli ultimi anni, ormai sono de facto i dischi da utilizzare su workstation e notebook, non sarebbero male da utilizzare anche sui NAS, c‚Äô√® per√≤ un ma, i dischi SSD hanno una data di ‚Äúscadenza‚Äù in base data dal numero di scritture eseguite, a questo si aggiunge un‚Äôaspetto che scoprirete dopo relativo al filesystem che verr√† utilizzato ZFS, che per come funziona andr√† sicuramente ad accorciarne ulteriormente la loro longevit√†.\nQuindi si la mia scelta √® stata‚Ä¶ HDD rotativi! e saranno da 2,5 pi√π facili da inserire dentro alla macchina SFF e meno energivori rispetto agli HDD classici da 3,5 pollici.\nInfine arriva il software da utilizzare, scelta gi√† fatta come avrete intuito dal titolo stesso dell‚Äôarticolo, Truenas CORE sar√† il software, il cuore e la mente del mio nuovo (vecchio) NAS.\nTrueNAS in versione CORE √® una piattaforma di storage open-source basata su FreeBSD, progettata per fornire soluzioni di storage e condivisione di dati scalabili e affidabili. Originariamente conosciuta come FreeNAS, TrueNAS √® stata ribattezzata per riflettere la sua evoluzione e le sue funzionalit√† avanzate. Ecco alcune delle sue caratteristiche principali:\nStorage Unificato: Supporta protocolli come SMB/CIFS, NFS, AFP, iSCSI, S3 e altri, consentendo la centralizzazione della gestione dei dati e l‚Äôaccesso da diversi sistemi operativi. ZFS File System: Utilizza il file system ZFS, noto per la sua robustezza, gestione avanzata dei dati e funzionalit√† di snapshot e clone. Virtualizzazione e Containerization: Fornisce funzionalit√† di virtualizzazione e containerization con supporto per VMware, Hyper-V, Bhyve e Docker. Ridondanza e Alta Disponibilit√†: Offre opzioni avanzate per la ridondanza e l‚Äôalta disponibilit√†, garantendo la continuit√† degli accessi ai dati e proteggendo da guasti hardware. Sistema di Archiviazione Condiviso: Configurabile come un sistema di archiviazione condiviso in reti aziendali, consentendo a diversi utenti di accedere e condividere dati in modo sicuro. Backup e Ripristino: Include funzionalit√† di backup e ripristino, consentendo la creazione di snapshot e la pianificazione di backup automatici per garantire la sicurezza dei dati. Interfaccia Web Intuitiva: Dispone di un‚Äôinterfaccia web intuitiva che semplifica la gestione e la configurazione del sistema, rendendo accessibili molte funzionalit√† avanzate. TrueNAS CORE e TrueNAS SCALE sono due varianti della piattaforma di storage TrueNAS, entrambe sviluppate da iXsystems. Ecco alcune differenze chiave tra i due:\nTrueNAS CORE:\nFile System ZFS: TrueNAS CORE sfrutta il file system ZFS per avanzate funzionalit√† di gestione dati, snapshot e resistenza ai guasti. Stabilit√† e Affidabilit√†: √à noto per la sua stabilit√† e affidabilit√†, consigliato per utilizzi in ambienti critici. Orientato agli Utenti Esperti: TrueNAS CORE √® pi√π indicato per utenti esperti e amministratori di sistema che desiderano maggiore controllo sulla configurazione. TrueNAS SCALE:\nBasato su Debian: TrueNAS SCALE ha una base Debian Linux, offrendo maggiore flessibilit√† nell‚Äôintegrazione con software basato su Linux. Architettura pi√π Moderna: Progettato con un‚Äôarchitettura moderna e scalabile, adatto a carichi di lavoro su larga scala. Interfaccia Kubernetes: TrueNAS SCALE presenta un‚Äôinterfaccia Kubernetes nativa, semplificando l‚Äôimplementazione e la gestione di applicazioni in contenitori. Approccio All-in-One: Pensato come soluzione all-in-one con un modello di implementazione semplificato, adatto anche a utenti meno esperti. App Store: TrueNAS SCALE include un App Store integrato con una variet√† di applicazioni e servizi aggiuntivi facilmente installabili e gestibili. In breve, mentre TrueNAS CORE offre stabilit√† consolidata e controllo avanzato, TrueNAS SCALE si orienta verso una flessibilit√† moderna, con facilit√† d‚Äôuso e integrazione di Kubernetes. La scelta tra i due dipende dalle esigenze specifiche dell‚Äôutente e dell‚Äôambiente.\nIo avendo gi√† un homelab con Proxmox, non ho l‚Äôesigenza di avere un ulteriore ambiente che sarebbe stato una sorta di ‚Äúdoppione‚Äù con Truenas in versione SCALE e poi a me serve un NAS.\nA questo punto dovrei mostrarvi il risultato‚Ä¶‚Ä¶ si per√≤ dovrete attendere che mi arrivino i componenti!\nCome diceva il grande JeeG Robot ‚ÄúMiwa lanciami i componenti!‚Äù\nEcco clicca QUI per la seconda parte dell\u0026rsquo;articolo\n","permalink":"https://marcofanuntza.it/2024/01/01/il-mio-nuovo-nas-con-truenas/","summary":"\u003cp\u003eAcquistai il mio primo NAS nel lontano 2011, uno Zyxel NSA320 che nonostante tutto funziona ancora ma il peso degli anni si sente tutto, √® ormai fuori supporto da tempo, nessuna opzione per upgrade o mod varie, versione tls obsoleta e opzioni su share NFS inesistenti. Quest‚Äôultimo aspetto mi sta dando problemi con i backup dei container che ho su Proxmox e per questo ho preso la decisione‚Ä¶ nuovo NAS sia!\u003c/p\u003e","title":"Il mio nuovo NAS con Truenas"},{"content":"Problema\nAbbiamo dimenticato la password di un container LXC in esecuzione su Proxmox e non abbiamo alternative se non quella di resettarla.\nSoluzione\n*Effettuare l‚Äôaccesso sulla web GUI del vostro cluster Proxmox.\n*Individuate il container LXC per il quale si desidera reimpostare la password e ricordarsi l‚ÄôID del container. Ad esempio, se vediamo un container chiamato:\n105 (passwdDimenticata) *105 sar√†il suo ID, passwdDimenticata sar√† il suo nome.\n*Avviare il container nel caso non lo fosse gi√†\n*Ora connettersi via SSH sull‚Äôhost di Proxmox che sta eseguendo il container (come utente root) o aprite una Shell/Console sulla web GUI di Proxmox, sempre sul nodo che sta eseguendo il container\n*Utilizzare il seguente comando per collegare la sessione al container LXC\nlxc-attach -n 105 *A questo punto ci troveremo all‚Äôinterno della shell del container e potremo eseguire i comandi, nello specifico per cambiare la password di root\npasswd *Digitare la nuova password, premere il tasto Invio, quindi digitare nuovamente la password e premere nuovamente Invio per confermare la nuova password di root del container. -Un a volta completato, √® possibile effettuare l‚Äôaccesso al container con la nuova password.\n","permalink":"https://marcofanuntza.it/2023/12/31/come-reimpostare-la-password-root-in-un-container-lxc-su-proxmox-ve/","summary":"\u003cp\u003e\u003cstrong\u003eProblema\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAbbiamo dimenticato la password di un container LXC in esecuzione su Proxmox e non abbiamo alternative se non quella di resettarla.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSoluzione\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e*Effettuare l‚Äôaccesso sulla web GUI del vostro cluster Proxmox.\u003c/p\u003e\n\u003cp\u003e*Individuate il container LXC per il quale si desidera reimpostare la password e ricordarsi l‚ÄôID del container. Ad esempio, se vediamo un container chiamato:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e105 (passwdDimenticata) \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e*105 sar√†il suo ID, passwdDimenticata sar√† il suo nome.\u003c/p\u003e\n\u003cp\u003e*Avviare il container nel caso non lo fosse gi√†\u003c/p\u003e","title":"Come reimpostare la password root in un container LXC su Proxmox VE"},{"content":"Spesso pu√≤ capitare la necessit√† di dover ridimensionare il disco di un container creato troppo grande o relativamente piccolo, a dire la verit√† spesso si tende ad aumentare lo spazio disco, vuoi crescita dei logs, aggiunta di nuove funzioni, in questo caso specifico per√≤ mostreremo come ridurre lo spazio.\nPer chi come me utilizza Proxmox in ambito domestico dovr√† fare subito i conti con la mancanza di una LAN a 10GbE, questo si tradurr√† in migrazioni tra i nodi eccessivamente lunghe, la regola quindi deve essere: container piccolo = veloce. Quando si crea un container pu√≤ succedere di non aver ben chiaro in mente quanto dimensionare lo spazio disco, ma niente paura si ridimensiona!\nProxmox di default quando crea il container utilizza LVM che sta per (in lingua inglese logical volume manager) in italiano gestore logico dei volumi, ringraziamo quindi questa scelta perch√© ci permetter√† di ridimensionare i nostri container con dei semplici passaggi che scopo di questo articolo vi elencher√≤ qui di seguito.\nPrima di tutto come sempre vi consiglio di eseguire un backup del container interessato, terminato il backup saremo pronti a partire.\nPer i comandi che seguono sar√† necessario operare da \u0026gt;_Shell, la potete attivare da Proxmox e sar√† sul nodo che in quel momento sta eseguendo il container interessato. Io personalmente preferisco operare direttamente da shell ssh ma sar√† uguale.\nIniziamo con individuare il volume utilizzato dal container eseguendo questo comando:\nlvdisplay | grep \u0026quot;LV Path\\|LV Size\u0026quot; il risultato sar√† simile al seguente; simile non uguale sia ben chiaro üòâ\nLV Size 141.23 GiB LV Path /dev/pve/swap LV Size 8.00 GiB LV Path /dev/pve/root LV Size 69.37 GiB LV Path /dev/pve/vm-101-disk-0 LV Size 22.00 GiB dovrete prestare attenzione alle ultime due righe, queste infatti si riferiscono al disco (volume) del container che andremo a modificare, che potete intuire sar√† vm-101-disk-0 sul percorso /dev/pve/vm-101-disk-0\nintanto procediamo con spegnere il container, si scusate non lo avevo ancora scritto, queste operazioni vanno eseguite con il container spento, eseguite il comando seguente per spegnere il container\npct stop 101 adesso partiamo con ridimensionare il filesystem del volume, nel nostro caso intento √® di farlo diventare da 10GB quindi eseguiamo:\nresize2fs /dev/pve/vm-101-disk-0 10G il risultato ci restituir√†:\nresize2fs 1.47.0 (5-Feb-2023) Resizing the filesystem on /dev/pve/vm-101-disk-0 to 2621440 (4k) blocks. The filesystem on /dev/pve/vm-101-disk-0 is now 2621440 (4k) blocks long. a questo punto siamo pronti con il comando che effettivamente ridimensioner√† il nostro volume, il comando lvreduce fa parte della suite messa a disposizione da LVM, per i pi√π curiosi c‚Äô√® sempre il MAN di linux per approfondimenti, che detto in Sardo ‚Äúmai ammanchidi‚Äù (mai manchi)\nlvreduce -L 10G /dev/pve/vm-101-disk-0 WARNING: Reducing active logical volume to 10.00 GiB. THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce pve/vm-101-disk-0? y/n]: y confermate con ‚Äúy‚Äù senza paura! avevate fatto il backup si? Il risultato che restituir√† il comando sar√† il seguente:\nSize of logical volume pve/vm-101-disk-0 changed from 22.00 GiB (5632 extents) to 10.00 GiB (2560 extents). Logical volume pve/vm-101-disk-0 successfully resized. le operazioni a questo punto sono praticamente completate, ma manca un ultimo piccolo passaggio e sar√† relativo alla modifica della configurazione del container, dobbiamo semplicemente fargli sapere che le dimensioni del disco sono cambiate\nvi /etc/pve/lxc/101.conf la stringa rootfs dovr√† cambiare semplicemente da 22GB a 10GB nel punto ‚Äúsize‚Äù\nrootfs: local-lvm:vm-101-disk-0,size=10G ora potete finalmente accendere il container e per avere la certezza della buona riuscita procedete con accedere nel vostro container su console e verificate lo spazio disco con un semplice df -h\nQuesto √® tutto, il ridimensionamento √® possibile anche in caso contrario, quindi aggiungendo spazio disco, il comando in quel caso sar√† lvextend +10G e resize2fs ma comunque sia Proxmox in quel caso vi viene in aiuto permettendovi di eseguirlo direttamente da gui e completamente automizzato.\n","permalink":"https://marcofanuntza.it/2023/12/17/ridimensionare-disco-container-su-proxmox/","summary":"\u003cp\u003eSpesso pu√≤ capitare la necessit√† di dover ridimensionare il disco di un container creato troppo grande o relativamente piccolo, a dire la verit√† spesso si tende ad aumentare lo spazio disco, vuoi crescita dei logs, aggiunta di nuove funzioni, in questo caso specifico per√≤ mostreremo come ridurre lo spazio.\u003c/p\u003e\n\u003cp\u003ePer chi come me utilizza Proxmox in ambito domestico dovr√† fare subito i conti con la mancanza di una LAN a 10GbE, questo si tradurr√† in migrazioni tra i nodi eccessivamente lunghe, la regola quindi deve essere: container piccolo = veloce. Quando si crea un container pu√≤ succedere di non aver ben chiaro in mente quanto dimensionare lo spazio disco, ma niente paura si ridimensiona!\u003c/p\u003e","title":"Ridimensionare disco container su Proxmox"},{"content":"Introduzione:\nNel corso della mia esperienza nel settore dell‚ÄôIT, se c‚Äô√® qualcosa che ho imparato √® che l‚Äôefficienza e la flessibilit√† sono fondamentali. √à qui che entra in gioco Proxmox, una piattaforma che ho scoperto essere un vero game-changer. In questo primo articolo, voglio condividere la mia esperienza con Proxmox e negli articoli che seguiranno mostrarvi come ho configurato il mio ‚Äúhomedatacenter‚Äù.\nChe cos‚Äô√® Proxmox?\n√® una piattaforma di virtualizzazione, un hypervisor di tipo 1 permette di virtualizzare virtual machine e container interfaccia web per il controllo permette configurazione in cluster gestisce storage, snapshot e backup automatizzati basata su Debian, utilizza KVM per le vm e LXC per i container completamente free e open source piani di licenza enterprise attivabili Virtualizzazione e Containerizzazione: La Combo Vincente\nAttraverso l‚Äôutilizzo di Proxmox, ho scoperto il potenziale di KVM (Kernel-based Virtual Machine) per una virtualizzazione performante delle VM e con LCX (Linux Containers) una rapida implementazione e gestione dei containers. La sinergia tra queste due tecnologie consente di raggiungere prestazioni elevate e un livello di efficienza senza precedenti.\nInterfaccia Web Intuitiva: Gestione a Portata di Click\nQuello che si apprezza subito di Proxmox √® la sua interfaccia web user-friendly. Posso monitorare le risorse in tempo reale, eseguire facilmente operazioni di backup e ripristino. La gestione diventa un‚Äôesperienza visiva e accessibile anche ai meno esperti.\nBackup e Ripristino: La Sicurezza dei Miei Dati al Primo Posto\nProxmox garantisce una tranquillit√† in pi√π con la sua robusta funzionalit√† di backup e ripristino. Creare snapshot delle VM e dei container √® un gioco da ragazzi. In caso di necessit√†, il ripristino √® veloce e indolore.\nCluster e Alta Disponibilit√†: Non Solo per le Aziende\nIl bello di Proxmox √® che non √® riservato solo alle grandi aziende. Anche nel mio ambiente domestico, ho potuto creare un cluster in alta affidabilit√†. La gestione centralizzata, la distribuzione automatica del carico e l‚Äôalta disponibilit√† delle risorse sono diventate una realt√† anche nel mio piccolo ‚Äúdata center casalingo‚Äù.\nComunit√† Attiva e Supporto Professionale: Una Rete di Supporto a Portata di Mano\nUnirsi alla comunit√† Proxmox √® stato un passo naturale. La condivisione di esperienze e il supporto degli sviluppatori e degli utenti sono inestimabili. E se mai avessi bisogno di un livello di assistenza pi√π professionale, c‚Äô√® il supporto dedicato per le esigenze aziendali.\nConclusioni:\nProxmox ha trasformato la mia gestione delle risorse server. Se stai cercando flessibilit√†, controllo e un‚Äôesperienza di gestione che si adatti alle tue esigenze, ti consiglio vivamente di dare un‚Äôocchiata a Proxmox. Sia che tu stia gestendo un ambiente aziendale complesso o stia creando un cluster nel tuo salotto, Proxmox offre un controllo senza precedenti sulle tue risorse server. Non potrei essere pi√π soddisfatto della mia scelta.\n","permalink":"https://marcofanuntza.it/2023/12/16/gestione-home-datacenter-con-proxmox/","summary":"\u003cp\u003e\u003cstrong\u003eIntroduzione:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNel corso della mia esperienza nel settore dell‚ÄôIT, se c‚Äô√® qualcosa che ho imparato √® che l‚Äôefficienza e la flessibilit√† sono fondamentali. √à qui che entra in gioco Proxmox, una piattaforma che ho scoperto essere un vero game-changer. In questo primo articolo, voglio condividere la mia esperienza con Proxmox e negli articoli che seguiranno mostrarvi come ho configurato il mio ‚Äúhomedatacenter‚Äù.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChe cos‚Äô√® Proxmox?\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e√® una piattaforma di virtualizzazione, un hypervisor di tipo 1\u003c/li\u003e\n\u003cli\u003epermette di virtualizzare virtual machine e container\u003c/li\u003e\n\u003cli\u003einterfaccia web per il controllo\u003c/li\u003e\n\u003cli\u003epermette configurazione in cluster\u003c/li\u003e\n\u003cli\u003egestisce storage, snapshot e backup automatizzati\u003c/li\u003e\n\u003cli\u003ebasata su Debian, utilizza KVM per le vm e LXC per i container\u003c/li\u003e\n\u003cli\u003ecompletamente free e open source\u003c/li\u003e\n\u003cli\u003epiani di licenza enterprise attivabili\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eVirtualizzazione e Containerizzazione: La Combo Vincente\u003c/strong\u003e\u003c/p\u003e","title":"Gestione home datacenter con Proxmox"}]